{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b068d31",
   "metadata": {},
   "source": [
    "### Cell A: Building Raw 2k manifest\n",
    "Goal: parse nslt_2000.json + WLASL_v0.3.json, map gloss→video ids→files under videos/, split into train/val/test, and write manifest_nslt2000_raw.csv with columns: path,gloss,label,split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e4bf5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root: /home/falasoul/notebooks/USD/AAI-590/Capstone/AAI-590-G3-ASL\n",
      "NSLT: True /home/falasoul/notebooks/USD/AAI-590/Capstone/AAI-590-G3-ASL/data/wlasl_preprocessed/nslt_2000.json\n",
      "WLASL: True /home/falasoul/notebooks/USD/AAI-590/Capstone/AAI-590-G3-ASL/data/wlasl_preprocessed/WLASL_v0.3.json\n",
      "Videos dir exists: True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# project root (your notebooks/ lives under repo root)\n",
    "root = Path(\"..\").resolve()\n",
    "data_dir   = root / \"data\" / \"wlasl_preprocessed\"\n",
    "videos_dir = data_dir / \"videos\"\n",
    "\n",
    "nslt_path  = data_dir / \"nslt_2000.json\"   # dict: { \"05798\": {\"subset\": \"...\", \"action\":[...]} }\n",
    "wlasl_path = data_dir / \"WLASL_v0.3.json\"  # list: [{gloss, instances:[{video_id, video_path, split}]}]\n",
    "\n",
    "print(\"Root:\", root)\n",
    "print(\"NSLT:\", nslt_path.exists(), nslt_path)\n",
    "print(\"WLASL:\", wlasl_path.exists(), wlasl_path)\n",
    "print(\"Videos dir exists:\", videos_dir.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df08522a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WLASL index videos: 21083 | glosses: 2000\n",
      "Sample: [('69241', {'gloss': 'book', 'url_path': 'http://aslbricks.org/New/ASL-Videos/book.mp4', 'split_wlasl': 'train'})]\n"
     ]
    }
   ],
   "source": [
    "def norm_split(s: str):\n",
    "    s = (s or \"\").lower()\n",
    "    if s.startswith(\"tra\"): return \"train\"\n",
    "    if s.startswith(\"val\") or s.startswith(\"dev\"): return \"val\"\n",
    "    if s.startswith(\"tes\"): return \"test\"\n",
    "    return s\n",
    "\n",
    "with open(wlasl_path, \"r\") as f:\n",
    "    wlasl = json.load(f)\n",
    "\n",
    "# video_id -> {gloss, url_path, split_wlasl}\n",
    "vid2wlasl = {}\n",
    "gloss2ids = {}\n",
    "\n",
    "for entry in wlasl:\n",
    "    gloss = entry.get(\"gloss\") or entry.get(\"label\") or entry.get(\"name\")\n",
    "    if not gloss:\n",
    "        continue\n",
    "    for inst in entry.get(\"instances\", []):\n",
    "        vid  = str(inst.get(\"video_id\") or inst.get(\"id\") or \"\").zfill(5)\n",
    "        vrel = inst.get(\"video_path\") or inst.get(\"path\") or inst.get(\"url\")\n",
    "        sp   = norm_split(inst.get(\"split\") or inst.get(\"subset\") or inst.get(\"phase\"))\n",
    "        if not (vid and vrel):\n",
    "            continue\n",
    "        vid2wlasl[vid] = {\"gloss\": gloss, \"url_path\": vrel, \"split_wlasl\": sp}\n",
    "        gloss2ids.setdefault(gloss, []).append(vid)\n",
    "\n",
    "print(f\"WLASL index videos: {len(vid2wlasl)} | glosses: {len(gloss2ids)}\")\n",
    "print(\"Sample:\", list(vid2wlasl.items())[:1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b790fa",
   "metadata": {},
   "source": [
    "Cell C — Join NSLT (authoritative split) → Manifest (includes video_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f19ee1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[manifest_nslt2000_raw.csv] rows=21083 | train=14289 val=3916 test=2878 | classes=2000\n",
      "⚠️ Missing 12 video_ids in WLASL index (skipped). e.g. ['12209', '39347', '60721', '16096', '13422', '57839', '47639', '51153', '20138', '48251']\n",
      "  video_id                                               path    gloss  split  \\\n",
      "0    65097  /home/falasoul/notebooks/USD/AAI-590/Capstone/...      art  train   \n",
      "1    35544  /home/falasoul/notebooks/USD/AAI-590/Capstone/...       me  train   \n",
      "2    32962  /home/falasoul/notebooks/USD/AAI-590/Capstone/...  lettuce    val   \n",
      "\n",
      "   label  \n",
      "0     98  \n",
      "1   1071  \n",
      "2   1017  \n"
     ]
    }
   ],
   "source": [
    "with open(nslt_path, \"r\") as f:\n",
    "    nslt = json.load(f)   # dict keyed by video_id\n",
    "\n",
    "rows, missing = [], []\n",
    "for vid_key, meta in nslt.items():\n",
    "    vid = str(vid_key).zfill(5)\n",
    "    split = norm_split(meta.get(\"subset\"))\n",
    "    w = vid2wlasl.get(vid)\n",
    "    if not w:\n",
    "        missing.append(vid)\n",
    "        continue\n",
    "    # build a *candidate* local path from the WLASL url_path (will be cleaned later)\n",
    "    candidate = (data_dir / \"videos\" / w[\"url_path\"]).as_posix()\n",
    "    rows.append({\n",
    "        \"video_id\": vid,\n",
    "        \"path\": candidate,          # will be normalized later\n",
    "        \"gloss\": w[\"gloss\"],\n",
    "        \"split\": split if split in (\"train\",\"val\",\"test\") else (w[\"split_wlasl\"] or \"train\"),\n",
    "    })\n",
    "\n",
    "df_raw = pd.DataFrame(rows).drop_duplicates(subset=[\"video_id\"])\n",
    "glosses = sorted(df_raw[\"gloss\"].unique())\n",
    "g2id = {g:i for i,g in enumerate(glosses)}\n",
    "df_raw[\"label\"] = df_raw[\"gloss\"].map(g2id).astype(int)\n",
    "\n",
    "# Save raw (includes video_id)\n",
    "man_raw = data_dir / \"manifest_nslt2000_raw.csv\"\n",
    "df_raw[[\"path\",\"gloss\",\"label\",\"split\",\"video_id\"]].to_csv(man_raw, index=False)\n",
    "\n",
    "pd.DataFrame({\"gloss\":glosses, \"label\":[g2id[g] for g in glosses]}).to_csv(\n",
    "    data_dir/\"class_map_nslt2000.csv\", index=False\n",
    ")\n",
    "\n",
    "print(f\"[manifest_nslt2000_raw.csv] rows={len(df_raw)} \"\n",
    "      f\"| train={(df_raw['split']=='train').sum()} \"\n",
    "      f\"val={(df_raw['split']=='val').sum()} \"\n",
    "      f\"test={(df_raw['split']=='test').sum()} \"\n",
    "      f\"| classes={len(glosses)}\")\n",
    "if missing:\n",
    "    print(f\"⚠️ Missing {len(missing)} video_ids in WLASL index (skipped). e.g. {missing[:10]}\")\n",
    "print(df_raw.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29a36ad",
   "metadata": {},
   "source": [
    "Cell D — Strict local manifest (only keep videos/<video_id>.mp4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60bbff33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "videos/*.mp4 on disk: 11980\n",
      "[manifest_nslt2000_localid.csv] rows: 11980 | unique paths: 11980 | classes: 2000\n",
      "Splits: {'train': 8313, 'val': 2253, 'test': 1414}\n",
      "Example: {'video_id': '65097', 'path': '/home/falasoul/notebooks/USD/AAI-590/Capstone/AAI-590-G3-ASL/data/wlasl_preprocessed/videos/65097.mp4', 'gloss': 'art', 'label': 98, 'split': 'train'}\n"
     ]
    }
   ],
   "source": [
    "# keep rows where videos/<video_id>.mp4 exists on disk; drop everything else\n",
    "def id_path_on_disk(vid):\n",
    "    p = videos_dir / f\"{str(vid).zfill(5)}.mp4\"\n",
    "    return p if p.exists() else None\n",
    "\n",
    "df_strict = df_raw.copy()\n",
    "df_strict[\"local_path\"] = df_strict[\"video_id\"].map(id_path_on_disk)\n",
    "df_strict = df_strict[df_strict[\"local_path\"].notna()].copy()\n",
    "df_strict[\"path\"] = df_strict[\"local_path\"].map(lambda p: p.as_posix())\n",
    "df_strict = df_strict.drop(columns=[\"local_path\"]).drop_duplicates(subset=[\"video_id\"])\n",
    "\n",
    "man_local = data_dir / \"manifest_nslt2000_localid.csv\"\n",
    "df_strict[[\"path\",\"gloss\",\"label\",\"split\",\"video_id\"]].to_csv(man_local, index=False)\n",
    "\n",
    "print(\"videos/*.mp4 on disk:\", len(list(videos_dir.glob(\"*.mp4\"))))\n",
    "print(\"[manifest_nslt2000_localid.csv] rows:\", len(df_strict),\n",
    "      \"| unique paths:\", df_strict[\"path\"].nunique(),\n",
    "      \"| classes:\", df_strict[\"gloss\"].nunique())\n",
    "print(\"Splits:\",\n",
    "      {s:int((df_strict['split']==s).sum()) for s in ('train','val','test')})\n",
    "print(\"Example:\", df_strict.iloc[0][[\"video_id\",\"path\",\"gloss\",\"label\",\"split\"]].to_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef740996",
   "metadata": {},
   "source": [
    "Cell E — Quick distribution sanity (optional but recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a8d1b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 classes by train samples:\n",
      " label    gloss  count\n",
      "   168   before     12\n",
      "   164      bed     12\n",
      "   789       go     11\n",
      "   418     cool     11\n",
      "  1797     thin     11\n",
      "   568    drink     11\n",
      "  1843    trade     10\n",
      "   392 computer     10\n",
      "   678   family     10\n",
      "  1751     tall     10\n",
      "\n",
      "Lowest 10 classes by train samples:\n",
      " label      gloss  count\n",
      "   250        bug      1\n",
      "   562     dragon      1\n",
      "  1043    look at      1\n",
      "  1867         tv      1\n",
      "  1922  wash face      1\n",
      "  1094       milk      1\n",
      "  1410    realize      1\n",
      "  1363 propaganda      1\n",
      "  1361    promote      1\n",
      "   448       cuba      1\n"
     ]
    }
   ],
   "source": [
    "# per-class counts (train split)\n",
    "train_counts = (df_strict[df_strict[\"split\"]==\"train\"]\n",
    "                .groupby([\"label\",\"gloss\"])[\"video_id\"].count()\n",
    "                .rename(\"count\").reset_index()\n",
    "                .sort_values(\"count\", ascending=False))\n",
    "\n",
    "print(\"Top 10 classes by train samples:\")\n",
    "print(train_counts.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nLowest 10 classes by train samples:\")\n",
    "print(train_counts.tail(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590f44cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
