{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdb629bf",
   "metadata": {},
   "source": [
    "### Cell A â€” Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1497cf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: /home/falasoul/notebooks/USD/AAI-590/Capstone/AAI-590-G3-ASL/data/wlasl_preprocessed/manifest_nslt2000_roi_top104_balanced_clean.csv\n",
      "Samples: 1159 | classes=104\n",
      "Columns: ['video_id', 'path', 'gloss', 'label', 'split', 'exists', 'label_new']\n",
      "label_new min/max: 0 103\n",
      "label_new nunique: 104\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>path</th>\n",
       "      <th>gloss</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "      <th>exists</th>\n",
       "      <th>label_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>639</td>\n",
       "      <td>/home/falasoul/notebooks/USD/AAI-590/Capstone/...</td>\n",
       "      <td>accident</td>\n",
       "      <td>8</td>\n",
       "      <td>train</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>624</td>\n",
       "      <td>/home/falasoul/notebooks/USD/AAI-590/Capstone/...</td>\n",
       "      <td>accident</td>\n",
       "      <td>8</td>\n",
       "      <td>train</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>632</td>\n",
       "      <td>/home/falasoul/notebooks/USD/AAI-590/Capstone/...</td>\n",
       "      <td>accident</td>\n",
       "      <td>8</td>\n",
       "      <td>train</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>623</td>\n",
       "      <td>/home/falasoul/notebooks/USD/AAI-590/Capstone/...</td>\n",
       "      <td>accident</td>\n",
       "      <td>8</td>\n",
       "      <td>train</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65009</td>\n",
       "      <td>/home/falasoul/notebooks/USD/AAI-590/Capstone/...</td>\n",
       "      <td>accident</td>\n",
       "      <td>8</td>\n",
       "      <td>train</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   video_id                                               path     gloss  \\\n",
       "0       639  /home/falasoul/notebooks/USD/AAI-590/Capstone/...  accident   \n",
       "1       624  /home/falasoul/notebooks/USD/AAI-590/Capstone/...  accident   \n",
       "2       632  /home/falasoul/notebooks/USD/AAI-590/Capstone/...  accident   \n",
       "3       623  /home/falasoul/notebooks/USD/AAI-590/Capstone/...  accident   \n",
       "4     65009  /home/falasoul/notebooks/USD/AAI-590/Capstone/...  accident   \n",
       "\n",
       "   label  split  exists  label_new  \n",
       "0      8  train    True          0  \n",
       "1      8  train    True          0  \n",
       "2      8  train    True          0  \n",
       "3      8  train    True          0  \n",
       "4      8  train    True          0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Cell A â€” Imports, reproducibility, load balanced ROI manifest ===\n",
    "import os, random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import cv2\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Keep CPU threads tame\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "root = Path(\"..\").resolve()\n",
    "data_dir = root / \"data\" / \"wlasl_preprocessed\"\n",
    "\n",
    "# Use the specific balanced ROI manifest you showed\n",
    "man_path = data_dir / \"manifest_nslt2000_roi_top104_balanced_clean.csv\"\n",
    "assert man_path.exists(), f\"Manifest not found: {man_path}\"\n",
    "\n",
    "df = pd.read_csv(man_path)\n",
    "print(\"Loaded:\", man_path)\n",
    "print(f\"Samples: {len(df)} | classes={df['gloss'].nunique()}\")\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "print(\"label_new min/max:\", df[\"label_new\"].min(), df[\"label_new\"].max())\n",
    "print(\"label_new nunique:\", df[\"label_new\"].nunique())\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ea8585",
   "metadata": {},
   "source": [
    "### Cell B â€” WLASLDataset (using ROI + label_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40ea786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell B â€” WLASLDataset (ROI, label_new) with safe video read ===\n",
    "import torch, numpy as np, cv2, decord\n",
    "from torch.utils.data import Dataset\n",
    "decord.bridge.set_bridge('torch')\n",
    "\n",
    "def _resize_112(frame_tchw: torch.Tensor) -> torch.Tensor:\n",
    "    T, C, H, W = frame_tchw.shape\n",
    "    arr = frame_tchw.permute(0, 2, 3, 1).cpu().numpy()\n",
    "    out = np.empty((T, 112, 112, C), dtype=np.float32)\n",
    "    for t in range(T):\n",
    "        out[t] = cv2.resize(arr[t], (112, 112), interpolation=cv2.INTER_AREA)\n",
    "    return torch.from_numpy(out).permute(0, 3, 1, 2)\n",
    "\n",
    "def _normalize(frame_tchw, mean=(0.45,)*3, std=(0.225,)*3):\n",
    "    mean = torch.tensor(mean, dtype=frame_tchw.dtype, device=frame_tchw.device)[None, :, None, None]\n",
    "    std  = torch.tensor(std,  dtype=frame_tchw.dtype, device=frame_tchw.device)[None, :, None, None]\n",
    "    return (frame_tchw - mean) / std\n",
    "\n",
    "def uniform_temporal_indices(n_total, clip_len, stride):\n",
    "    if n_total <= 0:\n",
    "        return [0] * clip_len\n",
    "    wanted = (clip_len - 1) * stride + 1\n",
    "    if n_total >= wanted:\n",
    "        start = (n_total - wanted) // 2\n",
    "        return [start + i * stride for i in range(clip_len)]\n",
    "    idxs = [min(i * stride, n_total - 1) for i in range(clip_len)]\n",
    "    return idxs\n",
    "\n",
    "class WLASLDataset(Dataset):\n",
    "    def __init__(self, df, clip_len=32, stride=2, train=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.clip_len = clip_len\n",
    "        self.stride = stride\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _safe_load_clip(self, path: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Try to read a clip with decord. If anything fails, return a dummy zero clip.\n",
    "        Shape: [T, C, H, W]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            vr = decord.VideoReader(path)\n",
    "            n = len(vr)\n",
    "            if n <= 0:\n",
    "                raise RuntimeError(\"no frames\")\n",
    "\n",
    "            idxs = uniform_temporal_indices(n, self.clip_len, self.stride)\n",
    "            batch = vr.get_batch(idxs)        # [T,H,W,C]\n",
    "            x = batch.float() / 255.0\n",
    "            x = x.permute(0, 3, 1, 2)         # [T,C,H,W]\n",
    "            x = _resize_112(x)\n",
    "            x = _normalize(x)\n",
    "            return x\n",
    "        except Exception as e:\n",
    "            # Log once per failure, but don't kill training\n",
    "            print(f\"[WARN] Failed to read video {path}: {e} â€” using zero clip.\")\n",
    "            # [T,C,H,W] of zeros\n",
    "            return torch.zeros(self.clip_len, 3, 112, 112, dtype=torch.float32)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        row = self.df.iloc[i]\n",
    "        path = row[\"path\"]\n",
    "        label = int(row[\"label_new\"])   # contiguous 0..C-1\n",
    "\n",
    "        x = self._safe_load_clip(path)\n",
    "        return x, label, path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94cb6cb",
   "metadata": {},
   "source": [
    "#### Cell C â€” Split DataFrames & DataLoaders (single worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8283e08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes: 831 train | 192 val | 136 test\n",
      "Device: cuda\n",
      "Sample batch shape: torch.Size([4, 32, 3, 112, 112]) | labels range: 60 -> 88\n"
     ]
    }
   ],
   "source": [
    "# === Cell C â€” Splits + DataLoaders (no multiprocessing) ===\n",
    "\n",
    "train_df = df[df[\"split\"] == \"train\"].reset_index(drop=True)\n",
    "val_df   = df[df[\"split\"] == \"val\"].reset_index(drop=True)\n",
    "test_df  = df[df[\"split\"] == \"test\"].reset_index(drop=True)\n",
    "\n",
    "print(\"Split sizes:\", len(train_df), \"train |\", len(val_df), \"val |\", len(test_df), \"test\")\n",
    "\n",
    "clip_len = 32\n",
    "stride   = 2\n",
    "batch_size = 4   # small to be safe on GPU\n",
    "\n",
    "train_ds = WLASLDataset(train_df, clip_len=clip_len, stride=stride, train=True)\n",
    "val_ds   = WLASLDataset(val_df,   clip_len=clip_len, stride=stride, train=False)\n",
    "test_ds  = WLASLDataset(test_df,  clip_len=clip_len, stride=stride, train=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,              # ðŸ”’ NO worker processes\n",
    "    pin_memory=(device.type == \"cuda\"),\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=(device.type == \"cuda\"),\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=(device.type == \"cuda\"),\n",
    ")\n",
    "\n",
    "x_dbg, y_dbg, _ = next(iter(train_loader))\n",
    "print(\"Sample batch shape:\", x_dbg.shape, \"| labels range:\", y_dbg.min().item(), \"->\", y_dbg.max().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c013386d",
   "metadata": {},
   "source": [
    "#### Cell D â€” Model: R3D-18 baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e525b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes: 104\n",
      "Model on: cuda\n"
     ]
    }
   ],
   "source": [
    "# === Cell D â€” CNN + BiGRU model (from scratch) ===\n",
    "\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "class CnnBiGRUClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        rnn_hidden: int = 256,\n",
    "        rnn_layers: int = 1,\n",
    "        dropout: float = 0.3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 2D CNN backbone (ResNet-18) â€” from scratch for now (no pretrained)\n",
    "        base = resnet18(weights=None)\n",
    "        # Take everything except the final FC and global pool\n",
    "        self.cnn = nn.Sequential(*list(base.children())[:-2])  # up to last conv block\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.cnn_out_dim = base.fc.in_features  # 512 for ResNet-18\n",
    "\n",
    "        # BiGRU over time\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=self.cnn_out_dim,\n",
    "            hidden_size=rnn_hidden,\n",
    "            num_layers=rnn_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(rnn_hidden * 2, num_classes)  # *2 for bidirectional\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [B, T, C, H, W]\n",
    "        \"\"\"\n",
    "        B, T, C, H, W = x.shape\n",
    "\n",
    "        # Merge batch and time to process frames with 2D CNN\n",
    "        x = x.view(B * T, C, H, W)              # [B*T, C, H, W]\n",
    "\n",
    "        feats = self.cnn(x)                     # [B*T, C', h, w]\n",
    "        feats = self.pool(feats)                # [B*T, C', 1, 1]\n",
    "        feats = feats.view(B, T, self.cnn_out_dim)  # [B, T, F]\n",
    "\n",
    "        # BiGRU over temporal dimension\n",
    "        rnn_out, _ = self.rnn(feats)           # [B, T, 2*hidden]\n",
    "\n",
    "        # Use last time step (you could also use mean over T)\n",
    "        last = rnn_out[:, -1, :]               # [B, 2*hidden]\n",
    "\n",
    "        out = self.dropout(last)\n",
    "        logits = self.fc(out)                  # [B, num_classes]\n",
    "        return logits\n",
    "\n",
    "# Build model\n",
    "num_classes = df[\"label_new\"].nunique()\n",
    "print(\"num_classes:\", num_classes)\n",
    "\n",
    "model = CnnBiGRUClassifier(\n",
    "    num_classes=num_classes,\n",
    "    rnn_hidden=256,\n",
    "    rnn_layers=1,\n",
    "    dropout=0.3,\n",
    ").to(device)\n",
    "\n",
    "print(\"Model on:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0758c3da",
   "metadata": {},
   "source": [
    "#### Cell E â€” Optimizer, Scaler, Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5c31001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell E â€” Optimizer, scaler, loss ===\n",
    "from torch.amp import GradScaler\n",
    "\n",
    "epochs = 20\n",
    "lr     = 3e-4\n",
    "wd     = 1e-2\n",
    "amp_on = True  # you can set to False if you want to debug more easily\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "scaler = GradScaler(enabled=amp_on)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # we can plug class-weights later if we want\n",
    "best_val_acc = -1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881fd9a6",
   "metadata": {},
   "source": [
    "#### Cell F â€” run_epoch (with correct [B,C,T,H,W] permute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68cf500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell F â€” Metrics + epoch runner (for CNN+BiGRU) ===\n",
    "\n",
    "def top1_acc(logits, y):\n",
    "    return (logits.argmax(1) == y).float().mean().item()\n",
    "\n",
    "def run_epoch(loader, train=True):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_acc  = 0.0\n",
    "    total_n    = 0\n",
    "\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "\n",
    "    for x, y, _ in loader:\n",
    "        # x is already [B, T, C, H, W] from WLASLDataset\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=amp_on):\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "        if train:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            acc = top1_acc(logits, y)\n",
    "            bs  = x.size(0)\n",
    "            total_loss += loss.item() * bs\n",
    "            total_acc  += acc * bs\n",
    "            total_n    += bs\n",
    "\n",
    "    return total_loss / total_n, total_acc / total_n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03f005f",
   "metadata": {},
   "source": [
    "#### Cell G â€” Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efa56b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/20 | train loss 4.8361 acc 0.008 | val loss 4.6972 acc 0.010\n",
      "  âžœ New best val acc=0.010 (model saved)\n",
      "Epoch 02/20 | train loss 4.7469 acc 0.007 | val loss 4.7297 acc 0.010\n",
      "Epoch 03/20 | train loss 4.7295 acc 0.008 | val loss 4.7156 acc 0.026\n",
      "  âžœ New best val acc=0.026 (model saved)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# === Cell G â€” Training Loop ===\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, epochs + \u001b[32m1\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     tr_loss, tr_acc = \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     va_loss, va_acc = run_epoch(val_loader,   train=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m           \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtrain loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m acc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m           \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mval loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mva_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m acc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mva_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mrun_epoch\u001b[39m\u001b[34m(loader, train)\u001b[39m\n\u001b[32m     14\u001b[39m total_n    = \u001b[32m0\u001b[39m\n\u001b[32m     16\u001b[39m opt.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# x is already [B, T, C, H, W] from WLASLDataset\u001b[39;49;00m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/ai-env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/ai-env/lib/python3.11/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/ai-env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/ai-env/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mWLASLDataset.__getitem__\u001b[39m\u001b[34m(self, i)\u001b[39m\n\u001b[32m     65\u001b[39m path = row[\u001b[33m\"\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     66\u001b[39m label = \u001b[38;5;28mint\u001b[39m(row[\u001b[33m\"\u001b[39m\u001b[33mlabel_new\u001b[39m\u001b[33m\"\u001b[39m])   \u001b[38;5;66;03m# contiguous 0..C-1\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_safe_load_clip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x, label, path\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mWLASLDataset._safe_load_clip\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mno frames\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     50\u001b[39m idxs = uniform_temporal_indices(n, \u001b[38;5;28mself\u001b[39m.clip_len, \u001b[38;5;28mself\u001b[39m.stride)\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m batch = \u001b[43mvr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43midxs\u001b[49m\u001b[43m)\u001b[49m        \u001b[38;5;66;03m# [T,H,W,C]\u001b[39;00m\n\u001b[32m     52\u001b[39m x = batch.float() / \u001b[32m255.0\u001b[39m\n\u001b[32m     53\u001b[39m x = x.permute(\u001b[32m0\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)         \u001b[38;5;66;03m# [T,C,H,W]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/ai-env/lib/python3.11/site-packages/decord/video_reader.py:175\u001b[39m, in \u001b[36mVideoReader.get_batch\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    174\u001b[39m indices = _nd.array(\u001b[38;5;28mself\u001b[39m._validate_indices(indices))\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m arr = \u001b[43m_CAPI_VideoReaderGetBatch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m bridge_out(arr)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/ai-env/lib/python3.11/site-packages/decord/_ffi/_ctypes/function.py:173\u001b[39m, in \u001b[36mFunctionBase.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    171\u001b[39m ret_val = DECORDValue()\n\u001b[32m    172\u001b[39m ret_tcode = ctypes.c_int()\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m check_call(\u001b[43m_LIB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDECORDFuncCall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtcodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mret_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mret_tcode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    176\u001b[39m _ = temp_args\n\u001b[32m    177\u001b[39m _ = args\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# === Cell G â€” Training Loop ===\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    tr_loss, tr_acc = run_epoch(train_loader, train=True)\n",
    "    va_loss, va_acc = run_epoch(val_loader,   train=False)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}/{epochs} | \"\n",
    "          f\"train loss {tr_loss:.4f} acc {tr_acc:.3f} | \"\n",
    "          f\"val loss {va_loss:.4f} acc {va_acc:.3f}\")\n",
    "\n",
    "    if va_acc > best_val_acc:\n",
    "        best_val_acc = va_acc\n",
    "        torch.save(model.state_dict(), \"best_cnn_bigru_top104.pt\")\n",
    "        print(f\"  âžœ New best val acc={best_val_acc:.3f} (model saved)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6789f1a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
