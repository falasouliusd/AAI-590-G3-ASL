{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bffc2c45-6c3b-406e-9409-afb8b83a3648",
   "metadata": {},
   "source": [
    "### Cell A — Setup & Config (paths, knobs, seeding)\n",
    "\n",
    "clip_len=32 / stride=2: good temporal coverage without blowing VRAM.\n",
    "\n",
    "r3d18_k400: strong pretrained baseline; we’ll fine-tune.\n",
    "\n",
    "staged unfreeze: learn head → then gently unfreeze layer4 to reduce overfitting/instability.\n",
    "\n",
    "AMP: speedup + memory savings; using the correct torch.amp.autocast('cuda') API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfd10b8c-08f3-4ef2-ab09-bca9df7e3851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root: /home/falasoul/notebooks/USD/AAI-590/Capstone/AAI-590-G3-ASL\n",
      "manifest: /home/falasoul/notebooks/USD/AAI-590/Capstone/AAI-590-G3-ASL/data/wlasl_preprocessed/manifest_wlasl300.csv\n",
      "checkpoints: /home/falasoul/notebooks/USD/AAI-590/Capstone/AAI-590-G3-ASL/checkpoints\n",
      "runs: /home/falasoul/notebooks/USD/AAI-590/Capstone/AAI-590-G3-ASL/runs\n",
      "run_tag: wlasl300_r3d18_k400_T32_S2_B8_20251110T052611Z\n"
     ]
    }
   ],
   "source": [
    "# === Cell A — Config / Paths / Globals (v2: larger subsets + aug/ema/clipping) ===\n",
    "from pathlib import Path\n",
    "import yaml, json, os\n",
    "from datetime import datetime\n",
    "\n",
    "# --- project root & src on sys.path (if you need your modules) ---\n",
    "root = Path(\"..\").resolve()                 # notebook sits at repo root\n",
    "CONFIG_YAML = root / \"configs\" / \"wlasl.yaml\"  # generic name; optional\n",
    "\n",
    "# --- load your existing yaml (optional; we still set overrides below) ---\n",
    "CFG = yaml.safe_load(open(CONFIG_YAML, \"r\")) if CONFIG_YAML.exists() else {}\n",
    "\n",
    "# Choose the dataset size here. We assume you have prebuilt manifests:\n",
    "#  - data/wlasl_preprocessed/manifest_nslt100.csv\n",
    "#  - data/wlasl_preprocessed/manifest_wlasl300.csv\n",
    "#  - data/wlasl_preprocessed/manifest_wlasl1000.csv\n",
    "DATASET_SIZE = CFG.get(\"dataset_size\", \"wlasl300\")  # ['nslt100','wlasl300','wlasl1000']\n",
    "\n",
    "_manifest_map = {\n",
    "    \"nslt100\":   \"data/wlasl_preprocessed/manifest_nslt100.csv\",\n",
    "    \"wlasl300\":  \"data/wlasl_preprocessed/manifest_wlasl300.csv\",\n",
    "    \"wlasl1000\": \"data/wlasl_preprocessed/manifest_wlasl1000.csv\",\n",
    "}\n",
    "_manifest_rel = _manifest_map.get(DATASET_SIZE, _manifest_map[\"wlasl300\"])\n",
    "\n",
    "# --- core run configuration (overrides are explicit here) ---\n",
    "CONFIG = {\n",
    "    # data\n",
    "    \"dataset_size\": DATASET_SIZE,          # tracked in run tag\n",
    "    \"manifest\": _manifest_rel,\n",
    "    \"clip_len\": 32,                        # temporal length per clip\n",
    "    \"frame_stride\": 2,                     # per-frame step\n",
    "    \"batch_size\": 8,\n",
    "    \"num_workers\": 4,\n",
    "    \"use_weighted_sampler\": True,          # class rebalancing\n",
    "    \"normalize\": \"kinetics\",               # ['kinetics','none']\n",
    "\n",
    "    # ✅ augmentation knobs (applied in Data cell):\n",
    "    # safe for sign language (NO horizontal flip)\n",
    "    \"aug\": {\n",
    "        \"random_resized_crop_scale\": [0.7, 1.0],   # spatial crop scale range\n",
    "        \"color_jitter\": [0.2, 0.2, 0.2, 0.05],     # brightness, contrast, saturation, hue\n",
    "        \"gaussian_noise_std\": 0.01,                # mild sensor noise\n",
    "        \"random_erasing_p\": 0.15,                  # small patch erase\n",
    "        \"temporal_jitter\": True,                   # random start offset within segment\n",
    "        \"temporal_shuffle_small\": True,            # tiny local shuffle (keeps semantics)\n",
    "        \"spatial_smooth_blur_p\": 0.10,             # occasional light blur\n",
    "    },\n",
    "\n",
    "    # model & training\n",
    "    \"backbone\": \"r3d18_k400\",              # ['r3d18_k400','c3dlite_gn']\n",
    "    \"dropout\": 0.2,\n",
    "    \"label_smoothing\": 0.05,               # a touch can help with larger sets\n",
    "    \"epochs\": 30,                          # a little longer for bigger data\n",
    "    \"warmup_epochs\": 2,\n",
    "\n",
    "    # staged LR (head -> partial backbone)\n",
    "    \"lr_head_stage1\": 1e-3,                # head-only stage\n",
    "    \"lr_back_stage1\": 0.0,\n",
    "    \"lr_head_stage2\": 1e-4,                # unfreeze last block\n",
    "    \"lr_back_stage2\": 1e-5,\n",
    "\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"cosine_eta_min\": 1e-5,\n",
    "\n",
    "    # extras that improve stability\n",
    "    \"amp\": True,                           # mixed precision\n",
    "    \"compile\": True,                       # torch.compile if available\n",
    "    \"freeze_backbone\": True,\n",
    "    \"unfreeze_at_epoch\": 5,\n",
    "    \"unfreeze_scope\": \"layer4\",\n",
    "    \"grad_clip_norm\": 1.0,                 # gradient clipping\n",
    "    \"ema\": True,                           # Exponential Moving Average of weights\n",
    "    \"ema_decay\": 0.999,\n",
    "\n",
    "    # optional regularizers (wired later; keep off by default)\n",
    "    \"mixup_alpha\": 0.0,                    # 0 disables\n",
    "    \"cutmix_alpha\": 0.0,                   # 0 disables\n",
    "\n",
    "    # checkpoints / runs\n",
    "    \"checkpoints_dir\": \"checkpoints\",\n",
    "    \"runs_dir\": \"runs\",\n",
    "\n",
    "    # reproducibility\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "# derived paths\n",
    "MANIFEST = root / CONFIG[\"manifest\"]\n",
    "CKPT_DIR = root / CONFIG[\"checkpoints_dir\"]\n",
    "RUNS_DIR = root / CONFIG[\"runs_dir\"]\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# run tag (deterministic name for files)\n",
    "run_tag = (\n",
    "    f\"{CONFIG['dataset_size']}_{CONFIG['backbone']}\"\n",
    "    f\"_T{CONFIG['clip_len']}_S{CONFIG['frame_stride']}_B{CONFIG['batch_size']}_\"\n",
    "    f\"{datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}\"\n",
    ")\n",
    "\n",
    "# persist run config for traceability\n",
    "with open(RUNS_DIR / f\"{run_tag}.config.json\", \"w\") as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "print(\"root:\", root)\n",
    "print(\"manifest:\", MANIFEST)\n",
    "print(\"checkpoints:\", CKPT_DIR)\n",
    "print(\"runs:\", RUNS_DIR)\n",
    "print(\"run_tag:\", run_tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab5b390e-41a2-4770-946f-50e1c1813774",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG.update({\n",
    "    \"batch_size\": 16,          # try 16; fall back to 8 + accum_steps=2 if OOM\n",
    "    \"accum_steps\": 1,          # set to 2 if you keep batch_size=8\n",
    "    \"epochs\": 50,              # more steps for 300 classes\n",
    "    \"mixup_alpha\": 0.2,        # helps with many classes + few clips\n",
    "    \"cutmix_alpha\": 0.0,       # keep off for sign\n",
    "    \"grad_clip_norm\": 1.0,     # stability\n",
    "    \"ema\": True,\n",
    "    \"ema_decay\": 0.999,\n",
    "    \"lr_back_stage2\": 3e-5,    # slightly faster backbone adaptation after unfreeze\n",
    "    \"warmup_epochs\": 3,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "026d152d-ca19-4bec-a497-a5cef70ec02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cell B0 manifest builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7258a6fa-527b-4475-ab39-72f98e089ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current notebook dir: /home/falasoul/notebooks/USD/AAI-590/Capstone/AAI-590-G3-ASL/notebooks\n",
      "root = /home/falasoul/notebooks/USD/AAI-590/Capstone/AAI-590-G3-ASL\n",
      "exists: True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "print(\"Current notebook dir:\", Path.cwd())\n",
    "print(\"root =\", Path(\"..\").resolve())\n",
    "print(\"exists:\", (Path(\"..\").resolve() / \"data\" / \"wlasl_preprocessed\" / \"nslt_300.json\").exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c454cd8c-3027-4e8b-b542-1cff03e614fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ nslt_300.json not found or ambiguous; will approximate 300 classes by taking the first 300 glosses present in WLASL_v0.3.json after file matching.\n",
      "[_manifest_tmp_full.csv]  Splits | train=8313  val=2253  test=1414  | classes=2000\n",
      "  Example: data/wlasl_preprocessed/videos/69241.mp4  gloss=book  label=210\n",
      "  Note: 9103 video ids not found in videos. Logged to _manifest_tmp_full.missing_video_ids.txt.\n",
      "[manifest_wlasl300.csv]  Splits | train=1271  val=355  test=228  | classes=300\n",
      "⚠️ nslt_1000.json not found or ambiguous; approximating 1000 classes similarly.\n",
      "[_manifest_tmp_full.csv]  Splits | train=8313  val=2253  test=1414  | classes=2000\n",
      "  Example: data/wlasl_preprocessed/videos/69241.mp4  gloss=book  label=210\n",
      "  Note: 9103 video ids not found in videos. Logged to _manifest_tmp_full.missing_video_ids.txt.\n",
      "[manifest_wlasl1000.csv]  Splits | train=4205  val=1136  test=734  | classes=1000\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# === Cell B0 — Build larger WLASL manifests (300 / 1000) from WLASL_v0.3.json ===\n",
    "from pathlib import Path\n",
    "import json, re\n",
    "import pandas as pd\n",
    "\n",
    "root = Path(\"..\").resolve()\n",
    "droot = root / \"data\" / \"wlasl_preprocessed\"\n",
    "videos_dir = droot / \"videos\"\n",
    "\n",
    "# Inputs we actually have\n",
    "wlasl_json = droot / \"WLASL_v0.3.json\"   # canonical annotations file\n",
    "json_300   = droot / \"nslt_300.json\"     # optional: to select top-300 glosses\n",
    "json_1000  = droot / \"nslt_1000.json\"    # optional: to select top-1000 glosses\n",
    "\n",
    "# Outputs\n",
    "out_300    = droot / \"manifest_wlasl300.csv\"\n",
    "out_1000   = droot / \"manifest_wlasl1000.csv\"\n",
    "\n",
    "def _load_json(path: Path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def _list_glosses_from_subset(path: Path):\n",
    "    \"\"\"\n",
    "    Returns a set of gloss names from nslt_*.json, regardless of whether it’s a list or dict-style file.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    data = _load_json(path)\n",
    "    glosses = set()\n",
    "\n",
    "    if isinstance(data, list):\n",
    "        # common format: list of {\"gloss\": \"...\", \"instances\": [...]}\n",
    "        for item in data:\n",
    "            g = item.get(\"gloss\") or item.get(\"label\") or item.get(\"category\")\n",
    "            if g:\n",
    "                glosses.add(str(g))\n",
    "    elif isinstance(data, dict):\n",
    "        # sometimes: {\"database\": { \"<id>\": {\"gloss\": \"...\", ...}, ...}}\n",
    "        if \"database\" in data and isinstance(data[\"database\"], dict):\n",
    "            for _, v in data[\"database\"].items():\n",
    "                g = v.get(\"gloss\") or v.get(\"label\") or v.get(\"category\")\n",
    "                if g:\n",
    "                    glosses.add(str(g))\n",
    "        else:\n",
    "            # fallback: any top-level dict entries with \"gloss\"\n",
    "            for _, v in data.items():\n",
    "                if isinstance(v, dict):\n",
    "                    g = v.get(\"gloss\") or v.get(\"label\") or v.get(\"category\")\n",
    "                    if g:\n",
    "                        glosses.add(str(g))\n",
    "    return glosses if glosses else None\n",
    "\n",
    "def _std_split_name(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    s = str(x).lower()\n",
    "    if s.startswith(\"train\"):\n",
    "        return \"train\"\n",
    "    if s.startswith(\"val\") or s in (\"dev\", \"validation\"):\n",
    "        return \"val\"\n",
    "    if s.startswith(\"test\"):\n",
    "        return \"test\"\n",
    "    return None\n",
    "\n",
    "def _video_path_for_id(vid: str):\n",
    "    \"\"\"\n",
    "    Try common filename patterns inside videos_dir for a given video_id.\n",
    "    \"\"\"\n",
    "    # clean id\n",
    "    v = str(vid).split(\"/\")[-1]\n",
    "    v = re.sub(r\"\\.mp4$\", \"\", v, flags=re.I)\n",
    "    v = re.sub(r\"[^A-Za-z0-9_\\-]\", \"\", v)\n",
    "\n",
    "    candidates = [\n",
    "        videos_dir / f\"{v}.mp4\",\n",
    "        videos_dir / f\"{v}.MP4\",\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if p.exists():\n",
    "            return p\n",
    "\n",
    "    # glob fallback (case-insensitive-ish via two patterns)\n",
    "    hits = list(videos_dir.glob(f\"*{v}*.mp4\")) + list(videos_dir.glob(f\"*{v}*.MP4\"))\n",
    "    return hits[0] if hits else None\n",
    "\n",
    "def _build_from_wlasl_json(limit_glosses: set | None, out_csv: Path):\n",
    "    if not wlasl_json.exists():\n",
    "        raise FileNotFoundError(f\"Missing {wlasl_json}\")\n",
    "\n",
    "    data = _load_json(wlasl_json)\n",
    "    if not isinstance(data, list):\n",
    "        raise ValueError(f\"Unexpected schema in {wlasl_json.name}: expected a list of items.\")\n",
    "\n",
    "    rows = []\n",
    "    missing_videos = []\n",
    "\n",
    "    for item in data:\n",
    "        gloss = item.get(\"gloss\") or item.get(\"label\") or item.get(\"category\")\n",
    "        if not gloss:\n",
    "            continue\n",
    "        if limit_glosses is not None and gloss not in limit_glosses:\n",
    "            continue\n",
    "\n",
    "        instances = item.get(\"instances\") or item.get(\"clips\") or []\n",
    "        for ins in instances:\n",
    "            split = _std_split_name(ins.get(\"subset\") or ins.get(\"split\") or ins.get(\"set\"))\n",
    "            if not split:\n",
    "                continue\n",
    "\n",
    "            # video id can be under different keys; WLASL v0.3 uses 'video_id'\n",
    "            vid = ins.get(\"video_id\") or ins.get(\"id\") or ins.get(\"name\") or ins.get(\"url\") or ins.get(\"path\")\n",
    "            if not vid:\n",
    "                continue\n",
    "\n",
    "            p = _video_path_for_id(vid)\n",
    "            if p is None:\n",
    "                missing_videos.append(str(vid))\n",
    "                continue\n",
    "\n",
    "            rows.append({\n",
    "                \"path\": str(p.relative_to(root)),\n",
    "                \"gloss\": str(gloss),\n",
    "                \"split\": split,\n",
    "            })\n",
    "\n",
    "    if not rows:\n",
    "        raise RuntimeError(\"No rows produced. Check that your videos/ names match the IDs in WLASL_v0.3.json.\")\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Map gloss -> label 0..C-1 (sorted for determinism)\n",
    "    glosses = sorted(df[\"gloss\"].unique())\n",
    "    g2id = {g:i for i,g in enumerate(glosses)}\n",
    "    df[\"label\"] = df[\"gloss\"].map(g2id).astype(int)\n",
    "\n",
    "    # Reorder to schema: path, gloss, label, split\n",
    "    df = df[[\"path\",\"gloss\",\"label\",\"split\"]]\n",
    "\n",
    "    # Drop rows whose file disappeared (safety)\n",
    "    df = df[df[\"path\"].map(lambda p: (root / p).exists())].reset_index(drop=True)\n",
    "\n",
    "    # Basic stats\n",
    "    n_train = (df[\"split\"]==\"train\").sum()\n",
    "    n_val   = (df[\"split\"]==\"val\").sum()\n",
    "    n_test  = (df[\"split\"]==\"test\").sum()\n",
    "    n_cls   = df[\"label\"].nunique()\n",
    "\n",
    "    print(f\"[{out_csv.name}]  Splits | train={n_train}  val={n_val}  test={n_test}  | classes={n_cls}\")\n",
    "    if len(df):\n",
    "        print(f\"  Example: {df.iloc[0]['path']}  gloss={df.iloc[0]['gloss']}  label={df.iloc[0]['label']}\")\n",
    "\n",
    "    # Save manifest\n",
    "    df.to_csv(out_csv, index=False)\n",
    "\n",
    "    # Save a small report of missing videos\n",
    "    miss_path = out_csv.with_suffix(\".missing_video_ids.txt\")\n",
    "    if missing_videos:\n",
    "        with open(miss_path, \"w\") as f:\n",
    "            for v in sorted(set(missing_videos)):\n",
    "                f.write(str(v) + \"\\n\")\n",
    "        print(f\"  Note: {len(set(missing_videos))} video ids not found in {videos_dir.name}. Logged to {miss_path.name}.\")\n",
    "    else:\n",
    "        if miss_path.exists():\n",
    "            miss_path.unlink()\n",
    "\n",
    "    return df\n",
    "\n",
    "# ---- Build 300 ----\n",
    "gloss300 = _list_glosses_from_subset(json_300)\n",
    "if gloss300 is None:\n",
    "    print(\"⚠️ nslt_300.json not found or ambiguous; will approximate 300 classes by taking the first 300 glosses present in WLASL_v0.3.json after file matching.\")\n",
    "    # Build full → then clip to 300 classes deterministically\n",
    "    full_tmp = droot / \"_manifest_tmp_full.csv\"\n",
    "    df_full = _build_from_wlasl_json(limit_glosses=None, out_csv=full_tmp)\n",
    "    keep = sorted(df_full[\"gloss\"].unique())[:300]\n",
    "    df_300 = df_full[df_full[\"gloss\"].isin(keep)].reset_index(drop=True)\n",
    "    # remap labels 0..C-1 for this subset\n",
    "    glosses = sorted(df_300[\"gloss\"].unique())\n",
    "    g2id = {g:i for i,g in enumerate(glosses)}\n",
    "    df_300[\"label\"] = df_300[\"gloss\"].map(g2id).astype(int)\n",
    "    df_300 = df_300[[\"path\",\"gloss\",\"label\",\"split\"]]\n",
    "    df_300.to_csv(out_300, index=False)\n",
    "    if full_tmp.exists(): full_tmp.unlink()\n",
    "    print(f\"[{out_300.name}]  Splits | train={(df_300['split']=='train').sum()}  val={(df_300['split']=='val').sum()}  test={(df_300['split']=='test').sum()}  | classes={df_300['label'].nunique()}\")\n",
    "else:\n",
    "    _build_from_wlasl_json(limit_glosses=gloss300, out_csv=out_300)\n",
    "\n",
    "# ---- Build 1000 ----\n",
    "gloss1000 = _list_glosses_from_subset(json_1000)\n",
    "if gloss1000 is None:\n",
    "    print(\"⚠️ nslt_1000.json not found or ambiguous; approximating 1000 classes similarly.\")\n",
    "    # Build full → then clip to 1000 classes deterministically\n",
    "    full_tmp = droot / \"_manifest_tmp_full.csv\"\n",
    "    df_full = _build_from_wlasl_json(limit_glosses=None, out_csv=full_tmp)\n",
    "    keep = sorted(df_full[\"gloss\"].unique())[:1000]\n",
    "    df_1000 = df_full[df_full[\"gloss\"].isin(keep)].reset_index(drop=True)\n",
    "    glosses = sorted(df_1000[\"gloss\"].unique())\n",
    "    g2id = {g:i for i,g in enumerate(glosses)}\n",
    "    df_1000[\"label\"] = df_1000[\"gloss\"].map(g2id).astype(int)\n",
    "    df_1000 = df_1000[[\"path\",\"gloss\",\"label\",\"split\"]]\n",
    "    df_1000.to_csv(out_1000, index=False)\n",
    "    if full_tmp.exists(): full_tmp.unlink()\n",
    "    print(f\"[{out_1000.name}]  Splits | train={(df_1000['split']=='train').sum()}  val={(df_1000['split']=='val').sum()}  test={(df_1000['split']=='test').sum()}  | classes={df_1000['label'].nunique()}\")\n",
    "else:\n",
    "    _build_from_wlasl_json(limit_glosses=gloss1000, out_csv=out_1000)\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ddcd77-51aa-4cd7-b537-07790c14b4fb",
   "metadata": {},
   "source": [
    "### Cell B — Data Loader (self-contained, robust)\n",
    "A tiny inline Dataset avoids surprises from older src.data.* code.\n",
    "\n",
    "We normalize like Kinetics (what r3d_18 expects).\n",
    "\n",
    "We skip missing/corrupt paths up front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8f08dd6-941b-466d-9b7d-85ab6679a4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decord active ✅\n",
      "Splits | train=1271 val=355 test=228 | classes=300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[h264 @ 0x596a6d980200] Invalid NAL unit size (71678 > 10776).\n",
      "[h264 @ 0x596a6d980200] Error splitting the input into NAL units.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batch: torch.Size([16, 32, 3, 112, 112]) [148, 294, 247, 278, 161, 235, 203, 198]\n"
     ]
    }
   ],
   "source": [
    "# === Cell B — Data loader (video-safe augmentation; manifest-driven) ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torch, random\n",
    "\n",
    "# ---- seed everything ----\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed); torch.backends.cudnn.deterministic = True\n",
    "seed_everything(CONFIG[\"seed\"])\n",
    "\n",
    "# ---- try decord first (faster); fallback to torchvision.io ----\n",
    "USE_DECORD = False\n",
    "try:\n",
    "    import decord\n",
    "    # ✅ ensure FFmpeg messages are suppressed\n",
    "    import logging, os\n",
    "    logging.getLogger(\"decord\").setLevel(logging.ERROR)\n",
    "    os.environ[\"DECORD_LOG_LEVEL\"] = \"ERROR\"\n",
    "    os.environ[\"FFMPEG_LOG_LEVEL\"] = \"error\"\n",
    "\n",
    "    # ✅ set torch bridge (so frames come as torch tensors)\n",
    "    decord.bridge.set_bridge('torch')\n",
    "\n",
    "    USE_DECORD = True\n",
    "    print(\"Decord active ✅\")\n",
    "except Exception as e:\n",
    "    print(\"⚠️ Decord unavailable, using torchvision.io. Reason:\", e)\n",
    "    from torchvision.io import read_video\n",
    "    USE_DECORD = False\n",
    "\n",
    "# ---------- Video I/O ----------\n",
    "def _read_raw_video(path):\n",
    "    \"\"\"\n",
    "    Returns frames as torch.uint8 [T, H, W, C] (range 0..255) if possible,\n",
    "    else returns an empty tensor.\n",
    "    \"\"\"\n",
    "    if USE_DECORD:\n",
    "        try:\n",
    "            vr = decord.VideoReader(path)\n",
    "            total = len(vr)\n",
    "            if total == 0:\n",
    "                return torch.empty(0, dtype=torch.uint8)\n",
    "            frames = vr.get_batch(range(total)).byte()  # [T,H,W,C], uint8\n",
    "            return frames\n",
    "        except Exception:\n",
    "            return torch.empty(0, dtype=torch.uint8)\n",
    "    else:\n",
    "        try:\n",
    "            v, _, _ = read_video(path, pts_unit=\"sec\")  # [T,H,W,C], uint8\n",
    "            return v.to(torch.uint8)\n",
    "        except Exception:\n",
    "            return torch.empty(0, dtype=torch.uint8)\n",
    "\n",
    "# ---------- Normalization ----------\n",
    "def kinetics_normalize(x):\n",
    "    # x [T,C,H,W] float32 in [0,1]\n",
    "    mean = torch.tensor((0.432,0.394,0.376), dtype=x.dtype, device=x.device)[None,:,None,None]\n",
    "    std  = torch.tensor((0.228,0.221,0.223), dtype=x.dtype, device=x.device)[None,:,None,None]\n",
    "    return (x - mean) / std\n",
    "\n",
    "# ---------- Spatial ops (consistent across frames) ----------\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def _param_random_resized_crop(h, w, scale):\n",
    "    \"\"\"\n",
    "    Returns parameters for a consistent RandomResizedCrop across frames.\n",
    "    \"\"\"\n",
    "    # Try a few times to sample a valid crop; otherwise fallback to center\n",
    "    for _ in range(10):\n",
    "        target_area = random.uniform(*scale) * (h * w)\n",
    "        aspect = random.uniform(3/4, 4/3)\n",
    "        nh = int(round((target_area * aspect) ** 0.5))\n",
    "        nw = int(round((target_area / aspect) ** 0.5))\n",
    "        if 0 < nh <= h and 0 < nw <= w:\n",
    "            top = random.randint(0, h - nh) if h - nh > 0 else 0\n",
    "            left = random.randint(0, w - nw) if w - nw > 0 else 0\n",
    "            return top, left, nh, nw\n",
    "    # fallback: center crop (as large as possible)\n",
    "    short = min(h, w)\n",
    "    top = (h - short) // 2\n",
    "    left = (w - short) // 2\n",
    "    return top, left, short, short\n",
    "\n",
    "def _apply_consistent_spatial(x_TCHW, out_hw=(112,112), aug_cfg=None, train=True):\n",
    "    \"\"\"\n",
    "    x_TCHW: float32 [T,C,H,W] in [0,1]\n",
    "    Applies spatial transforms consistently across T.\n",
    "    \"\"\"\n",
    "    Tt, C, H, W = x_TCHW.shape\n",
    "    out_h, out_w = out_hw\n",
    "\n",
    "    if train and aug_cfg is not None:\n",
    "        # 1) RandomResizedCrop (consistent)\n",
    "        scale = tuple(aug_cfg.get(\"random_resized_crop_scale\", [1.0, 1.0]))\n",
    "        top, left, hh, ww = _param_random_resized_crop(H, W, scale)\n",
    "        x_TCHW = x_TCHW[:, :, top:top+hh, left:left+ww]\n",
    "        # 2) Resize to model resolution\n",
    "        x_TCHW = F.interpolate(x_TCHW, size=(out_h, out_w), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        # 3) Color jitter — single set of params for whole clip\n",
    "        # brightness, contrast, saturation, hue\n",
    "        bj, cj, sj, hj = aug_cfg.get(\"color_jitter\", [0.0, 0.0, 0.0, 0.0])\n",
    "        if any(v > 0 for v in [bj, cj, sj, hj]):\n",
    "            b = 1.0 + random.uniform(-bj, bj)\n",
    "            c = 1.0 + random.uniform(-cj, cj)\n",
    "            s = 1.0 + random.uniform(-sj, sj)\n",
    "            h = random.uniform(-hj, hj)\n",
    "            # Apply to each frame (consistent params)\n",
    "            x_TCHW = torch.stack([TF.adjust_hue(\n",
    "                                      TF.adjust_saturation(\n",
    "                                          TF.adjust_contrast(\n",
    "                                              TF.adjust_brightness(x_TCHW[t], b), c),\n",
    "                                          s),\n",
    "                                      h)\n",
    "                                  for t in range(Tt)], dim=0)\n",
    "\n",
    "        # 4) Occasional slight blur (helps sensor variance)\n",
    "        if random.random() < aug_cfg.get(\"spatial_smooth_blur_p\", 0.0):\n",
    "            # 3x3 gaussian-like box blur\n",
    "            kernel = torch.tensor([[1,2,1],[2,4,2],[1,2,1]], dtype=x_TCHW.dtype, device=x_TCHW.device)\n",
    "            kernel = kernel / kernel.sum()\n",
    "            kernel = kernel.view(1,1,3,3)\n",
    "            # depthwise per-channel conv\n",
    "            x_TCHW = F.conv2d(x_TCHW.reshape(Tt*C,1,out_h,out_w), kernel, padding=1).reshape(Tt,C,out_h,out_w)\n",
    "\n",
    "        # 5) Random erasing on a random frame (small patch)\n",
    "        if random.random() < aug_cfg.get(\"random_erasing_p\", 0.0):\n",
    "            t = random.randrange(Tt)\n",
    "            area = out_h * out_w\n",
    "            erase_area = random.uniform(0.02, 0.1) * area\n",
    "            aspect = random.uniform(0.3, 3.3)\n",
    "            eh = int(round((erase_area * aspect) ** 0.5))\n",
    "            ew = int(round((erase_area / aspect) ** 0.5))\n",
    "            if eh < out_h and ew < out_w:\n",
    "                ty = random.randint(0, out_h - eh)\n",
    "                tx = random.randint(0, out_w - ew)\n",
    "                x_TCHW[t, :, ty:ty+eh, tx:tx+ew] = 0.0\n",
    "\n",
    "    else:\n",
    "        # eval path: center crop then resize\n",
    "        short = min(H, W)\n",
    "        top = (H - short) // 2\n",
    "        left = (W - short) // 2\n",
    "        x_TCHW = x_TCHW[:, :, top:top+short, left:left+short]\n",
    "        x_TCHW = F.interpolate(x_TCHW, size=(out_h, out_w), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    return x_TCHW\n",
    "\n",
    "# ---------- Temporal ops ----------\n",
    "def _sample_indices(total, Treq, stride, temporal_jitter=True):\n",
    "    \"\"\"\n",
    "    Returns frame indices length Treq with given stride; loops if short.\n",
    "    If temporal_jitter: randomize starting offset within feasible window.\n",
    "    \"\"\"\n",
    "    if total <= 0:\n",
    "        return np.zeros(Treq, dtype=np.int64)\n",
    "\n",
    "    span = (Treq - 1) * stride + 1\n",
    "    if total >= span:\n",
    "        max_start = total - span\n",
    "        start = random.randint(0, max_start) if temporal_jitter else max_start // 2\n",
    "        idxs = start + np.arange(0, Treq * stride, stride)\n",
    "    else:\n",
    "        # loop if too short\n",
    "        base = np.arange(0, Treq * stride, stride) % total\n",
    "        if temporal_jitter and total > 1:\n",
    "            jitter = random.randint(0, total-1)\n",
    "            base = (base + jitter) % total\n",
    "        idxs = base\n",
    "    return idxs.astype(np.int64)\n",
    "\n",
    "def _tiny_temporal_shuffle(idxs, prob=0.15):\n",
    "    \"\"\"\n",
    "    Small local shuffle: with given prob, swap a few adjacent positions.\n",
    "    Preserves global order cues while adding robustness.\n",
    "    \"\"\"\n",
    "    idxs = idxs.copy()\n",
    "    if prob <= 0: \n",
    "        return idxs\n",
    "    Tn = len(idxs)\n",
    "    for i in range(Tn - 1):\n",
    "        if random.random() < prob * 0.15:  # keep very mild\n",
    "            idxs[i], idxs[i+1] = idxs[i+1], idxs[i]\n",
    "    return idxs\n",
    "\n",
    "# ---------- Clip reader w/ augmentation ----------\n",
    "def read_clip(path, clip_len, stride, size=(112,112), train=False, aug_cfg=None, normalize=\"kinetics\"):\n",
    "    \"\"\"\n",
    "    Returns a Tensor [T, C, H, W] in float32.\n",
    "    - Samples T=clip_len frames with step=stride (+temporal jitter if train).\n",
    "    - Applies consistent spatial aug across frames (train only).\n",
    "    - Optional tiny temporal shuffle (train only).\n",
    "    - Kinetics normalization (by default).\n",
    "    \"\"\"\n",
    "    raw = _read_raw_video(path)  # [T,H,W,C] uint8\n",
    "    if raw.numel() == 0:\n",
    "        # fallback: blank clip\n",
    "        x = torch.zeros(clip_len, 3, size[0], size[1], dtype=torch.float32)\n",
    "        return x\n",
    "\n",
    "    total = raw.shape[0]\n",
    "    tids = _sample_indices(total, clip_len, stride, temporal_jitter=train and aug_cfg.get(\"temporal_jitter\", True))\n",
    "    if train and aug_cfg.get(\"temporal_shuffle_small\", True):\n",
    "        tids = _tiny_temporal_shuffle(tids, prob=0.15)\n",
    "\n",
    "    # gather frames, to float [0,1] and permute to T,C,H,W\n",
    "    frames = raw[tids].float() / 255.0           # [T,H,W,C] -> float\n",
    "    frames = frames.permute(0,3,1,2).contiguous()# -> [T,C,H,W]\n",
    "\n",
    "    # consistent spatial aug + resize\n",
    "    frames = _apply_consistent_spatial(frames, out_hw=size, aug_cfg=aug_cfg, train=train)\n",
    "\n",
    "    # mild gaussian noise (train only)\n",
    "    if train and aug_cfg.get(\"gaussian_noise_std\", 0.0) > 0:\n",
    "        std = aug_cfg[\"gaussian_noise_std\"]\n",
    "        noise = torch.randn_like(frames) * std\n",
    "        frames = torch.clamp(frames + noise, 0.0, 1.0)\n",
    "\n",
    "    # normalize\n",
    "    if normalize == \"kinetics\":\n",
    "        frames = kinetics_normalize(frames)\n",
    "\n",
    "    return frames  # [T,C,H,W]\n",
    "\n",
    "# ---------- Dataset ----------\n",
    "class WLASLFromManifest(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset reading WLASL manifest with columns: path|video_path, gloss, label, split\n",
    "    Returns: X [T,C,H,W], y (int), meta (dict)\n",
    "    \"\"\"\n",
    "    def __init__(self, df, clip_len=32, stride=2, train=False, normalize=\"kinetics\", aug_cfg=None):\n",
    "        self.df = df.reset_index(drop=True).copy()\n",
    "        self.clip_len = clip_len\n",
    "        self.stride = stride\n",
    "        self.train = train\n",
    "        self.normalize = normalize\n",
    "        self.aug_cfg = aug_cfg or {}\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        p = row.get(\"path\", row.get(\"video_path\"))\n",
    "        x = read_clip(\n",
    "            p, \n",
    "            self.clip_len, \n",
    "            self.stride, \n",
    "            size=(112,112), \n",
    "            train=self.train, \n",
    "            aug_cfg=self.aug_cfg if self.train else {},\n",
    "            normalize=self.normalize\n",
    "        )\n",
    "        y = int(row[\"label\"])\n",
    "        return x, torch.tensor(y, dtype=torch.long), {\"path\": p}\n",
    "\n",
    "# ---------- Read manifest & harmonize paths ----------\n",
    "m = pd.read_csv(MANIFEST)\n",
    "if \"video_path\" in m.columns and \"path\" not in m.columns:\n",
    "    m = m.rename(columns={\"video_path\": \"path\"})\n",
    "m[\"path\"]  = m[\"path\"].apply(lambda p: str((root / p).resolve()))\n",
    "m[\"label\"] = m[\"label\"].astype(int)\n",
    "\n",
    "# drop missing to avoid reader errors\n",
    "exists_mask = m[\"path\"].map(lambda p: Path(p).exists())\n",
    "missing = (~exists_mask).sum()\n",
    "if missing:\n",
    "    print(f\"[Data] Skipping {missing} missing/corrupt videos.\")\n",
    "    m = m[exists_mask].reset_index(drop=True)\n",
    "\n",
    "# splits\n",
    "train_df = m[m.split==\"train\"].reset_index(drop=True)\n",
    "val_df   = m[m.split==\"val\"].reset_index(drop=True)\n",
    "test_df  = m[m.split==\"test\"].reset_index(drop=True)\n",
    "num_classes = m[\"label\"].nunique()\n",
    "\n",
    "# datasets (train uses augmentation)\n",
    "train_ds = WLASLFromManifest(\n",
    "    train_df,\n",
    "    clip_len=CONFIG[\"clip_len\"],\n",
    "    stride=CONFIG[\"frame_stride\"],\n",
    "    train=True,\n",
    "    normalize=CONFIG[\"normalize\"],\n",
    "    aug_cfg=CONFIG.get(\"aug\", {})\n",
    ")\n",
    "val_ds   = WLASLFromManifest(\n",
    "    val_df,\n",
    "    clip_len=CONFIG[\"clip_len\"],\n",
    "    stride=CONFIG[\"frame_stride\"],\n",
    "    train=False,\n",
    "    normalize=CONFIG[\"normalize\"]\n",
    ")\n",
    "test_ds  = WLASLFromManifest(\n",
    "    test_df,\n",
    "    clip_len=CONFIG[\"clip_len\"],\n",
    "    stride=CONFIG[\"frame_stride\"],\n",
    "    train=False,\n",
    "    normalize=CONFIG[\"normalize\"]\n",
    ")\n",
    "\n",
    "# loaders\n",
    "bs, nw = CONFIG[\"batch_size\"], CONFIG[\"num_workers\"]\n",
    "if CONFIG[\"use_weighted_sampler\"]:\n",
    "    counts  = train_df[\"label\"].value_counts().to_dict()\n",
    "    weights = train_df[\"label\"].map(lambda y: 1.0 / counts[y]).values.astype(\"float32\")\n",
    "    sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
    "    train_loader = DataLoader(train_ds, batch_size=bs, sampler=sampler, num_workers=nw, pin_memory=True)\n",
    "else:\n",
    "    train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True, num_workers=nw, pin_memory=True)\n",
    "\n",
    "val_loader  = DataLoader(val_ds,  batch_size=bs, shuffle=False, num_workers=nw, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=bs, shuffle=False, num_workers=nw, pin_memory=True)\n",
    "\n",
    "print(f\"Splits | train={len(train_ds)} val={len(val_ds)} test={len(test_ds)} | classes={num_classes}\")\n",
    "\n",
    "# quick sanity batch\n",
    "xb, yb, mb = next(iter(train_loader))\n",
    "print(\"train batch:\", xb.shape, yb[:min(8,len(yb))].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a41476-28aa-4095-8a52-95be86198d40",
   "metadata": {},
   "source": [
    "### Cell C — Training (staged unfreeze, AMP fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c000e8-b284-4903-b851-91298e3e7dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell C — Training (EMA + grad clipping + optional mixup/cutmix) ===\n",
    "import torch, torch.nn as nn, numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# ---- model ----\n",
    "from torchvision.models.video import r3d_18, R3D_18_Weights\n",
    "\n",
    "if CONFIG[\"backbone\"] == \"r3d18_k400\":\n",
    "    weights = R3D_18_Weights.KINETICS400_V1\n",
    "    model = r3d_18(weights=weights)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "else:\n",
    "    class C3DliteGN(nn.Module):\n",
    "        def __init__(self, num_classes=100, drop=0.5):\n",
    "            super().__init__()\n",
    "            def gn(c): return nn.GroupNorm(num_groups=8, num_channels=c)\n",
    "            def block(cin, cout, pool_t=2):\n",
    "                return nn.Sequential(\n",
    "                    nn.Conv3d(cin, cout, 3, padding=1, bias=False),\n",
    "                    gn(cout), nn.ReLU(inplace=True),\n",
    "                    nn.MaxPool3d(kernel_size=(pool_t,2,2), stride=(pool_t,2,2))\n",
    "                )\n",
    "            self.stem = nn.Sequential(nn.Conv3d(3,32,3,padding=1,bias=False), gn(32), nn.ReLU(inplace=True))\n",
    "            self.b1 = block(32,  64); self.b2 = block(64, 128)\n",
    "            self.b3 = block(128, 256); self.b4 = block(256, 256)\n",
    "            self.head = nn.Sequential(nn.AdaptiveAvgPool3d(1), nn.Flatten(),\n",
    "                                      nn.Dropout(CONFIG[\"dropout\"]), nn.Linear(256, num_classes))\n",
    "        def forward(self, x):              # x: [B,T,C,H,W]\n",
    "            x = x.permute(0,2,1,3,4).contiguous()  # -> [B,C,T,H,W]\n",
    "            x = self.stem(x); x = self.b1(x); x = self.b2(x); x = self.b3(x); x = self.b4(x)\n",
    "            return self.head(x)\n",
    "    model = C3DliteGN(num_classes=num_classes, drop=CONFIG[\"dropout\"])\n",
    "\n",
    "model.to(device)\n",
    "EXPECTS_BCTHW = (CONFIG[\"backbone\"] == \"r3d18_k400\")\n",
    "\n",
    "# ---- freezing (stage 1: head only) ----\n",
    "def set_frozen(module, frozen=True):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad = not frozen\n",
    "\n",
    "if CONFIG[\"freeze_backbone\"]:\n",
    "    set_frozen(model, True)\n",
    "    if hasattr(model, \"fc\"): set_frozen(model.fc, False)\n",
    "    scope = CONFIG[\"unfreeze_scope\"]\n",
    "    if hasattr(model, scope): set_frozen(getattr(model, scope), True)\n",
    "\n",
    "# ---- criterion / optim / sched / scaler ----\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=CONFIG[\"label_smoothing\"])\n",
    "\n",
    "head_params = list(model.fc.parameters()) if hasattr(model, \"fc\") else []\n",
    "head_ids = {id(p) for p in head_params}\n",
    "backbone_params = [p for p in model.parameters() if id(p) not in head_ids]\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    [{\"params\": head_params, \"lr\": CONFIG[\"lr_head_stage1\"]},\n",
    "     {\"params\": backbone_params, \"lr\": CONFIG[\"lr_back_stage1\"]}],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    ")\n",
    "\n",
    "scheduler = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=max(1, CONFIG[\"epochs\"] - CONFIG[\"warmup_epochs\"]),\n",
    "    eta_min=CONFIG[\"cosine_eta_min\"],\n",
    ")\n",
    "\n",
    "scaler = GradScaler(enabled=CONFIG[\"amp\"])\n",
    "\n",
    "# Optional compile\n",
    "if CONFIG.get(\"compile\", True) and hasattr(torch, \"compile\"):\n",
    "    try:\n",
    "        model = torch.compile(model)\n",
    "        print(\"torch.compile: ON\")\n",
    "    except Exception as e:\n",
    "        print(\"torch.compile skipped:\", str(e))\n",
    "\n",
    "# ---- EMA ----\n",
    "class ModelEMA:\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        m = model._orig_mod if hasattr(model, \"_orig_mod\") else model\n",
    "        for name, p in m.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                self.shadow[name] = p.detach().clone()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, model):\n",
    "        m = model._orig_mod if hasattr(model, \"_orig_mod\") else model\n",
    "        for name, p in m.named_parameters():\n",
    "            if not p.requires_grad: continue\n",
    "            s = self.shadow.get(name)\n",
    "            if s is None: self.shadow[name] = p.detach().clone()\n",
    "            else: s.mul_(self.decay).add_(p.detach(), alpha=1.0 - self.decay)\n",
    "\n",
    "    def store(self, model):\n",
    "        m = model._orig_mod if hasattr(model, \"_orig_mod\") else model\n",
    "        self.backup = {name: p.detach().clone() for name, p in m.named_parameters()}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def copy_to(self, model):\n",
    "        m = model._orig_mod if hasattr(model, \"_orig_mod\") else model\n",
    "        for name, p in m.named_parameters():\n",
    "            if name in self.shadow:\n",
    "                p.data.copy_(self.shadow[name].data)\n",
    "\n",
    "    def restore(self, model):\n",
    "        m = model._orig_mod if hasattr(model, \"_orig_mod\") else model\n",
    "        for name, p in m.named_parameters():\n",
    "            p.data.copy_(self.backup[name].data)\n",
    "\n",
    "ema = ModelEMA(model, decay=CONFIG.get(\"ema_decay\", 0.999)) if CONFIG.get(\"ema\", True) else None\n",
    "\n",
    "# ---- Mixup / Cutmix (off unless alphas > 0) ----\n",
    "mixup_alpha  = float(CONFIG.get(\"mixup_alpha\", 0.0))\n",
    "cutmix_alpha = float(CONFIG.get(\"cutmix_alpha\", 0.0))\n",
    "\n",
    "def rand_bbox(W, H, lam):\n",
    "    cut_w = int(W * (1 - lam) ** 0.5)\n",
    "    cut_h = int(H * (1 - lam) ** 0.5)\n",
    "    cx = np.random.randint(W); cy = np.random.randint(H)\n",
    "    x1 = np.clip(cx - cut_w // 2, 0, W); y1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    x2 = np.clip(cx + cut_w // 2, 0, W); y2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    return x1, y1, x2, y2\n",
    "\n",
    "def apply_mix(x, y):\n",
    "    has_mixup  = mixup_alpha  > 0.0\n",
    "    has_cutmix = cutmix_alpha > 0.0\n",
    "    if not (has_mixup or has_cutmix):\n",
    "        return x, y, y, 1.0, \"none\"\n",
    "    mode = \"mixup\" if (has_mixup and (not has_cutmix or np.random.rand() < 0.5)) else \"cutmix\"\n",
    "    if mode == \"mixup\":\n",
    "        lam = np.random.beta(mixup_alpha, mixup_alpha)\n",
    "        perm = torch.randperm(x.size(0), device=x.device)\n",
    "        x = lam * x + (1 - lam) * x[perm]\n",
    "        return x, y, y[perm], lam, \"mixup\"\n",
    "    else:\n",
    "        lam = np.random.beta(cutmix_alpha, cutmix_alpha)\n",
    "        B, T, C, H, W = x.shape\n",
    "        perm = torch.randperm(B, device=x.device); x_perm = x[perm]\n",
    "        x1, y1, x2, y2 = rand_bbox(W, H, lam)\n",
    "        x[:, :, :, y1:y2, x1:x2] = x_perm[:, :, :, y1:y2, x1:x2]\n",
    "        lam = 1.0 - ((x2 - x1) * (y2 - y1) / (W * H))\n",
    "        return x, y, y[perm], lam, \"cutmix\"\n",
    "\n",
    "def compute_loss(logits, y, y_b=None, lam=1.0, mode=\"none\"):\n",
    "    if mode == \"none\": return criterion(logits, y)\n",
    "    return lam * criterion(logits, y) + (1 - lam) * criterion(logits, y_b)\n",
    "\n",
    "def warmup_lr(optim, epoch, warmup_epochs, base_head, base_back):\n",
    "    if epoch < warmup_epochs:\n",
    "        s = float(epoch + 1) / max(1, warmup_epochs)\n",
    "        optim.param_groups[0][\"lr\"] = base_head * s\n",
    "        optim.param_groups[1][\"lr\"] = base_back * s\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def run_one_epoch(loader, train=True):\n",
    "    model.train(train)\n",
    "    total_loss = total_acc = total_n = 0\n",
    "    for xb, yb, _ in loader:\n",
    "        xb = xb.to(device, non_blocking=True)   # [B,T,C,H,W]\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "        if train:\n",
    "            xb, y_a, y_b, lam, mode = apply_mix(xb, yb)\n",
    "        else:\n",
    "            y_a, y_b, lam, mode = yb, yb, 1.0, \"none\"\n",
    "\n",
    "        if EXPECTS_BCTHW:\n",
    "            xb_in = xb.permute(0,2,1,3,4).contiguous()\n",
    "        else:\n",
    "            xb_in = xb\n",
    "\n",
    "        if train: optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(\"cuda\", enabled=CONFIG[\"amp\"]):\n",
    "            logits = model(xb_in)\n",
    "            loss = compute_loss(logits, y_a, y_b=y_b, lam=lam, mode=mode)\n",
    "\n",
    "        if train:\n",
    "            scaler.scale(loss).backward()\n",
    "            if CONFIG.get(\"grad_clip_norm\", 0) > 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), CONFIG[\"grad_clip_norm\"])\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            if ema is not None: ema.update(model)\n",
    "\n",
    "        bs = yb.size(0)\n",
    "        total_n += bs\n",
    "        total_loss += loss.item() * bs\n",
    "        total_acc  += (logits.argmax(1) == y_a).float().sum().item()\n",
    "\n",
    "    return total_loss / max(1,total_n), total_acc / max(1,total_n)\n",
    "\n",
    "best_val = -1.0\n",
    "best_path = CKPT_DIR / \"best.pt\"\n",
    "last_path = CKPT_DIR / \"last.pt\"\n",
    "\n",
    "print(f\"Start training for {CONFIG['epochs']} epochs on {device} | run: {run_tag}\")\n",
    "\n",
    "for epoch in range(CONFIG[\"epochs\"]):\n",
    "    warmed = warmup_lr(\n",
    "        optimizer, epoch, CONFIG[\"warmup_epochs\"],\n",
    "        CONFIG[\"lr_head_stage1\"], CONFIG[\"lr_back_stage1\"]\n",
    "    )\n",
    "    if not warmed:\n",
    "        scheduler.step()\n",
    "\n",
    "    if CONFIG[\"freeze_backbone\"] and epoch == CONFIG[\"unfreeze_at_epoch\"]:\n",
    "        scope = CONFIG[\"unfreeze_scope\"]\n",
    "        if hasattr(model, scope):\n",
    "            print(f\"Unfreezing {scope} at epoch {epoch}\")\n",
    "            set_frozen(getattr(model, scope), False)\n",
    "        optimizer.param_groups[0][\"lr\"] = CONFIG[\"lr_head_stage2\"]\n",
    "        optimizer.param_groups[1][\"lr\"] = CONFIG[\"lr_back_stage2\"]\n",
    "\n",
    "    tr_loss, tr_acc = run_one_epoch(train_loader, train=True)\n",
    "\n",
    "    # validate under EMA weights if enabled\n",
    "    if ema is not None:\n",
    "        ema.store(model); ema.copy_to(model)\n",
    "    va_loss, va_acc = run_one_epoch(val_loader, train=False)\n",
    "    if ema is not None:\n",
    "        ema.restore(model)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:03d}/{CONFIG['epochs']} | \"\n",
    "          f\"train loss {tr_loss:.4f} acc {tr_acc:.3f} | \"\n",
    "          f\"val loss {va_loss:.4f} acc {va_acc:.3f}\")\n",
    "\n",
    "    # save last + best (save EMA snapshot if enabled)\n",
    "    to_save = model\n",
    "    if ema is not None:\n",
    "        ema.store(model); ema.copy_to(model); to_save = model\n",
    "\n",
    "    unwrapped = to_save._orig_mod if hasattr(to_save, \"_orig_mod\") else to_save\n",
    "    state = {\n",
    "        \"model\": unwrapped.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "        \"val_acc\": va_acc,\n",
    "        \"config\": CONFIG,\n",
    "        \"run_tag\": run_tag,\n",
    "        \"num_classes\": num_classes,\n",
    "    }\n",
    "    torch.save(state, last_path)\n",
    "    if va_acc > best_val:\n",
    "        best_val = va_acc\n",
    "        torch.save(state, best_path)\n",
    "\n",
    "    if ema is not None:\n",
    "        ema.restore(model)\n",
    "\n",
    "print(f\"[DONE] Best val acc: {best_val:.3f} | saved to {best_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1109af-c59a-4967-a3cc-16353b6cec56",
   "metadata": {},
   "source": [
    "#### Cell D — Sanity & Throughput Diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1edc58-611e-461c-8c3e-dbabfbdd2e22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# === Cell D0 — Sanity & Throughput Diagnostics ===\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, time, math, threading, subprocess, shutil, sys, os\n",
    "\n",
    "root = Path(\"..\").resolve()\n",
    "MANIFEST = root / CONFIG[\"manifest\"]\n",
    "\n",
    "print(\"Manifest:\", MANIFEST)\n",
    "df = pd.read_csv(MANIFEST)\n",
    "\n",
    "# -------------------------------\n",
    "# 1) DATA HEALTH & BALANCE\n",
    "# -------------------------------\n",
    "def split_counts(df):\n",
    "    return {s: int((df[\"split\"]==s).sum()) for s in [\"train\",\"val\",\"test\"]}\n",
    "\n",
    "sc = split_counts(df)\n",
    "n_cls = df[\"label\"].nunique()\n",
    "print(f\"Splits | train={sc['train']} val={sc['val']} test={sc['test']} | classes={n_cls}\")\n",
    "\n",
    "# Per-class train counts\n",
    "train_counts = (df[df.split==\"train\"]\n",
    "                .groupby([\"label\",\"gloss\"], as_index=False)\n",
    "                .size().rename(columns={\"size\":\"n\"}))\n",
    "if not len(train_counts):\n",
    "    raise RuntimeError(\"No train rows found in manifest; cannot analyze.\")\n",
    "\n",
    "# Summary stats\n",
    "n = train_counts[\"n\"].to_numpy()\n",
    "print(\"\\n[Train per-class counts]\")\n",
    "print(f\"min={n.min()}  p10={np.percentile(n,10):.1f}  median={np.median(n):.1f}  p90={np.percentile(n,90):.1f}  max={n.max()}\")\n",
    "\n",
    "low_mask = train_counts[\"n\"] < 3\n",
    "few = train_counts[low_mask].sort_values(\"n\")\n",
    "print(f\"Classes with <3 train clips: {len(few)}/{n_cls}\")\n",
    "print(few.head(10).to_string(index=False))\n",
    "\n",
    "# Save a CSV report (full histogram & low-sample classes)\n",
    "out_dir = root / \"runs\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "perclass_csv = out_dir / f\"{run_tag}.train_per_class_counts.csv\"\n",
    "low_csv      = out_dir / f\"{run_tag}.low_sample_classes_lt3.csv\"\n",
    "train_counts.sort_values(\"n\").to_csv(perclass_csv, index=False)\n",
    "few.to_csv(low_csv, index=False)\n",
    "print(f\"\\nSaved:\\n- {perclass_csv}\\n- {low_csv}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2) VIDEO INTEGRITY CHECK\n",
    "# -------------------------------\n",
    "# Try opening a sample of videos with Decord; read a tiny indexed batch.\n",
    "bad = []\n",
    "tested = 0\n",
    "\n",
    "try:\n",
    "    import decord\n",
    "    decord.bridge.set_bridge('torch')\n",
    "    # sample up to this many across splits to keep it quick\n",
    "    SAMPLE_MAX = 400\n",
    "    # prefer train+val since they impact learning curves\n",
    "    df_check = pd.concat([\n",
    "        df[df.split==\"train\"].sample(min(SAMPLE_MAX//2, (df.split==\"train\").sum()), random_state=0),\n",
    "        df[df.split==\"val\"].sample(min(SAMPLE_MAX//3, (df.split==\"val\").sum()), random_state=1),\n",
    "        df[df.split==\"test\"].sample(min(SAMPLE_MAX//6, (df.split==\"test\").sum()), random_state=2),\n",
    "    ], ignore_index=True).drop_duplicates(subset=[\"path\"])\n",
    "\n",
    "    for p in df_check[\"path\"]:\n",
    "        vp = root / p\n",
    "        try:\n",
    "            vr = decord.VideoReader(str(vp))\n",
    "            total = len(vr)\n",
    "            if total == 0:\n",
    "                raise RuntimeError(\"zero frames\")\n",
    "            # probe a few sparse frames\n",
    "            idxs = np.unique(np.clip(np.array([0, total//3, 2*total//3, total-1]), 0, max(total-1,0))).astype(int)\n",
    "            _ = vr.get_batch(idxs)\n",
    "        except Exception as e:\n",
    "            bad.append({\"path\": str(p), \"error\": type(e).__name__ + \": \" + str(e)})\n",
    "        tested += 1\n",
    "\n",
    "    bad_df = pd.DataFrame(bad)\n",
    "    bad_path = out_dir / f\"{run_tag}.bad_videos_sample.csv\"\n",
    "    if len(bad_df):\n",
    "        bad_df.to_csv(bad_path, index=False)\n",
    "        print(f\"\\n[Video health] Tested {tested} files | Bad: {len(bad_df)} (sample). Logged to {bad_path}\")\n",
    "        print(bad_df.head(8).to_string(index=False))\n",
    "    else:\n",
    "        print(f\"\\n[Video health] Tested {tested} files | No decode errors in sample ✅\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\n[Video health] Skipped (Decord not available or failed to init).\", e)\n",
    "\n",
    "# -------------------------------\n",
    "# 3) THROUGHPUT & GPU UTILIZATION\n",
    "# -------------------------------\n",
    "# We’ll poll nvidia-smi while running a short forward-only loop.\n",
    "def gpu_util_samples(duration_s=10, interval_s=0.5):\n",
    "    samples = []\n",
    "    if shutil.which(\"nvidia-smi\") is None:\n",
    "        return samples\n",
    "    end = time.time() + duration_s\n",
    "    while time.time() < end:\n",
    "        try:\n",
    "            out = subprocess.check_output(\n",
    "                [\"nvidia-smi\", \"--query-gpu=utilization.gpu\", \"--format=csv,noheader,nounits\"],\n",
    "                stderr=subprocess.DEVNULL\n",
    "            ).decode().strip().splitlines()\n",
    "            # if multiple GPUs, take the one with CUDA_VISIBLE_DEVICES=0 implicitly\n",
    "            util = int(out[0])\n",
    "            samples.append(util)\n",
    "        except Exception:\n",
    "            pass\n",
    "        time.sleep(interval_s)\n",
    "    return samples\n",
    "\n",
    "# Short benchmark: data time vs compute time\n",
    "import torch\n",
    "from torch.amp import autocast\n",
    "\n",
    "have_model = \"model\" in globals()\n",
    "if not have_model:\n",
    "    print(\"\\n[GPU load] Note: model not found in globals(). Will only time the dataloader.\")\n",
    "    model = None\n",
    "\n",
    "iters = min(60, len(train_loader))  # short probe\n",
    "data_times, compute_times = [], []\n",
    "\n",
    "# Start background GPU poller\n",
    "UTIL_DURATION = 12\n",
    "UTIL_INTERVAL = 0.4\n",
    "utils = []\n",
    "stop_flag = False\n",
    "\n",
    "def poller():\n",
    "    nonlocal utils, stop_flag\n",
    "    while not stop_flag:\n",
    "        s = gpu_util_samples(duration_s=UTIL_INTERVAL, interval_s=UTIL_INTERVAL)\n",
    "        if len(s): utils.extend(s)\n",
    "\n",
    "thr = threading.Thread(target=poller, daemon=True)\n",
    "thr.start()\n",
    "\n",
    "# Run a quick forward-only loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if have_model:\n",
    "    model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (xb, yb, _) in enumerate(train_loader):\n",
    "        t0 = time.time()\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "        t1 = time.time()\n",
    "\n",
    "        if have_model:\n",
    "            if CONFIG[\"backbone\"] == \"r3d18_k400\":\n",
    "                xb_in = xb.permute(0,2,1,3,4).contiguous()\n",
    "            else:\n",
    "                xb_in = xb\n",
    "            with autocast(\"cuda\", enabled=CONFIG.get(\"amp\", True)):\n",
    "                _ = model(xb_in)\n",
    "        t2 = time.time()\n",
    "\n",
    "        data_times.append(t1 - t0)\n",
    "        compute_times.append(max(0.0, t2 - t1))\n",
    "        if i+1 >= iters:\n",
    "            break\n",
    "\n",
    "# Stop poller\n",
    "stop_flag = True\n",
    "thr.join(timeout=1.0)\n",
    "\n",
    "def pct(x, q): \n",
    "    return np.percentile(x, q) if len(x) else float(\"nan\")\n",
    "\n",
    "print(\"\\n[Throughput timing over\", len(data_times), \"iters]\")\n",
    "print(f\"Data time (sec/iter):  mean={np.mean(data_times):.3f}  p50={pct(data_times,50):.3f}  p90={pct(data_times,90):.3f}\")\n",
    "if have_model:\n",
    "    print(f\"Compute time (sec/iter): mean={np.mean(compute_times):.3f}  p50={pct(compute_times,50):.3f}  p90={pct(compute_times,90):.3f}\")\n",
    "\n",
    "if len(utils):\n",
    "    print(f\"\\n[GPU utilization samples] n={len(utils)}  mean={np.mean(utils):.1f}%  p50={pct(utils,50):.1f}%  p90={pct(utils,90):.1f}%  max={np.max(utils):.0f}%\")\n",
    "else:\n",
    "    print(\"\\n[GPU utilization] nvidia-smi not found or no samples gathered.\")\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac98c52-ee4f-4147-8433-685ec6fbcf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell D1 — Clean manifest: drop decode-bad files & classes with <3 train clips ===\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "root = Path(\"..\").resolve()\n",
    "man_in  = root / CONFIG[\"manifest\"]  # current wlasl300\n",
    "runs_dir = root / \"runs\"\n",
    "bad_csv  = max(runs_dir.glob(f\"{run_tag}.bad_videos_sample.csv\"), default=None)\n",
    "man_out  = man_in.with_name(\"manifest_wlasl300_clean.csv\")\n",
    "\n",
    "df = pd.read_csv(man_in)\n",
    "\n",
    "# 1) Drop known-bad video paths (from the Decord probe)\n",
    "if bad_csv and bad_csv.exists():\n",
    "    bad = pd.read_csv(bad_csv)[\"path\"].astype(str).tolist()\n",
    "    before = len(df)\n",
    "    df = df[~df[\"path\"].isin(bad)].reset_index(drop=True)\n",
    "    print(f\"Dropped decode-bad files: {before - len(df)}\")\n",
    "else:\n",
    "    print(\"No bad-video CSV found; skipping file drops.\")\n",
    "\n",
    "# 2) Prune classes with <3 train clips (only affects 'train' rows; keep val/test of remaining classes)\n",
    "train = df[df.split==\"train\"]\n",
    "counts = train.groupby(\"label\")[\"path\"].count()\n",
    "keep_labels = set(counts[counts >= 3].index.tolist())\n",
    "print(f\"Keeping {len(keep_labels)} classes with >=3 train clips; dropping {df['label'].nunique() - len(keep_labels)} low-sample classes.\")\n",
    "\n",
    "df = df[df[\"label\"].isin(keep_labels)].reset_index(drop=True)\n",
    "\n",
    "# 3) Re-map labels to 0..C-1 (deterministic, by gloss sort)\n",
    "glosses = sorted(df[\"gloss\"].unique())\n",
    "g2id = {g:i for i,g in enumerate(glosses)}\n",
    "df[\"label\"] = df[\"gloss\"].map(g2id).astype(int)\n",
    "\n",
    "df = df[[\"path\",\"gloss\",\"label\",\"split\"]]\n",
    "df.to_csv(man_out, index=False)\n",
    "\n",
    "# Stats\n",
    "sc = {s: int((df[\"split\"]==s).sum()) for s in [\"train\",\"val\",\"test\"]}\n",
    "print(f\"[{man_out.name}] Splits | train={sc['train']} val={sc['val']} test={sc['test']} | classes={df['label'].nunique()}\")\n",
    "print(\"Example:\", df.iloc[0].to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45653356-9a1d-4aae-ae1d-1fe59d5ff881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell D2 — GPU/Data timing probe ===\n",
    "import time, subprocess, shutil, numpy as np, torch\n",
    "from torch.amp import autocast\n",
    "\n",
    "def gpu_util_once():\n",
    "    if shutil.which(\"nvidia-smi\") is None:\n",
    "        return None\n",
    "    try:\n",
    "        out = subprocess.check_output(\n",
    "            [\"nvidia-smi\",\"--query-gpu=utilization.gpu\",\"--format=csv,noheader,nounits\"],\n",
    "            stderr=subprocess.DEVNULL\n",
    "        ).decode().strip().splitlines()\n",
    "        return int(out[0])\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "have_model = \"model\" in globals()\n",
    "if have_model: model.eval()\n",
    "\n",
    "iters = min(40, len(train_loader))\n",
    "data_t, comp_t, utils = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (xb, yb, _) in enumerate(train_loader):\n",
    "        t0 = time.time()\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "        t1 = time.time()\n",
    "\n",
    "        if have_model:\n",
    "            if CONFIG[\"backbone\"] == \"r3d18_k400\":\n",
    "                xb_in = xb.permute(0,2,1,3,4).contiguous()\n",
    "            else:\n",
    "                xb_in = xb\n",
    "            with autocast(\"cuda\", enabled=CONFIG.get(\"amp\", True)):\n",
    "                _ = model(xb_in)\n",
    "        t2 = time.time()\n",
    "\n",
    "        data_t.append(t1 - t0)\n",
    "        comp_t.append(max(0.0, t2 - t1))\n",
    "        u = gpu_util_once()\n",
    "        if u is not None: utils.append(u)\n",
    "\n",
    "        if i+1 >= iters: break\n",
    "\n",
    "def pct(x,q): return np.percentile(x,q) if len(x) else float(\"nan\")\n",
    "\n",
    "print(f\"Data time (s/iter):   mean={np.mean(data_t):.3f}  p50={pct(data_t,50):.3f}  p90={pct(data_t,90):.3f}\")\n",
    "print(f\"Compute time (s/iter): mean={np.mean(comp_t):.3f}  p50={pct(comp_t,50):.3f}  p90={pct(comp_t,90):.3f}\")\n",
    "if utils:\n",
    "    print(f\"GPU util (%):          mean={np.mean(utils):.1f}  p50={pct(utils,50):.1f}  p90={pct(utils,90):.1f}  max={np.max(utils):.0f}\")\n",
    "else:\n",
    "    print(\"GPU util: nvidia-smi unavailable or no samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5af6f01-ac7a-4a8f-b585-827a8fc4aa70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
