{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4e4a89b-be80-4cf5-b7a1-f35bdfb918b0",
   "metadata": {},
   "source": [
    "(compact baseline, torchvision-free)\n",
    "\n",
    "This is a small 3D CNN baseline (C3D-lite). It’s not as strong as an official R(2+1)D/ResNet-3D, but it trains fast and avoids extra library constraints. You can later swap in a stronger backbone with the same dataloaders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4a66cc-3233-432f-a610-fb298a9cc23a",
   "metadata": {},
   "source": [
    "### Cell 1 — Root, config, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9d549ab-4cd5-45a1-b068-8a7538956030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTHONPATH added: /home/falasoul/notebooks/USD/AAI-590/Capstone/AAI-590-G3-ASL\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Cell 1: project root, sys.path, config, utils ---\n",
    "from pathlib import Path\n",
    "import sys, yaml\n",
    "\n",
    "# 1) Point to project root and add to sys.path BEFORE any src imports\n",
    "root = Path(\"..\").resolve()\n",
    "if str(root) not in sys.path:\n",
    "    sys.path.insert(0, str(root))\n",
    "if str(root / \"src\") not in sys.path:\n",
    "    sys.path.insert(0, str(root / \"src\"))\n",
    "print(\"PYTHONPATH added:\", root)\n",
    "\n",
    "# 2) Now it's safe to import from src/*\n",
    "from src.data.wlasl_ds import WLASLDataset\n",
    "from src.utils.seed import seed_everything\n",
    "from src.utils.checkpoints import save_checkpoint, load_checkpoint\n",
    "\n",
    "# 3) Load config & prepare dirs\n",
    "cfg_path = root / \"configs\" / \"wlasl100.yaml\"\n",
    "assert cfg_path.exists(), f\"Config not found: {cfg_path}\"\n",
    "CFG = yaml.safe_load(open(cfg_path, \"r\"))\n",
    "\n",
    "CKPT_DIR = root / CFG[\"paths\"][\"checkpoints_dir\"]\n",
    "LOG_DIR  = root / CFG[\"paths\"][\"logs_dir\"]\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 4) Seed\n",
    "seed_everything(CFG[\"wlasl\"][\"split_seed\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d87bf12-5754-45f3-8686-12af4c1bf6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports OK; seed: 42\n"
     ]
    }
   ],
   "source": [
    "from src.utils.seed import seed_everything\n",
    "from src.utils.checkpoints import save_checkpoint, load_checkpoint\n",
    "print(\"imports OK; seed:\", seed_everything(42))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7dc46fa-5e35-46da-8396-c3248f1218b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np, cv2, decord, random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "decord.bridge.set_bridge(\"torch\")\n",
    "\n",
    "def _resize_112(frame_tchw: torch.Tensor) -> torch.Tensor:\n",
    "    # frame_tchw: [T,C,H,W] float32 [0,1]\n",
    "    T,C,H,W = frame_tchw.shape\n",
    "    # Use OpenCV for speed; convert to NHWC\n",
    "    arr = frame_tchw.permute(0,2,3,1).cpu().numpy()  # T,H,W,C\n",
    "    out = np.empty((T,112,112,C), dtype=np.float32)\n",
    "    for t in range(T):\n",
    "        out[t] = cv2.resize(arr[t], (112,112), interpolation=cv2.INTER_AREA)\n",
    "    out = torch.from_numpy(out).permute(0,3,1,2)  # T,C,112,112\n",
    "    return out\n",
    "\n",
    "def _normalize(frame_tchw: torch.Tensor, mean=(0.45,0.45,0.45), std=(0.225,0.225,0.225)) -> torch.Tensor:\n",
    "    # per-channel normalization\n",
    "    mean = torch.tensor(mean, dtype=frame_tchw.dtype, device=frame_tchw.device)[None,:,None,None]\n",
    "    std  = torch.tensor(std,  dtype=frame_tchw.dtype, device=frame_tchw.device)[None,:,None,None]\n",
    "    return (frame_tchw - mean) / std\n",
    "\n",
    "def uniform_temporal_indices(n_total, clip_len, stride):\n",
    "    # Aim to cover as much as possible; for short videos, loop-pad\n",
    "    if n_total <= 0: return [0]*clip_len\n",
    "    wanted = (clip_len-1)*stride + 1\n",
    "    if n_total >= wanted:\n",
    "        # center-start for consistent coverage\n",
    "        start = (n_total - wanted)//2\n",
    "        return [start + i*stride for i in range(clip_len)]\n",
    "    # not enough frames: repeat last index\n",
    "    idxs = [min(i*stride, n_total-1) for i in range(clip_len)]\n",
    "    return idxs\n",
    "\n",
    "class WLASLDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, clip_len=32, stride=2, train=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.clip_len = clip_len\n",
    "        self.stride = stride\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        row = self.df.iloc[i]\n",
    "        path = row[\"path\"]\n",
    "        label = int(row[\"label\"])\n",
    "        vr = decord.VideoReader(path)\n",
    "        n = len(vr)\n",
    "\n",
    "        idxs = uniform_temporal_indices(n, self.clip_len, self.stride)\n",
    "        batch = vr.get_batch(idxs)  # [T,H,W,C] uint8\n",
    "        # to float [0,1], TCHW\n",
    "        x = batch.float()/255.0\n",
    "        x = x.permute(0,3,1,2)\n",
    "        # spatial resize 112x112\n",
    "        x = _resize_112(x)\n",
    "        # normalize\n",
    "        x = _normalize(x)\n",
    "        return x, label, path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcacc2a-8c41-42c6-b5cd-85b269a21c61",
   "metadata": {},
   "source": [
    "####  Cell 2 — Load manifest & build DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdcdb093-bced-4c3b-98d5-9d2ffc45156a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded manifest: /home/falasoul/notebooks/USD/AAI-590/Capstone/AAI-590-G3-ASL/data/metadata/wlasl100_manifest.csv\n",
      "Total samples: 752\n",
      "Splits: {'train': np.int64(547), 'val': np.int64(124), 'test': np.int64(81)}\n",
      "Classes: 100\n",
      "Train batches: 69 | Val batches: 16 | Test batches: 11\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from collections import Counter\n",
    "\n",
    "# === Load dataset manifest (created in 02_preprocess_segments.ipynb) ===\n",
    "MANIFEST = root / \"data\" / \"metadata\" / \"wlasl100_manifest.csv\"\n",
    "m = pd.read_csv(MANIFEST)\n",
    "print(\"Loaded manifest:\", MANIFEST)\n",
    "print(\"Total samples:\", len(m))\n",
    "print(\"Splits:\", dict(m[\"split\"].value_counts()))\n",
    "\n",
    "# === Split subsets ===\n",
    "train_df = m[m[\"split\"] == \"train\"].copy()\n",
    "val_df   = m[m[\"split\"] == \"val\"].copy()\n",
    "test_df  = m[m[\"split\"] == \"test\"].copy()\n",
    "\n",
    "# === Read config values ===\n",
    "clip_len   = CFG[\"model\"][\"clip_len\"]\n",
    "frame_step = CFG[\"model\"][\"frame_stride\"]\n",
    "bs         = CFG[\"train\"][\"batch_size\"]\n",
    "nw         = CFG[\"train\"][\"num_workers\"]\n",
    "\n",
    "# === Import the dataset class (from 03_dataset_preview.ipynb or src/data/wlasl_ds.py) ===\n",
    "# If you have the Dataset defined in the preview notebook, just re-run that cell before this.\n",
    "# Otherwise, place it in `src/data/wlasl_ds.py` and import as shown:\n",
    "# from src.data.wlasl_ds import WLASLDataset\n",
    "\n",
    "# === Create train/val/test datasets ===\n",
    "train_ds = WLASLDataset(train_df, clip_len=clip_len, stride=frame_step, train=True)\n",
    "val_ds   = WLASLDataset(val_df,   clip_len=clip_len, stride=frame_step, train=False)\n",
    "test_ds  = WLASLDataset(test_df,  clip_len=clip_len, stride=frame_step, train=False)\n",
    "\n",
    "# === Handle class imbalance via WeightedRandomSampler ===\n",
    "counts = train_df[\"label\"].value_counts().to_dict()\n",
    "weights = train_df[\"label\"].map(lambda y: 1.0 / max(1, counts[y])).values\n",
    "sampler = WeightedRandomSampler(\n",
    "    torch.tensor(weights, dtype=torch.double),\n",
    "    num_samples=len(train_df),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# === Build DataLoaders ===\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=bs, sampler=sampler,\n",
    "    num_workers=nw, pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=bs, shuffle=False,\n",
    "    num_workers=nw, pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_ds, batch_size=bs, shuffle=False,\n",
    "    num_workers=nw, pin_memory=True\n",
    ")\n",
    "\n",
    "# === Confirm stats ===\n",
    "num_classes = m[\"label\"].nunique()\n",
    "print(f\"Classes: {num_classes}\")\n",
    "print(f\"Train batches: {len(train_loader)} | Val batches: {len(val_loader)} | Test batches: {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071cf9c7-0eb3-4f56-a2fc-be9897883706",
   "metadata": {},
   "source": [
    "### Cell 3 (Notebook) — Small 3D CNN, AMP-ready, compile-ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dbd0ddc-8009-4ab9-adfd-3fe9a7a34757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.compile enabled\n",
      "Params: 2.96M\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# (Optional) Slightly faster matmul on Ada/Lovelace\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "class C3Dlite(nn.Module):\n",
    "    \"\"\"\n",
    "    A compact 3D CNN that trains fast on WLASL100 clips (112x112, T=32).\n",
    "    Input expected as [B, T, C, H, W]; we permute internally to [B, C, T, H, W].\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=100, drop=0.5):\n",
    "        super().__init__()\n",
    "        def block(cin, cout, pool_t=2):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv3d(cin, cout, kernel_size=3, padding=1, bias=False),\n",
    "                nn.BatchNorm3d(cout),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool3d(kernel_size=(pool_t,2,2), stride=(pool_t,2,2))\n",
    "            )\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv3d(3, 32, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.b1 = block(32,  64, pool_t=2)\n",
    "        self.b2 = block(64, 128, pool_t=2)\n",
    "        self.b3 = block(128, 256, pool_t=2)\n",
    "        self.b4 = block(256, 256, pool_t=2)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool3d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=drop),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):           # x: [B,T,C,H,W]\n",
    "        x = x.permute(0,2,1,3,4)    # -> [B,C,T,H,W]\n",
    "        # ✅ make 5D tensor channels-last for 3D convs (NDHWC)\n",
    "        x = x.contiguous(memory_format=torch.channels_last_3d)\n",
    "        x = self.stem(x)\n",
    "        x = self.b1(x); x = self.b2(x); x = self.b3(x); x = self.b4(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "num_classes = m[\"label\"].nunique()\n",
    "model = C3Dlite(num_classes=num_classes).cuda()\n",
    "\n",
    "# Speed/memory hints\n",
    "#model = model.to(memory_format=torch.channels_last)  # helps on Ada/Lovelace\n",
    "use_compile = True\n",
    "if use_compile:\n",
    "    try:\n",
    "        model = torch.compile(model)  # PyTorch 2.7.1 present in your env\n",
    "        print(\"torch.compile enabled\")\n",
    "    except Exception as e:\n",
    "        print(\"compile skipped:\", e)\n",
    "\n",
    "print(f\"Params: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b53a45d-f2e5-4aee-843c-c2ed7b128eba",
   "metadata": {},
   "source": [
    "#### Cell 4 (Notebook) — Train loop with AMP, checkpoints, resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "283d8af1-044f-42fc-90c3-e5ceaf15402c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs=30 (int)  lr=0.001 (float)  wd=0.0001 (float)  amp_on=True (bool)  grad_acc=1 (int)\n",
      "Epoch 001/30 | train loss 4.8943 top1 0.015 | val loss 4.8427 top1 0.024\n",
      "Epoch 002/30 | train loss 4.7149 top1 0.015 | val loss 4.8105 top1 0.000\n",
      "Epoch 003/30 | train loss 4.6237 top1 0.018 | val loss 4.7979 top1 0.016\n",
      "Epoch 004/30 | train loss 4.5790 top1 0.027 | val loss 4.6666 top1 0.008\n",
      "Epoch 005/30 | train loss 4.5973 top1 0.024 | val loss 4.8655 top1 0.008\n",
      "Epoch 006/30 | train loss 4.6130 top1 0.024 | val loss 4.6790 top1 0.024\n",
      "Epoch 007/30 | train loss 4.5344 top1 0.022 | val loss 4.6636 top1 0.016\n",
      "Epoch 008/30 | train loss 4.4970 top1 0.024 | val loss 4.6514 top1 0.016\n",
      "Epoch 009/30 | train loss 4.5345 top1 0.015 | val loss 4.7258 top1 0.000\n",
      "Epoch 010/30 | train loss 4.5081 top1 0.024 | val loss 4.6124 top1 0.024\n",
      "Epoch 011/30 | train loss 4.4786 top1 0.035 | val loss 4.6409 top1 0.016\n",
      "Epoch 012/30 | train loss 4.4210 top1 0.022 | val loss 4.5000 top1 0.040\n",
      "Epoch 013/30 | train loss 4.4085 top1 0.035 | val loss 4.5546 top1 0.016\n",
      "Epoch 014/30 | train loss 4.4225 top1 0.026 | val loss 4.5328 top1 0.016\n",
      "Epoch 015/30 | train loss 4.3305 top1 0.026 | val loss 4.5529 top1 0.032\n",
      "Epoch 016/30 | train loss 4.4239 top1 0.024 | val loss 4.5441 top1 0.016\n",
      "Epoch 017/30 | train loss 4.3683 top1 0.037 | val loss 4.4876 top1 0.024\n",
      "Epoch 018/30 | train loss 4.3772 top1 0.022 | val loss 4.5001 top1 0.032\n",
      "Epoch 019/30 | train loss 4.2558 top1 0.051 | val loss 4.4690 top1 0.024\n",
      "Epoch 020/30 | train loss 4.2482 top1 0.059 | val loss 4.5066 top1 0.024\n",
      "Epoch 021/30 | train loss 4.2309 top1 0.055 | val loss 4.5045 top1 0.008\n",
      "Epoch 022/30 | train loss 4.2121 top1 0.031 | val loss 4.5260 top1 0.016\n",
      "Epoch 023/30 | train loss 4.2095 top1 0.055 | val loss 4.5125 top1 0.024\n",
      "Epoch 024/30 | train loss 4.2281 top1 0.046 | val loss 4.5844 top1 0.016\n",
      "Epoch 025/30 | train loss 4.1786 top1 0.049 | val loss 4.6434 top1 0.048\n",
      "Epoch 026/30 | train loss 4.2224 top1 0.035 | val loss 4.5280 top1 0.032\n",
      "Epoch 027/30 | train loss 4.1917 top1 0.048 | val loss 4.5569 top1 0.032\n",
      "Epoch 028/30 | train loss 4.0674 top1 0.053 | val loss 4.6381 top1 0.016\n",
      "Epoch 029/30 | train loss 4.1246 top1 0.059 | val loss 4.7566 top1 0.024\n",
      "Epoch 030/30 | train loss 4.1280 top1 0.055 | val loss 4.5576 top1 0.000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.amp import autocast, GradScaler       # ✅ new AMP API\n",
    "\n",
    "from src.utils.seed import seed_everything\n",
    "from src.utils.checkpoints import save_checkpoint, load_checkpoint\n",
    "\n",
    "# reproducibility\n",
    "seed_everything(CFG[\"wlasl\"][\"split_seed\"])\n",
    "\n",
    "# --- helpers to safely cast config values ---\n",
    "def as_float(x, default):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def as_int(x, default):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def as_bool(x, default):\n",
    "    if isinstance(x, bool):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        return x.strip().lower() in {\"1\",\"true\",\"yes\",\"y\",\"on\"}\n",
    "    return bool(x) if x is not None else default\n",
    "\n",
    "# --- parse training parameters from YAML ---\n",
    "epochs   = as_int(CFG[\"train\"].get(\"epochs\", 12), 12)\n",
    "lr       = as_float(CFG[\"train\"].get(\"lr\", 3e-4), 3e-4)\n",
    "wd       = as_float(CFG[\"train\"].get(\"weight_decay\", 0.01), 0.01)\n",
    "amp_on   = as_bool(CFG[\"train\"].get(\"amp\", True), True)\n",
    "grad_acc = as_int(CFG[\"train\"].get(\"grad_accum_steps\", 1), 1)\n",
    "\n",
    "print(f\"epochs={epochs} (int)  lr={lr} (float)  wd={wd} (float)  \"\n",
    "      f\"amp_on={amp_on} (bool)  grad_acc={grad_acc} (int)\")\n",
    "\n",
    "# --- optimizer + scaler ---\n",
    "opt = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "scaler = GradScaler(\"cuda\", enabled=amp_on)     # ✅ new API\n",
    "\n",
    "best_val_top1 = -1.0\n",
    "start_epoch = 0\n",
    "\n",
    "# optional resume checkpoint\n",
    "resume_path = CFG[\"train\"].get(\"resume_checkpoint\", None)\n",
    "if resume_path:\n",
    "    rp = root / resume_path\n",
    "    if rp.exists():\n",
    "        start_epoch, best_val_top1 = load_checkpoint(str(rp), model, opt, scaler)\n",
    "        print(f\"Resumed from {rp} at epoch {start_epoch}, best={best_val_top1:.3f}\")\n",
    "\n",
    "# --- metric helper ---\n",
    "def topk_acc(logits, target, k=1):\n",
    "    with torch.no_grad():\n",
    "        pred = logits.topk(k, dim=1).indices\n",
    "        return (pred.eq(target[:, None]).any(dim=1).float().mean().item())\n",
    "\n",
    "# --- one epoch loop ---\n",
    "def run_epoch(loader, train=True):\n",
    "    model.train() if train else model.eval()\n",
    "    total_loss, total_top1, total_n = 0.0, 0.0, 0\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "\n",
    "    for step, (x, y, _) in enumerate(loader):\n",
    "        x = x.cuda(non_blocking=True)\n",
    "        y = y.cuda(non_blocking=True)\n",
    "\n",
    "        with autocast(\"cuda\", enabled=amp_on):   # ✅ updated syntax\n",
    "            logits = model(x)\n",
    "            loss = F.cross_entropy(logits, y) / grad_acc\n",
    "\n",
    "        if train:\n",
    "            scaler.scale(loss).backward()\n",
    "            if (step + 1) % grad_acc == 0:\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            top1 = topk_acc(logits.detach(), y, k=1)\n",
    "            bs = x.size(0)\n",
    "            total_loss += (loss.item() * grad_acc) * bs\n",
    "            total_top1 += top1 * bs\n",
    "            total_n += bs\n",
    "\n",
    "    return total_loss / total_n, total_top1 / total_n\n",
    "\n",
    "# --- training loop ---\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    tr_loss, tr_top1 = run_epoch(train_loader, train=True)\n",
    "    va_loss, va_top1 = run_epoch(val_loader, train=False)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:03d}/{epochs} | \"\n",
    "          f\"train loss {tr_loss:.4f} top1 {tr_top1:.3f} | \"\n",
    "          f\"val loss {va_loss:.4f} top1 {va_top1:.3f}\")\n",
    "\n",
    "    # save checkpoints\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optim_state\": opt.state_dict(),\n",
    "        \"scaler_state\": scaler.state_dict(),\n",
    "        \"best_metric\": best_val_top1,\n",
    "    }\n",
    "    save_checkpoint(state, is_best=False, ckpt_dir=str(CKPT_DIR), filename=\"last.pt\")\n",
    "\n",
    "    if va_top1 > best_val_top1:\n",
    "        best_val_top1 = va_top1\n",
    "        save_checkpoint(state, is_best=True, ckpt_dir=str(CKPT_DIR), filename=\"best.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89264e53-a7f4-4e01-a500-0792ea3eea9f",
   "metadata": {},
   "source": [
    "#### Cell 5 (Notebook) — Evaluate best checkpoint on test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f32a87c-79e0-472a-811c-1c35cba0e78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] loss 4.7616 top1 0.025\n"
     ]
    }
   ],
   "source": [
    "# Load best.pt and evaluate on test_loader (AMP-friendly eval)\n",
    "from torch.amp import autocast  # new API\n",
    "\n",
    "best_path = CKPT_DIR / \"best.pt\"\n",
    "if best_path.exists():\n",
    "    _, _ = load_checkpoint(str(best_path), model)  # weights only\n",
    "else:\n",
    "    print(\"best.pt not found, evaluating with current weights.\")\n",
    "\n",
    "model.eval()\n",
    "test_loss, test_top1, n = 0.0, 0.0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y, _ in test_loader:\n",
    "        x = x.cuda(non_blocking=True)\n",
    "        y = y.cuda(non_blocking=True)\n",
    "\n",
    "        # AMP in eval for speed\n",
    "        with autocast(\"cuda\", enabled=True):\n",
    "            logits = model(x)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        bs = x.size(0)\n",
    "        test_loss += loss.item() * bs\n",
    "        test_top1 += (logits.topk(1, dim=1).indices.squeeze(1) == y).float().sum().item()\n",
    "        n += bs\n",
    "\n",
    "if n > 0:\n",
    "    print(f\"[TEST] loss {test_loss/n:.4f} top1 {test_top1/n:.3f}\")\n",
    "else:\n",
    "    print(\"[TEST] loader is empty; no samples to evaluate.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cf3c66-4ad6-4a1d-b562-80562377796f",
   "metadata": {},
   "source": [
    "Early epochs with very low val top-1 (0–2%) usually means one of these: label mismatch, BN instability (small batches), learning rate/schedule not helping, or input/normalization quirks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7514366a-cc76-4d29-9de7-6288ba21c620",
   "metadata": {},
   "source": [
    "### A) Verify label space & splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3606858-9340-476c-b659-af38a6c91c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes: 100\n",
      "train classes: 100\n",
      "val classes: 82\n",
      "test classes: 65\n",
      "val-only classes (should be empty or small): []\n"
     ]
    }
   ],
   "source": [
    "# Run in the training notebook\n",
    "import pandas as pd\n",
    "m = pd.read_csv(root / \"data\" / \"metadata\" / \"wlasl100_manifest.csv\")\n",
    "print(\"classes:\", m[\"label\"].nunique())\n",
    "print(\"train classes:\", m[m.split==\"train\"][\"label\"].nunique())\n",
    "print(\"val classes:\",   m[m.split==\"val\"][\"label\"].nunique())\n",
    "print(\"test classes:\",  m[m.split==\"test\"][\"label\"].nunique())\n",
    "missing_in_train = set(m[m.split==\"val\"][\"label\"].unique()) - set(m[m.split==\"train\"][\"label\"].unique())\n",
    "print(\"val-only classes (should be empty or small):\", sorted(list(missing_in_train))[:10])\n",
    "assert m[\"label\"].min()==0 and m[\"label\"].max()==99\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c919fe0-2586-4098-88b9-f070e1cf811d",
   "metadata": {},
   "source": [
    "### B) Overfit a tiny subset (should climb fast if pipeline is healthy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cecfc7c-becf-4a3b-9daa-6735da276283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TINY epoch 1 loss 5.0827 acc 0.005\n",
      "TINY epoch 2 loss 4.7057 acc 0.015\n",
      "TINY epoch 3 loss 4.6524 acc 0.015\n",
      "TINY epoch 4 loss 4.5761 acc 0.010\n",
      "TINY epoch 5 loss 4.5739 acc 0.010\n"
     ]
    }
   ],
   "source": [
    "# Build a tiny train subset\n",
    "tiny_idx = train_df.sample(200, random_state=0).index\n",
    "tiny_ds = WLASLDataset(train_df.loc[tiny_idx], clip_len=clip_len, stride=frame_step, train=True)\n",
    "tiny_loader = DataLoader(tiny_ds, batch_size=bs, shuffle=True, num_workers=0)\n",
    "\n",
    "# One-epoch loop on tiny set\n",
    "model.train()\n",
    "opt = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "from torch.amp import autocast, GradScaler\n",
    "scaler = GradScaler(\"cuda\", enabled=True)\n",
    "\n",
    "for e in range(5):\n",
    "    tot, acc, n = 0.0, 0.0, 0\n",
    "    for x,y,_ in tiny_loader:\n",
    "        x=x.cuda(); y=y.cuda()\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        with autocast(\"cuda\", enabled=True):\n",
    "            logits = model(x)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt); scaler.update()\n",
    "        # metrics\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(1)\n",
    "            bs = y.size(0); n += bs\n",
    "            tot += loss.item()*bs\n",
    "            acc += (pred==y).float().sum().item()\n",
    "    print(f\"TINY epoch {e+1} loss {tot/n:.4f} acc {acc/n:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bec6fa-c9a1-4c0f-91f4-f56eeee42f20",
   "metadata": {},
   "source": [
    "##### since we are not over fitting, we will be doing some sanity and label diagnostic check. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1baf8e5-cd6d-4b9a-8778-7d433bcb0efb",
   "metadata": {},
   "source": [
    "1) Sanity on labels vs logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "864df6c1-90a1-4629-b67a-cad7f893d761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label dtype: int64 min/max: 0 99 unique: 100\n",
      "logits shape: torch.Size([8, 100]) min/max label in this batch: 29 85\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, torch\n",
    "\n",
    "# Manifest label space\n",
    "m = pd.read_csv(root / \"data\" / \"metadata\" / \"wlasl100_manifest.csv\")\n",
    "print(\"label dtype:\", m[\"label\"].dtype, \"min/max:\", m[\"label\"].min(), m[\"label\"].max(), \"unique:\", m[\"label\"].nunique())\n",
    "\n",
    "# Model output size\n",
    "dummy_x, dummy_y, _ = next(iter(train_loader))\n",
    "with torch.no_grad():\n",
    "    logits = model(dummy_x.cuda())\n",
    "print(\"logits shape:\", logits.shape, \"min/max label in this batch:\", int(dummy_y.min()), int(dummy_y.max()))\n",
    "assert logits.shape[1] == m[\"label\"].nunique(), \"num_classes mismatch!\"\n",
    "assert int(m[\"label\"].min()) == 0 and int(m[\"label\"].max()) == logits.shape[1]-1, \"labels must be 0..num_classes-1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7e70d6-5790-402a-b343-c988439935a1",
   "metadata": {},
   "source": [
    "2) Check for NaNs / exploding activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fad35c8-42d8-44af-9fe0-12f5792797d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits stats: -11.715280532836914 6.308036804199219 any_nan: False\n",
      "loss: 5.692608833312988\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "x, y, _ = next(iter(train_loader))\n",
    "x = x.cuda(); y = y.cuda()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    z = model(x)\n",
    "    print(\"logits stats:\", z.float().mean().item(), z.float().std().item(),\n",
    "          \"any_nan:\", torch.isnan(z).any().item())\n",
    "    print(\"loss:\", F.cross_entropy(z, y).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe322df-e67c-4c12-867e-d66c1c058ecb",
   "metadata": {},
   "source": [
    "3) BatchNorm vs small batches\n",
    "\n",
    "BatchNorm can be unstable with small, variable batches (video lengths, sampler). Two quick experiments:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65dc0b8-636c-4795-862d-0a5a8a2feb80",
   "metadata": {},
   "source": [
    "(a) Freeze BN (uses running stats only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51c243cf-e03d-42c8-80b3-7c21303bf274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): C3Dlite(\n",
       "    (stem): Sequential(\n",
       "      (0): Conv3d(3, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (b1): Sequential(\n",
       "      (0): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (b2): Sequential(\n",
       "      (0): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (b3): Sequential(\n",
       "      (0): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (b4): Sequential(\n",
       "      (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "      (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (head): Sequential(\n",
       "      (0): AdaptiveAvgPool3d(output_size=1)\n",
       "      (1): Flatten(start_dim=1, end_dim=-1)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=256, out_features=100, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def set_bn_eval(m):\n",
    "    if isinstance(m, torch.nn.modules.batchnorm._BatchNorm):\n",
    "        m.eval()\n",
    "model.apply(set_bn_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d10ae79-f11d-42e7-ac09-122c23edc545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TINY epoch 1 loss 4.4329 acc 0.045\n",
      "TINY epoch 2 loss 4.3162 acc 0.045\n",
      "TINY epoch 3 loss 4.2662 acc 0.055\n",
      "TINY epoch 4 loss 4.3038 acc 0.055\n",
      "TINY epoch 5 loss 4.0960 acc 0.065\n"
     ]
    }
   ],
   "source": [
    "# Build a tiny train subset\n",
    "tiny_idx = train_df.sample(200, random_state=0).index\n",
    "tiny_ds  = WLASLDataset(train_df.loc[tiny_idx], clip_len=clip_len, stride=frame_step, train=True)\n",
    "tiny_loader = DataLoader(tiny_ds, batch_size=bs, shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "# One-epoch loop on tiny set\n",
    "model.train()\n",
    "opt = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "from torch.amp import autocast, GradScaler\n",
    "scaler = GradScaler(\"cuda\", enabled=True)\n",
    "\n",
    "for e in range(5):\n",
    "    tot, acc, n = 0.0, 0.0, 0\n",
    "    for x,y,_ in tiny_loader:\n",
    "        x=x.cuda(); y=y.cuda()\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        with autocast(\"cuda\", enabled=True):\n",
    "            logits = model(x)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt); scaler.update()\n",
    "        # metrics\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(1)\n",
    "            bs = y.size(0); n += bs\n",
    "            tot += loss.item()*bs\n",
    "            acc += (pred==y).float().sum().item()\n",
    "    print(f\"TINY epoch {e+1} loss {tot/n:.4f} acc {acc/n:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cce0b06-e444-4e9d-b620-f6de7ceb47ef",
   "metadata": {},
   "source": [
    "1) Sanity check: labels vs. logits (must be contiguous 0..99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cf0663b-aa67-4910-ac36-9040cef3df6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels unique: 100 min/max: 0 99\n",
      "logits shape: (8, 100) | batch label min/max: 7 81\n",
      "any NaN logits?: False loss: 5.221561431884766\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, torch, torch.nn.functional as F\n",
    "\n",
    "m = pd.read_csv(root / \"data\" / \"metadata\" / \"wlasl100_manifest.csv\")\n",
    "print(\"labels unique:\", m[\"label\"].nunique(), \"min/max:\", m[\"label\"].min(), m[\"label\"].max())\n",
    "\n",
    "xb, yb, _ = next(iter(train_loader))\n",
    "xb = xb.cuda(); yb = yb.cuda()\n",
    "with torch.no_grad():\n",
    "    z = model(xb)  # logits\n",
    "print(\"logits shape:\", tuple(z.shape), \"| batch label min/max:\", int(yb.min()), int(yb.max()))\n",
    "print(\"any NaN logits?:\", torch.isnan(z).any().item(), \"loss:\", F.cross_entropy(z, yb).item())\n",
    "\n",
    "# Hard assertions\n",
    "assert z.shape[1] == m[\"label\"].nunique(), \"num_classes mismatch\"\n",
    "assert int(m[\"label\"].min()) == 0 and int(m[\"label\"].max()) == z.shape[1]-1, \"labels must be 0..num_classes-1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b1cfe0-754b-47e4-999b-fee1db929db0",
   "metadata": {},
   "source": [
    "Great—that output tells us a lot:\n",
    "\n",
    "Labels are contiguous 0..99 ✅\n",
    "\n",
    "Model outputs 100 logits ✅\n",
    "\n",
    "No NaNs ✅\n",
    "\n",
    "So the pipeline is sane. The remaining culprit is almost certainly BatchNorm instability with small/variable video batches + early LR dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a6d2d3-9ee5-49ae-b1c1-1af4d9f800d3",
   "metadata": {},
   "source": [
    "1) Do the one-batch overfit (must pass)\n",
    "\n",
    "Run this as-is. If the model can’t overfit a single batch to ~100% within ~200 steps, we still have a hidden issue; if it can, it’s just training dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ce2a65a-6af5-4409-8267-2ab7510aa6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 020 | loss 3.979 | acc 0.250\n",
      "step 040 | loss 2.976 | acc 0.375\n",
      "step 060 | loss 2.160 | acc 0.625\n",
      "step 080 | loss 1.521 | acc 0.750\n",
      "step 100 | loss 1.232 | acc 0.625\n",
      "step 120 | loss 0.899 | acc 0.750\n",
      "step 140 | loss 0.668 | acc 0.875\n",
      "step 160 | loss 0.335 | acc 0.875\n",
      "step 180 | loss 0.214 | acc 1.000\n",
      "step 200 | loss 0.031 | acc 1.000\n"
     ]
    }
   ],
   "source": [
    "# One-batch overfit (should approach ~1.0 acc)\n",
    "from torch.amp import autocast, GradScaler\n",
    "import torch.optim as optim, torch.nn.functional as F\n",
    "\n",
    "model.train()\n",
    "xb, yb, _ = next(iter(train_loader))\n",
    "xb = xb.cuda(); yb = yb.cuda()\n",
    "\n",
    "opt = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scaler = GradScaler(\"cuda\", enabled=True)\n",
    "\n",
    "for t in range(200):\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    with autocast(\"cuda\", enabled=True):\n",
    "        logits = model(xb)\n",
    "        loss = F.cross_entropy(logits, yb)\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(opt); scaler.update()\n",
    "\n",
    "    if (t+1) % 20 == 0:\n",
    "        acc = (logits.argmax(1) == yb).float().mean().item()\n",
    "        print(f\"step {t+1:03d} | loss {loss.item():.3f} | acc {acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0d3fe2-e6bf-4793-a2cb-7be0d0de87db",
   "metadata": {},
   "source": [
    "2) Make normalization video-friendly (small but helpful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3df2879-c50a-4ef2-ba43-bc227086d7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize(x, mean=(0.432, 0.394, 0.376), std=(0.228, 0.221, 0.223)):\n",
    "    mean = torch.tensor(mean, dtype=x.dtype, device=x.device)[None,:,None,None]\n",
    "    std  = torch.tensor(std,  dtype=x.dtype, device=x.device)[None,:,None,None]\n",
    "    return (x - mean) / std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef93e8eb-ba09-4de1-a561-6063ec7b3982",
   "metadata": {},
   "source": [
    "3) Replace BatchNorm3d with GroupNorm (robust for small batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6d9fe3e-5f38-46a7-92a7-32d2a5177698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: 2.96M\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def gn(c): return nn.GroupNorm(num_groups=8, num_channels=c)\n",
    "\n",
    "class C3DliteGN(nn.Module):\n",
    "    def __init__(self, num_classes=100, drop=0.5):\n",
    "        super().__init__()\n",
    "        def block(cin, cout, pool_t=2):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv3d(cin, cout, kernel_size=3, padding=1, bias=False),\n",
    "                gn(cout), nn.ReLU(inplace=True),\n",
    "                nn.MaxPool3d(kernel_size=(pool_t,2,2), stride=(pool_t,2,2))\n",
    "            )\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv3d(3, 32, kernel_size=3, padding=1, bias=False),\n",
    "            gn(32), nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.b1 = block(32,  64)\n",
    "        self.b2 = block(64, 128)\n",
    "        self.b3 = block(128, 256)\n",
    "        self.b4 = block(256, 256)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool3d(1), nn.Flatten(),\n",
    "            nn.Dropout(p=drop), nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):              # x: [B,T,C,H,W]\n",
    "        x = x.permute(0,2,1,3,4).contiguous()  # [B,C,T,H,W]\n",
    "        x = self.stem(x)\n",
    "        x = self.b1(x); x = self.b2(x); x = self.b3(x); x = self.b4(x)\n",
    "        return self.head(x)\n",
    "\n",
    "num_classes = m[\"label\"].nunique()\n",
    "model = C3DliteGN(num_classes=num_classes).cuda()\n",
    "print(f\"Params: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87df1a02-ae26-4461-8519-128408000e8d",
   "metadata": {},
   "source": [
    "4) Add warmup + cosine schedule (prevents early stalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b92db850-ee4d-47a7-aa72-7c28c761dd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/30 | train loss 4.8558 top1 0.011 | val loss 4.7708 top1 0.008\n",
      "Epoch 002/30 | train loss 4.7311 top1 0.013 | val loss 4.6861 top1 0.008\n",
      "Epoch 003/30 | train loss 4.7015 top1 0.016 | val loss 4.6427 top1 0.008\n",
      "Epoch 004/30 | train loss 4.6561 top1 0.013 | val loss 4.6425 top1 0.008\n",
      "Epoch 005/30 | train loss 4.6802 top1 0.011 | val loss 4.6015 top1 0.016\n",
      "Epoch 006/30 | train loss 4.6378 top1 0.011 | val loss 4.6174 top1 0.000\n",
      "Epoch 007/30 | train loss 4.6476 top1 0.009 | val loss 4.6177 top1 0.000\n",
      "Epoch 008/30 | train loss 4.6480 top1 0.009 | val loss 4.6186 top1 0.016\n",
      "Epoch 009/30 | train loss 4.6171 top1 0.009 | val loss 4.6275 top1 0.016\n",
      "Epoch 010/30 | train loss 4.6390 top1 0.009 | val loss 4.6112 top1 0.016\n",
      "Epoch 011/30 | train loss 4.6401 top1 0.011 | val loss 4.6106 top1 0.008\n",
      "Epoch 012/30 | train loss 4.6275 top1 0.013 | val loss 4.6050 top1 0.008\n",
      "Epoch 013/30 | train loss 4.6067 top1 0.020 | val loss 4.6238 top1 0.008\n",
      "Epoch 014/30 | train loss 4.6326 top1 0.018 | val loss 4.6076 top1 0.008\n",
      "Epoch 015/30 | train loss 4.6075 top1 0.013 | val loss 4.6052 top1 0.008\n",
      "Epoch 016/30 | train loss 4.6262 top1 0.007 | val loss 4.5903 top1 0.016\n",
      "Epoch 017/30 | train loss 4.6094 top1 0.015 | val loss 4.6003 top1 0.024\n",
      "Epoch 018/30 | train loss 4.6083 top1 0.011 | val loss 4.6103 top1 0.008\n",
      "Epoch 019/30 | train loss 4.5999 top1 0.026 | val loss 4.6109 top1 0.008\n",
      "Epoch 020/30 | train loss 4.6071 top1 0.015 | val loss 4.5990 top1 0.016\n",
      "Epoch 021/30 | train loss 4.5914 top1 0.027 | val loss 4.5954 top1 0.024\n",
      "Epoch 022/30 | train loss 4.6064 top1 0.016 | val loss 4.5960 top1 0.024\n",
      "Epoch 023/30 | train loss 4.5967 top1 0.011 | val loss 4.5972 top1 0.016\n",
      "Epoch 024/30 | train loss 4.5849 top1 0.011 | val loss 4.5838 top1 0.016\n",
      "Epoch 025/30 | train loss 4.5697 top1 0.022 | val loss 4.6098 top1 0.016\n",
      "Epoch 026/30 | train loss 4.5700 top1 0.020 | val loss 4.5865 top1 0.016\n",
      "Epoch 027/30 | train loss 4.5408 top1 0.018 | val loss 4.5960 top1 0.008\n",
      "Epoch 028/30 | train loss 4.5581 top1 0.026 | val loss 4.5525 top1 0.008\n",
      "Epoch 029/30 | train loss 4.5199 top1 0.013 | val loss 4.5653 top1 0.008\n",
      "Epoch 030/30 | train loss 4.5451 top1 0.018 | val loss 4.5351 top1 0.024\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "opt = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "scaler = GradScaler(\"cuda\", enabled=amp_on)\n",
    "warmup_epochs = 2\n",
    "sched = CosineAnnealingLR(opt, T_max=max(1, epochs - warmup_epochs), eta_min=lr*0.1)\n",
    "\n",
    "def run_epoch(loader, train=True, epoch=0):\n",
    "    model.train() if train else model.eval()\n",
    "    total_loss=total_top1=total_n=0.0\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    for step, (x,y,_) in enumerate(loader):\n",
    "        x=x.cuda(non_blocking=True); y=y.cuda(non_blocking=True)\n",
    "        with autocast(\"cuda\", enabled=amp_on):\n",
    "            logits = model(x)\n",
    "            loss = F.cross_entropy(logits, y) / grad_acc\n",
    "        if train:\n",
    "            scaler.scale(loss).backward()\n",
    "            if (step+1) % grad_acc == 0:\n",
    "                scaler.step(opt); scaler.update(); opt.zero_grad(set_to_none=True)\n",
    "        with torch.no_grad():\n",
    "            bs = x.size(0); total_n += bs\n",
    "            total_loss += (loss.item()*grad_acc)*bs\n",
    "            total_top1 += (logits.argmax(1)==y).float().sum().item()\n",
    "    if train:\n",
    "        if epoch < warmup_epochs:\n",
    "            for g in opt.param_groups:\n",
    "                g[\"lr\"] = lr * float(epoch + 1) / warmup_epochs\n",
    "        else:\n",
    "            sched.step()\n",
    "    return total_loss/total_n, total_top1/total_n\n",
    "\n",
    "# --- training loop ---\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    tr_loss, tr_top1 = run_epoch(train_loader, train=True,  epoch=epoch)\n",
    "    va_loss, va_top1 = run_epoch(val_loader,   train=False, epoch=epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:03d}/{epochs} | \"\n",
    "          f\"train loss {tr_loss:.4f} top1 {tr_top1:.3f} | \"\n",
    "          f\"val loss {va_loss:.4f} top1 {va_top1:.3f}\")\n",
    "\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optim_state\": opt.state_dict(),\n",
    "        \"scaler_state\": scaler.state_dict(),\n",
    "        \"best_metric\": best_val_top1,\n",
    "    }\n",
    "\n",
    "    # always save rolling \"last.pt\"\n",
    "    save_checkpoint(state, is_best=False, ckpt_dir=str(CKPT_DIR), filename=\"last.pt\")\n",
    "\n",
    "    # update \"best.pt\" if improved (let save_checkpoint copy last->best)\n",
    "    if va_top1 > best_val_top1:\n",
    "        best_val_top1 = va_top1\n",
    "        save_checkpoint(state, is_best=True, ckpt_dir=str(CKPT_DIR), filename=\"last.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83365209-11db-452b-9e6f-3bb2c5addcbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
