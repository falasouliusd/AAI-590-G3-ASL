{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdb629bf",
   "metadata": {},
   "source": [
    "###  Cell A â€” Imports, config, run tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1497cf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run tag: roi_top104_r3d18_k400_T32_B8_20251117T041330Z\n",
      "Manifest: /home/falasoul/notebooks/USD/AAI-590/Capstone/AAI-590-G3-ASL/data/wlasl_preprocessed/manifest_nslt2000_roi_top104_balanced_clean.csv\n"
     ]
    }
   ],
   "source": [
    "# === Cell A â€” Imports, CONFIG, run tag ===\n",
    "from pathlib import Path\n",
    "import sys, yaml, json\n",
    "from datetime import datetime\n",
    "\n",
    "# --- project paths ---\n",
    "root = Path(\"..\").resolve()\n",
    "if str(root) not in sys.path:\n",
    "    sys.path.insert(0, str(root))\n",
    "if str(root / \"src\") not in sys.path:\n",
    "    sys.path.insert(0, str(root / \"src\"))\n",
    "\n",
    "# --- load global YAML config ---\n",
    "cfg_path = root / \"configs\" / \"wlasl100.yaml\"\n",
    "CFG = yaml.safe_load(open(cfg_path, \"r\"))\n",
    "\n",
    "# === ROI manifest for top-104 balanced classes ===\n",
    "# CHANGE THIS if your cleaned manifest has a different name\n",
    "DATA_MANIFEST = root / \"data\" / \"wlasl_preprocessed\" / \"manifest_nslt2000_roi_top104_balanced_clean.csv\"\n",
    "\n",
    "assert DATA_MANIFEST.exists(), f\"Missing manifest: {DATA_MANIFEST}\"\n",
    "\n",
    "# --- base CONFIG (you can tweak) ---\n",
    "CONFIG = {\n",
    "    # data\n",
    "    \"clip_len\":        32,        # number of frames per clip\n",
    "    \"frame_stride\":    2,\n",
    "    \"batch_size\":      8,\n",
    "    \"num_workers\":     4,\n",
    "    \"use_weighted_sampler\": True, # balance classes via sampler\n",
    "\n",
    "    # model\n",
    "    \"backbone\":        \"r3d18_k400\",  # [\"c3dlite_gn\", \"r3d18_k400\", \"r2plus1d_k400\"]\n",
    "    \"dropout\":         0.2,\n",
    "    \"label_smoothing\": 0.0,       # 0.0..0.2\n",
    "    \"normalize\":       \"kinetics\",# [\"kinetics\",\"none\"]\n",
    "    \"amp\":             True,      # mixed precision\n",
    "    \"compile\":         True,      # torch.compile if available\n",
    "    \"seed\":            CFG[\"wlasl\"][\"split_seed\"],\n",
    "\n",
    "    # optimization\n",
    "    \"epochs\":          25,\n",
    "    \"lr\":              1e-4,      # lower LR for pretrained\n",
    "    \"weight_decay\":    1e-5,\n",
    "    \"grad_accum\":      1,\n",
    "    \"warmup_epochs\":   2,\n",
    "    \"cosine_eta_min\":  1e-5,\n",
    "\n",
    "    # staged fine-tuning (optional, wired in for r3d18_k400)\n",
    "    \"freeze_backbone\": True,      # stage 1: train head only\n",
    "    \"unfreeze_at_epoch\": 5,       # stage 2: unfreeze later\n",
    "    \"unfreeze_scope\":  \"layer4\",  # deepest block\n",
    "\n",
    "    # resume / checkpoints\n",
    "    \"resume\":          \"\",        # e.g. \"checkpoints/best_roi_r3d18.pt\"\n",
    "    \"save_every_epoch\": False,\n",
    "}\n",
    "\n",
    "# derived paths\n",
    "CKPT_DIR = root / CFG[\"paths\"][\"checkpoints_dir\"]\n",
    "LOG_DIR  = root / CFG[\"paths\"][\"logs_dir\"]\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# record this run config (nice for reproducibility)\n",
    "stamp = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "run_tag = f\"roi_top104_{CONFIG['backbone']}_T{CONFIG['clip_len']}_B{CONFIG['batch_size']}_{stamp}\"\n",
    "with open(LOG_DIR / f\"{run_tag}.config.json\", \"w\") as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "print(\"Run tag:\", run_tag)\n",
    "print(\"Manifest:\", DATA_MANIFEST)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ea8585",
   "metadata": {},
   "source": [
    "### Cell B â€” Dataset, Kinetics normalization, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40ea786a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes: 104\n",
      "Splits | train=831 val=192 test=136 | classes=104\n"
     ]
    }
   ],
   "source": [
    "# === Cell B â€” Dataset, normalization, loaders ===\n",
    "import torch, torch.nn.functional as F, numpy as np, pandas as pd\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "\n",
    "from src.utils.seed import seed_everything\n",
    "from src.data.wlasl_ds import WLASLDataset\n",
    "import src.data.wlasl_ds as wds_mod\n",
    "\n",
    "# ---------- reproducibility ----------\n",
    "seed_everything(CONFIG[\"seed\"])\n",
    "\n",
    "# ---------- Kinetics-style normalization ----------\n",
    "def kinetics_normalize(x):\n",
    "    # x: [T,C,H,W], float32 in [0,1]\n",
    "    mean = torch.tensor((0.432, 0.394, 0.376), dtype=x.dtype, device=x.device)[None,:,None,None]\n",
    "    std  = torch.tensor((0.228, 0.221, 0.223), dtype=x.dtype, device=x.device)[None,:,None,None]\n",
    "    return (x - mean) / std\n",
    "\n",
    "if CONFIG[\"normalize\"] == \"kinetics\":\n",
    "    wds_mod._normalize = kinetics_normalize\n",
    "elif CONFIG[\"normalize\"] == \"none\":\n",
    "    wds_mod._normalize = lambda x: x\n",
    "\n",
    "# ---------- load ROI manifest ----------\n",
    "m = pd.read_csv(DATA_MANIFEST)\n",
    "assert {\"path\",\"label_new\",\"split\"}.issubset(m.columns), m.columns\n",
    "\n",
    "# Use label_new as our contiguous label\n",
    "m = m.copy()\n",
    "m[\"label\"] = m[\"label_new\"].astype(int)\n",
    "num_classes = m[\"label\"].nunique()\n",
    "print(\"num_classes:\", num_classes)\n",
    "\n",
    "clip_len = CONFIG[\"clip_len\"]\n",
    "stride   = CONFIG[\"frame_stride\"]\n",
    "bs       = CONFIG[\"batch_size\"]\n",
    "nw       = CONFIG[\"num_workers\"]\n",
    "\n",
    "train_df = m[m.split == \"train\"].reset_index(drop=True)\n",
    "val_df   = m[m.split == \"val\"].reset_index(drop=True)\n",
    "test_df  = m[m.split == \"test\"].reset_index(drop=True)\n",
    "\n",
    "train_ds = WLASLDataset(train_df, clip_len=clip_len, stride=stride, train=True)\n",
    "val_ds   = WLASLDataset(val_df,   clip_len=clip_len, stride=stride, train=False)\n",
    "test_ds  = WLASLDataset(test_df,  clip_len=clip_len, stride=stride, train=False)\n",
    "\n",
    "if CONFIG[\"use_weighted_sampler\"]:\n",
    "    counts  = train_df[\"label\"].value_counts().to_dict()\n",
    "    weights = train_df[\"label\"].map(lambda y: 1.0 / counts[y]).values.astype(np.float32)\n",
    "    sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
    "    train_loader = DataLoader(train_ds, batch_size=bs, sampler=sampler,\n",
    "                              num_workers=nw, pin_memory=True)\n",
    "else:\n",
    "    train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True,\n",
    "                              num_workers=nw, pin_memory=True)\n",
    "\n",
    "val_loader  = DataLoader(val_ds,  batch_size=bs, shuffle=False,\n",
    "                         num_workers=nw, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=bs, shuffle=False,\n",
    "                         num_workers=nw, pin_memory=True)\n",
    "\n",
    "print(f\"Splits | train={len(train_ds)} val={len(val_ds)} test={len(test_ds)} | classes={num_classes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94cb6cb",
   "metadata": {},
   "source": [
    "#### Cell C â€” Models (C3Dlite and R3D18 K400, extendable to R(2+1)D later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8283e08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.compile: ON\n",
      "Backbone: r3d18_k400\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# === Cell C â€” Model factory (C3Dlite GN, R3D18 K400, extensible) ===\n",
    "import torch.nn as nn\n",
    "from torchvision.models.video import r3d_18, R3D_18_Weights\n",
    "\n",
    "# Small 3D GN CNN (from your old code)\n",
    "class C3DliteGN(nn.Module):\n",
    "    def __init__(self, num_classes=100, drop=0.5):\n",
    "        super().__init__()\n",
    "        def gn(c): return nn.GroupNorm(num_groups=8, num_channels=c)\n",
    "        def block(cin, cout, pool_t=2):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv3d(cin, cout, 3, padding=1, bias=False),\n",
    "                gn(cout), nn.ReLU(inplace=True),\n",
    "                nn.MaxPool3d(kernel_size=(pool_t,2,2), stride=(pool_t,2,2)),\n",
    "            )\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv3d(3, 32, 3, padding=1, bias=False),\n",
    "            gn(32), nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.b1 = block(32,  64)\n",
    "        self.b2 = block(64, 128)\n",
    "        self.b3 = block(128, 256)\n",
    "        self.b4 = block(256, 256)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool3d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):             # x [B,T,C,H,W]\n",
    "        x = x.permute(0,2,1,3,4).contiguous()   # [B,C,T,H,W]\n",
    "        x = self.stem(x)\n",
    "        x = self.b1(x); x = self.b2(x); x = self.b3(x); x = self.b4(x)\n",
    "        return self.head(x)\n",
    "\n",
    "class R3D18WithPermute(nn.Module):\n",
    "    \"\"\"Wrap r3d_18 to accept [B, T, C, H, W] and permute internally.\"\"\"\n",
    "    def __init__(self, num_classes, pretrained=True, dropout=0.2):\n",
    "        super().__init__()\n",
    "        weights = R3D_18_Weights.KINETICS400_V1 if pretrained else None\n",
    "        self.backbone = r3d_18(weights=weights)\n",
    "        in_feats = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Linear(in_feats, num_classes)\n",
    "\n",
    "    def forward(self, x):  # x: [B, T, C, H, W]\n",
    "        x = x.permute(0, 2, 1, 3, 4).contiguous()  # [B,C,T,H,W]\n",
    "        return self.backbone(x)\n",
    "\n",
    "def make_model(backbone, num_classes, drop):\n",
    "    if backbone == \"r3d18_k400\":\n",
    "        return R3D18WithPermute(num_classes=num_classes, pretrained=True, dropout=drop)\n",
    "    elif backbone == \"c3dlite_gn\":\n",
    "        return C3DliteGN(num_classes=num_classes, drop=drop)\n",
    "    # extend here later for r2plus1d_k400, mc3_k400, etc.\n",
    "    raise ValueError(f\"Unknown backbone: {backbone}\")\n",
    "\n",
    "# build model\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "model = make_model(CONFIG[\"backbone\"], num_classes=num_classes, drop=CONFIG[\"dropout\"]).to(device)\n",
    "\n",
    "if CONFIG[\"compile\"]:\n",
    "    try:\n",
    "        model = torch.compile(model)\n",
    "        print(\"torch.compile: ON\")\n",
    "    except Exception as e:\n",
    "        print(\"torch.compile skipped:\", e)\n",
    "\n",
    "print(\"Backbone:\", CONFIG[\"backbone\"])\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c013386d",
   "metadata": {},
   "source": [
    "#### Cell D â€” Optimizer, scheduler, scaler, optional staged unfreezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e525b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell D â€” Optimizer, scheduler, GradScaler, staged unfreezing ===\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.amp import GradScaler\n",
    "\n",
    "epochs   = int(CONFIG[\"epochs\"])\n",
    "lr       = float(CONFIG[\"lr\"])\n",
    "wd       = float(CONFIG[\"weight_decay\"])\n",
    "amp_on   = bool(CONFIG[\"amp\"])\n",
    "gs       = int(CONFIG[\"grad_accum\"])\n",
    "warmup   = int(CONFIG[\"warmup_epochs\"])\n",
    "eta_min  = float(CONFIG[\"cosine_eta_min\"])\n",
    "\n",
    "opt   = AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "sched = CosineAnnealingLR(opt, T_max=max(1, epochs - warmup), eta_min=eta_min)\n",
    "scaler = GradScaler(device.type if device.type == \"cuda\" else \"cpu\", enabled=amp_on)\n",
    "\n",
    "best_val = -1.0\n",
    "start_epoch = 0\n",
    "\n",
    "# optional resume\n",
    "resume = CONFIG.get(\"resume\") or \"\"\n",
    "if resume:\n",
    "    from src.utils.checkpoints import load_checkpoint\n",
    "    rp = root / resume\n",
    "    if rp.exists():\n",
    "        start_epoch, best_val = load_checkpoint(str(rp), model, opt, scaler)\n",
    "        print(f\"Resumed from {rp} @ epoch {start_epoch} best={best_val:.3f}\")\n",
    "\n",
    "def maybe_freeze_backbone(epoch):\n",
    "    \"\"\"Stage-1 freeze backbone, then unfreeze scope (e.g. layer4) after unfreeze_at_epoch.\"\"\"\n",
    "    if CONFIG[\"backbone\"] != \"r3d18_k400\":\n",
    "        return\n",
    "\n",
    "    freeze = CONFIG.get(\"freeze_backbone\", False)\n",
    "    unfreeze_at = CONFIG.get(\"unfreeze_at_epoch\", 0)\n",
    "    scope = CONFIG.get(\"unfreeze_scope\", \"layer4\")\n",
    "\n",
    "    if not freeze:\n",
    "        return\n",
    "\n",
    "    if epoch == 0:\n",
    "        # freeze all backbone parameters initially\n",
    "        for name, p in model.named_parameters():\n",
    "            if \"backbone\" in name:\n",
    "                p.requires_grad = False\n",
    "        # but keep final FC trainable\n",
    "        for name, p in model.named_parameters():\n",
    "            if \"backbone.fc\" in name:\n",
    "                p.requires_grad = True\n",
    "        print(\"[FT] Backbone frozen except final fc.\")\n",
    "\n",
    "    if epoch == unfreeze_at:\n",
    "        # unfreeze selected scope\n",
    "        for name, p in model.named_parameters():\n",
    "            if f\"backbone.{scope}\" in name:\n",
    "                p.requires_grad = True\n",
    "        print(f\"[FT] Unfroze backbone scope: {scope}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0758c3da",
   "metadata": {},
   "source": [
    "#### Cell E â€” Epoch runner + Training loop + Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5c31001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FT] Backbone frozen except final fc.\n",
      "Epoch 001/25 | train loss 4.7646 acc 0.007 | val loss 4.7375 acc 0.005\n",
      "Epoch 002/25 | train loss 4.6808 acc 0.012 | val loss 4.6974 acc 0.016\n",
      "Epoch 003/25 | train loss 4.6265 acc 0.016 | val loss 4.6832 acc 0.005\n",
      "Epoch 004/25 | train loss 4.5736 acc 0.035 | val loss 4.6754 acc 0.010\n",
      "Epoch 005/25 | train loss 4.5254 acc 0.030 | val loss 4.6655 acc 0.016\n",
      "[FT] Unfroze backbone scope: layer4\n",
      "Epoch 006/25 | train loss 4.4755 acc 0.048 | val loss 4.6502 acc 0.010\n",
      "Epoch 007/25 | train loss 4.4548 acc 0.047 | val loss 4.6603 acc 0.005\n",
      "Epoch 008/25 | train loss 4.4253 acc 0.053 | val loss 4.6430 acc 0.021\n",
      "Epoch 009/25 | train loss 4.3962 acc 0.066 | val loss 4.6476 acc 0.021\n",
      "Epoch 010/25 | train loss 4.3879 acc 0.082 | val loss 4.6300 acc 0.026\n",
      "Epoch 011/25 | train loss 4.3482 acc 0.108 | val loss 4.6252 acc 0.031\n",
      "Epoch 012/25 | train loss 4.3257 acc 0.100 | val loss 4.6111 acc 0.036\n",
      "Epoch 013/25 | train loss 4.2766 acc 0.123 | val loss 4.5978 acc 0.026\n",
      "Epoch 014/25 | train loss 4.2679 acc 0.142 | val loss 4.5903 acc 0.047\n",
      "Epoch 015/25 | train loss 4.2148 acc 0.166 | val loss 4.5922 acc 0.057\n",
      "Epoch 016/25 | train loss 4.2451 acc 0.154 | val loss 4.5847 acc 0.036\n",
      "Epoch 017/25 | train loss 4.1969 acc 0.167 | val loss 4.5711 acc 0.047\n",
      "Epoch 018/25 | train loss 4.2020 acc 0.174 | val loss 4.5747 acc 0.036\n",
      "Epoch 019/25 | train loss 4.1717 acc 0.213 | val loss 4.5716 acc 0.052\n",
      "Epoch 020/25 | train loss 4.1600 acc 0.231 | val loss 4.5632 acc 0.047\n",
      "Epoch 021/25 | train loss 4.1649 acc 0.190 | val loss 4.5628 acc 0.062\n",
      "Epoch 022/25 | train loss 4.1716 acc 0.205 | val loss 4.5568 acc 0.057\n",
      "Epoch 023/25 | train loss 4.1325 acc 0.229 | val loss 4.5609 acc 0.068\n",
      "Epoch 024/25 | train loss 4.1321 acc 0.221 | val loss 4.5576 acc 0.068\n",
      "Epoch 025/25 | train loss 4.1574 acc 0.193 | val loss 4.5545 acc 0.068\n"
     ]
    }
   ],
   "source": [
    "# === Cell E â€” Train / eval loops, checkpointing ===\n",
    "from src.utils.checkpoints import save_checkpoint\n",
    "\n",
    "def top1_acc(logits, y):\n",
    "    with torch.no_grad():\n",
    "        return (logits.argmax(1) == y).float().mean().item()\n",
    "\n",
    "def run_epoch(loader, train=True, epoch=0):\n",
    "    model.train() if train else model.eval()\n",
    "    tot_loss = tot_acc = tot_n = 0.0\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "\n",
    "    for step, (x, y, _) in enumerate(loader):\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=amp_on):\n",
    "            logits = model(x)\n",
    "            loss = F.cross_entropy(\n",
    "                logits, y,\n",
    "                label_smoothing=CONFIG[\"label_smoothing\"]\n",
    "            ) / gs\n",
    "\n",
    "        if train:\n",
    "            scaler.scale(loss).backward()\n",
    "            if (step + 1) % gs == 0:\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            bs = x.size(0)\n",
    "            tot_loss += (loss.item() * gs) * bs\n",
    "            tot_acc  += top1_acc(logits, y) * bs\n",
    "            tot_n    += bs\n",
    "\n",
    "    if train:\n",
    "        if epoch < warmup:\n",
    "            # linear warmup\n",
    "            warm_lr = lr * float(epoch + 1) / max(1, warmup)\n",
    "            for g in opt.param_groups:\n",
    "                g[\"lr\"] = warm_lr\n",
    "        else:\n",
    "            sched.step()\n",
    "\n",
    "    return tot_loss / max(1, tot_n), tot_acc / max(1, tot_n)\n",
    "\n",
    "# ----- MAIN TRAINING LOOP -----\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    maybe_freeze_backbone(epoch)\n",
    "\n",
    "    tr_loss, tr_acc = run_epoch(train_loader, train=True,  epoch=epoch)\n",
    "    va_loss, va_acc = run_epoch(val_loader,   train=False, epoch=epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:03d}/{epochs} | \"\n",
    "          f\"train loss {tr_loss:.4f} acc {tr_acc:.3f} | \"\n",
    "          f\"val loss {va_loss:.4f} acc {va_acc:.3f}\")\n",
    "\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optim_state\": opt.state_dict(),\n",
    "        \"scaler_state\": scaler.state_dict(),\n",
    "        \"best_metric\": best_val,\n",
    "    }\n",
    "    # always save last\n",
    "    save_checkpoint(state, is_best=False, ckpt_dir=str(CKPT_DIR), filename=f\"{run_tag}_last.pt\")\n",
    "\n",
    "    if CONFIG[\"save_every_epoch\"]:\n",
    "        save_checkpoint(state, is_best=False, ckpt_dir=str(CKPT_DIR),\n",
    "                        filename=f\"{run_tag}_epoch_{epoch:04d}.pt\")\n",
    "\n",
    "    if va_acc > best_val:\n",
    "        best_val = va_acc\n",
    "        save_checkpoint(state, is_best=True, ckpt_dir=str(CKPT_DIR), filename=f\"{run_tag}_best.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881fd9a6",
   "metadata": {},
   "source": [
    "#### Cell F â€” run_epoch (with correct [B,C,T,H,W] permute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68cf500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell F â€” Metrics + epoch runner (for CNN+BiGRU) ===\n",
    "\n",
    "def top1_acc(logits, y):\n",
    "    return (logits.argmax(1) == y).float().mean().item()\n",
    "\n",
    "def run_epoch(loader, train=True):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_acc  = 0.0\n",
    "    total_n    = 0\n",
    "\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "\n",
    "    for x, y, _ in loader:\n",
    "        x = x.to(device, non_blocking=True)  # [B,T,C,H,W]\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        # ðŸ”½ New: permute for 3D CNN\n",
    "        x = x.permute(0, 2, 1, 3, 4).contiguous()  # [B,C,T,H,W]\n",
    "\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=amp_on):\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "        if train:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(opt)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            acc = top1_acc(logits, y)\n",
    "            bs  = x.size(0)\n",
    "            total_loss += loss.item() * bs\n",
    "            total_acc  += acc * bs\n",
    "            total_n    += bs\n",
    "\n",
    "    return total_loss / total_n, total_acc / total_n\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03f005f",
   "metadata": {},
   "source": [
    "#### Cell G â€” Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5efa56b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/20 | train loss 1.3226 acc 0.936 | val loss 3.2730 acc 0.406\n",
      "  âžœ New best val acc=0.406 \n",
      "Epoch 02/20 | train loss 1.1859 acc 0.966 | val loss 3.3076 acc 0.396\n",
      "Epoch 03/20 | train loss 1.1111 acc 0.969 | val loss 3.2866 acc 0.417\n",
      "  âžœ New best val acc=0.417 \n",
      "Epoch 04/20 | train loss 1.0726 acc 0.971 | val loss 3.2002 acc 0.396\n",
      "Epoch 05/20 | train loss 1.0412 acc 0.976 | val loss 3.3046 acc 0.380\n",
      "Epoch 06/20 | train loss 1.0268 acc 0.977 | val loss 3.3609 acc 0.401\n",
      "Epoch 07/20 | train loss 1.0143 acc 0.978 | val loss 3.4485 acc 0.375\n",
      "Epoch 08/20 | train loss 1.0180 acc 0.980 | val loss 3.5573 acc 0.396\n",
      "Epoch 09/20 | train loss 0.9648 acc 0.986 | val loss 3.5597 acc 0.411\n",
      "Epoch 10/20 | train loss 0.9828 acc 0.982 | val loss 3.3082 acc 0.438\n",
      "  âžœ New best val acc=0.438 \n",
      "Epoch 11/20 | train loss 0.9577 acc 0.984 | val loss 3.4350 acc 0.422\n",
      "Epoch 12/20 | train loss 0.9466 acc 0.989 | val loss 3.4187 acc 0.391\n",
      "Epoch 13/20 | train loss 0.9667 acc 0.982 | val loss 3.5652 acc 0.391\n",
      "Epoch 14/20 | train loss 0.9376 acc 0.986 | val loss 3.4205 acc 0.385\n",
      "Epoch 15/20 | train loss 0.9630 acc 0.978 | val loss 3.2487 acc 0.427\n",
      "Epoch 16/20 | train loss 0.9413 acc 0.987 | val loss 3.3495 acc 0.438\n",
      "Epoch 17/20 | train loss 0.9173 acc 0.990 | val loss 3.6324 acc 0.417\n",
      "Epoch 18/20 | train loss 0.8992 acc 0.996 | val loss 3.1877 acc 0.484\n",
      "  âžœ New best val acc=0.484 \n",
      "Epoch 19/20 | train loss 0.9036 acc 0.990 | val loss 3.2591 acc 0.432\n",
      "Epoch 20/20 | train loss 0.9104 acc 0.990 | val loss 3.6644 acc 0.417\n"
     ]
    }
   ],
   "source": [
    "# === Cell G â€” Training Loop ===\n",
    "ckpt_dir = root / \"checkpoints\"\n",
    "ckpt_dir.mkdir(exist_ok=True)\n",
    "\n",
    "best_val_acc = -1.0\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    tr_loss, tr_acc = run_epoch(train_loader, train=True)\n",
    "    va_loss, va_acc = run_epoch(val_loader,   train=False)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}/{epochs} | \"\n",
    "          f\"train loss {tr_loss:.4f} acc {tr_acc:.3f} | \"\n",
    "          f\"val loss {va_loss:.4f} acc {va_acc:.3f}\")\n",
    "\n",
    "    if va_acc > best_val_acc:\n",
    "        best_val_acc = va_acc\n",
    "        save_path = ckpt_dir / \"best_r2plus1d_k400_top104.pt\"\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"  âžœ New best val acc={best_val_acc:.3f} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6789f1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST â€” loss 3.2564 | acc 0.449\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = root / \"checkpoints\"\n",
    "best_path = ckpt_dir / \"best_r2plus1d_k400_top104.pt\"\n",
    "\n",
    "model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "test_loss, test_acc = run_epoch(test_loader, train=False)\n",
    "print(f\"TEST â€” loss {test_loss:.4f} | acc {test_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4515dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1a5a59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
