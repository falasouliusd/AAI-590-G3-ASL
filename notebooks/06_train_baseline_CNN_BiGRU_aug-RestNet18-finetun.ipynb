{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdb629bf",
   "metadata": {},
   "source": [
    "### Cell A â€” Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1497cf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: /home/falasoul/notebooks/USD/AAI-590/Capstone/AAI-590-G3-ASL/data/wlasl_preprocessed/manifest_nslt2000_roi_top104_balanced_clean.csv\n",
      "Samples: 1159 | classes=104\n",
      "Columns: ['video_id', 'path', 'gloss', 'label', 'split', 'exists', 'label_new']\n",
      "label_new min/max: 0 103\n",
      "label_new nunique: 104\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>path</th>\n",
       "      <th>gloss</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "      <th>exists</th>\n",
       "      <th>label_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>639</td>\n",
       "      <td>/home/falasoul/notebooks/USD/AAI-590/Capstone/...</td>\n",
       "      <td>accident</td>\n",
       "      <td>8</td>\n",
       "      <td>train</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>624</td>\n",
       "      <td>/home/falasoul/notebooks/USD/AAI-590/Capstone/...</td>\n",
       "      <td>accident</td>\n",
       "      <td>8</td>\n",
       "      <td>train</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>632</td>\n",
       "      <td>/home/falasoul/notebooks/USD/AAI-590/Capstone/...</td>\n",
       "      <td>accident</td>\n",
       "      <td>8</td>\n",
       "      <td>train</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>623</td>\n",
       "      <td>/home/falasoul/notebooks/USD/AAI-590/Capstone/...</td>\n",
       "      <td>accident</td>\n",
       "      <td>8</td>\n",
       "      <td>train</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65009</td>\n",
       "      <td>/home/falasoul/notebooks/USD/AAI-590/Capstone/...</td>\n",
       "      <td>accident</td>\n",
       "      <td>8</td>\n",
       "      <td>train</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   video_id                                               path     gloss  \\\n",
       "0       639  /home/falasoul/notebooks/USD/AAI-590/Capstone/...  accident   \n",
       "1       624  /home/falasoul/notebooks/USD/AAI-590/Capstone/...  accident   \n",
       "2       632  /home/falasoul/notebooks/USD/AAI-590/Capstone/...  accident   \n",
       "3       623  /home/falasoul/notebooks/USD/AAI-590/Capstone/...  accident   \n",
       "4     65009  /home/falasoul/notebooks/USD/AAI-590/Capstone/...  accident   \n",
       "\n",
       "   label  split  exists  label_new  \n",
       "0      8  train    True          0  \n",
       "1      8  train    True          0  \n",
       "2      8  train    True          0  \n",
       "3      8  train    True          0  \n",
       "4      8  train    True          0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Cell A â€” Imports, reproducibility, load balanced ROI manifest ===\n",
    "import os, random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import cv2\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Keep CPU threads tame\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "root = Path(\"..\").resolve()\n",
    "data_dir = root / \"data\" / \"wlasl_preprocessed\"\n",
    "\n",
    "# Use the specific balanced ROI manifest you showed\n",
    "man_path = data_dir / \"manifest_nslt2000_roi_top104_balanced_clean.csv\"\n",
    "assert man_path.exists(), f\"Manifest not found: {man_path}\"\n",
    "\n",
    "df = pd.read_csv(man_path)\n",
    "print(\"Loaded:\", man_path)\n",
    "print(f\"Samples: {len(df)} | classes={df['gloss'].nunique()}\")\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "print(\"label_new min/max:\", df[\"label_new\"].min(), df[\"label_new\"].max())\n",
    "print(\"label_new nunique:\", df[\"label_new\"].nunique())\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ea8585",
   "metadata": {},
   "source": [
    "### Cell B â€” WLASLDataset (using ROI + label_new) + WLASLDataset with on-the-fly augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40ea786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell B â€” WLASLDataset (ROI, label_new) with safe loading + on-the-fly augmentation ===\n",
    "import torch, numpy as np, cv2, decord, random\n",
    "from torch.utils.data import Dataset\n",
    "decord.bridge.set_bridge('torch')\n",
    "\n",
    "\n",
    "def _resize_112(frame_tchw: torch.Tensor) -> torch.Tensor:\n",
    "    T, C, H, W = frame_tchw.shape\n",
    "    arr = frame_tchw.permute(0, 2, 3, 1).cpu().numpy()\n",
    "    out = np.empty((T, 112, 112, C), dtype=np.float32)\n",
    "    for t in range(T):\n",
    "        out[t] = cv2.resize(arr[t], (112, 112), interpolation=cv2.INTER_AREA)\n",
    "    return torch.from_numpy(out).permute(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "def _normalize(frame_tchw, mean=(0.45,)*3, std=(0.225,)*3):\n",
    "    mean = torch.tensor(mean, dtype=frame_tchw.dtype, device=frame_tchw.device)[None, :, None, None]\n",
    "    std  = torch.tensor(std,  dtype=frame_tchw.dtype, device=frame_tchw.device)[None, :, None, None]\n",
    "    return (frame_tchw - mean) / std\n",
    "\n",
    "\n",
    "def uniform_temporal_indices(n_total, clip_len, stride):\n",
    "    if n_total <= 0:\n",
    "        return [0] * clip_len\n",
    "    wanted = (clip_len - 1) * stride + 1\n",
    "    if n_total >= wanted:\n",
    "        start = (n_total - wanted) // 2\n",
    "        return [start + i * stride for i in range(clip_len)]\n",
    "    idxs = [min(i * stride, n_total - 1) for i in range(clip_len)]\n",
    "    return idxs\n",
    "\n",
    "\n",
    "class WLASLDataset(Dataset):\n",
    "    def __init__(self, df, clip_len=32, stride=2, train=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.clip_len = clip_len\n",
    "        self.stride = stride\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    # --------- augmentation helpers (on-the-fly, train only) ---------\n",
    "    def _augment(self, frames: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        frames: [T, C, H, W], values in [0,1] (float32).\n",
    "        Only applied when self.train == True.\n",
    "        \"\"\"\n",
    "        if not self.train:\n",
    "            return frames\n",
    "\n",
    "        T, C, H, W = frames.shape\n",
    "\n",
    "        # 1) Random horizontal flip\n",
    "        if random.random() < 0.5:\n",
    "            frames = torch.flip(frames, dims=[3])  # flip width\n",
    "\n",
    "        # 2) Random Gaussian blur\n",
    "        if random.random() < 0.3:\n",
    "            k = random.choice([3, 5])\n",
    "            fr_np = frames.permute(0, 2, 3, 1).cpu().numpy()  # [T,H,W,C]\n",
    "            for t in range(T):\n",
    "                fr_np[t] = cv2.GaussianBlur(fr_np[t], (k, k), 0)\n",
    "            frames = torch.from_numpy(fr_np).permute(0, 3, 1, 2)\n",
    "\n",
    "        # 3) Random brightness / contrast\n",
    "        if random.random() < 0.3:\n",
    "            alpha = 1.0 + 0.4 * (random.random() - 0.5)   # contrast ~ [0.8, 1.2]\n",
    "            beta  = 0.1 * (random.random() - 0.5)         # brightness ~ [-0.05, 0.05]\n",
    "            frames = frames * alpha + beta\n",
    "            frames = frames.clamp(0.0, 1.0)\n",
    "\n",
    "        # 4) Random cutout mask (simulate occlusion)\n",
    "        if random.random() < 0.3:\n",
    "            mask_size = random.randint(16, 40)\n",
    "            y0 = random.randint(0, max(0, H - mask_size))\n",
    "            x0 = random.randint(0, max(0, W - mask_size))\n",
    "            frames[:, :, y0:y0+mask_size, x0:x0+mask_size] = 0.0\n",
    "\n",
    "        return frames\n",
    "\n",
    "    def _safe_load_clip(self, path: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Try to read a clip with decord. If anything fails, return a dummy zero clip.\n",
    "        Returned shape: [T, C, H, W], float32 in [0,1].\n",
    "        \"\"\"\n",
    "        try:\n",
    "            vr = decord.VideoReader(path)\n",
    "            n = len(vr)\n",
    "            if n <= 0:\n",
    "                raise RuntimeError(\"no frames\")\n",
    "\n",
    "            idxs = uniform_temporal_indices(n, self.clip_len, self.stride)\n",
    "            batch = vr.get_batch(idxs)        # [T,H,W,C]\n",
    "            x = batch.float() / 255.0         # [0,1]\n",
    "            x = x.permute(0, 3, 1, 2)         # [T,C,H,W]\n",
    "            x = _resize_112(x)                # [T,3,112,112]\n",
    "            x = self._augment(x)              # on-the-fly aug (train only)\n",
    "            x = _normalize(x)                 # final normalization\n",
    "            return x\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to read video {path}: {e} â€” using zero clip.\")\n",
    "            x = torch.zeros(self.clip_len, 3, 112, 112, dtype=torch.float32)\n",
    "            x = _normalize(x)\n",
    "            return x\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        row = self.df.iloc[i]\n",
    "        path = row[\"path\"]\n",
    "        label = int(row[\"label_new\"])   # contiguous 0..C-1\n",
    "\n",
    "        x = self._safe_load_clip(path)\n",
    "        return x, label, path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94cb6cb",
   "metadata": {},
   "source": [
    "#### Cell C â€” Split DataFrames & DataLoaders (single worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8283e08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes: 831 train | 192 val | 136 test\n",
      "Device: cuda\n",
      "Sample batch shape: torch.Size([4, 32, 3, 112, 112]) | labels range: 60 -> 88\n"
     ]
    }
   ],
   "source": [
    "# === Cell C â€” Splits + DataLoaders (no multiprocessing) ===\n",
    "\n",
    "train_df = df[df[\"split\"] == \"train\"].reset_index(drop=True)\n",
    "val_df   = df[df[\"split\"] == \"val\"].reset_index(drop=True)\n",
    "test_df  = df[df[\"split\"] == \"test\"].reset_index(drop=True)\n",
    "\n",
    "print(\"Split sizes:\", len(train_df), \"train |\", len(val_df), \"val |\", len(test_df), \"test\")\n",
    "\n",
    "clip_len = 32\n",
    "stride   = 2\n",
    "batch_size = 4   # small to be safe on GPU\n",
    "\n",
    "train_ds = WLASLDataset(train_df, clip_len=clip_len, stride=stride, train=True)\n",
    "val_ds   = WLASLDataset(val_df,   clip_len=clip_len, stride=stride, train=False)\n",
    "test_ds  = WLASLDataset(test_df,  clip_len=clip_len, stride=stride, train=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,              # ðŸ”’ NO worker processes\n",
    "    pin_memory=(device.type == \"cuda\"),\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=(device.type == \"cuda\"),\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=(device.type == \"cuda\"),\n",
    ")\n",
    "\n",
    "x_dbg, y_dbg, _ = next(iter(train_loader))\n",
    "print(\"Sample batch shape:\", x_dbg.shape, \"| labels range:\", y_dbg.min().item(), \"->\", y_dbg.max().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c013386d",
   "metadata": {},
   "source": [
    "#### Cell D â€” Model: R3D-18 baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e525b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes: 104\n",
      "Model on: cuda\n"
     ]
    }
   ],
   "source": [
    "# === Cell D â€” CNN + BiGRU model with PRETRAINED ResNet-18 ===\n",
    "\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "class CnnBiGRUClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        rnn_hidden: int = 256,\n",
    "        rnn_layers: int = 1,\n",
    "        dropout: float = 0.3,\n",
    "        use_pretrained: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 2D CNN backbone (ResNet-18)\n",
    "        # use_pretrained=True -> ImageNet weights, False -> from scratch\n",
    "        if use_pretrained:\n",
    "            try:\n",
    "                # Newer torchvision API\n",
    "                from torchvision.models import ResNet18_Weights\n",
    "                weights = ResNet18_Weights.IMAGENET1K_V1\n",
    "                base = resnet18(weights=weights)\n",
    "            except Exception:\n",
    "                # Fallback for older versions\n",
    "                base = resnet18(pretrained=True)\n",
    "        else:\n",
    "            base = resnet18(weights=None)\n",
    "\n",
    "        # Take everything except the final FC and global pool\n",
    "        self.cnn = nn.Sequential(*list(base.children())[:-2])  # conv -> layer4\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.cnn_out_dim = base.fc.in_features  # 512 for ResNet-18\n",
    "\n",
    "        # BiGRU over time\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=self.cnn_out_dim,\n",
    "            hidden_size=rnn_hidden,\n",
    "            num_layers=rnn_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(rnn_hidden * 2, num_classes)  # *2 for bidirectional\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [B, T, C, H, W]\n",
    "        \"\"\"\n",
    "        B, T, C, H, W = x.shape\n",
    "\n",
    "        # Merge batch and time to process frames with 2D CNN\n",
    "        x = x.view(B * T, C, H, W)              # [B*T, C, H, W]\n",
    "\n",
    "        feats = self.cnn(x)                     # [B*T, C', h, w]\n",
    "        feats = self.pool(feats)                # [B*T, C', 1, 1]\n",
    "        feats = feats.view(B, T, self.cnn_out_dim)  # [B, T, F]\n",
    "\n",
    "        # BiGRU over temporal dimension\n",
    "        rnn_out, _ = self.rnn(feats)           # [B, T, 2*hidden]\n",
    "\n",
    "        # Use last time step\n",
    "        last = rnn_out[:, -1, :]               # [B, 2*hidden]\n",
    "\n",
    "        out = self.dropout(last)\n",
    "        logits = self.fc(out)                  # [B, num_classes]\n",
    "        return logits\n",
    "\n",
    "# Build PRETRAINED model\n",
    "num_classes = df[\"label_new\"].nunique()\n",
    "print(\"num_classes:\", num_classes)\n",
    "\n",
    "model = CnnBiGRUClassifier(\n",
    "    num_classes=num_classes,\n",
    "    rnn_hidden=256,\n",
    "    rnn_layers=1,\n",
    "    dropout=0.3,\n",
    "    use_pretrained=True,    # ðŸ”´ this is the key difference\n",
    ").to(device)\n",
    "\n",
    "print(\"Model on:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0758c3da",
   "metadata": {},
   "source": [
    "#### Cell E â€” Optimizer, Scaler, Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5c31001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell E â€” Optimizer, scaler, loss (pretrained) ===\n",
    "from torch.amp import GradScaler\n",
    "\n",
    "epochs = 20\n",
    "lr     = 1e-4      # ðŸ”½ slightly lower than 3e-4 used for scratch\n",
    "wd     = 1e-2\n",
    "amp_on = True\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "scaler = GradScaler(enabled=amp_on)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # can keep or remove smoothing\n",
    "best_val_acc = -1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881fd9a6",
   "metadata": {},
   "source": [
    "#### Cell F â€” run_epoch (with correct [B,C,T,H,W] permute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68cf500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell F â€” Metrics + epoch runner (for CNN+BiGRU) ===\n",
    "\n",
    "def top1_acc(logits, y):\n",
    "    return (logits.argmax(1) == y).float().mean().item()\n",
    "\n",
    "def run_epoch(loader, train=True):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_acc  = 0.0\n",
    "    total_n    = 0\n",
    "\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "\n",
    "    for x, y, _ in loader:\n",
    "        # x is already [B, T, C, H, W] from WLASLDataset\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=amp_on):\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "        if train:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            acc = top1_acc(logits, y)\n",
    "            bs  = x.size(0)\n",
    "            total_loss += loss.item() * bs\n",
    "            total_acc  += acc * bs\n",
    "            total_n    += bs\n",
    "\n",
    "    return total_loss / total_n, total_acc / total_n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03f005f",
   "metadata": {},
   "source": [
    "#### Cell G â€” Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efa56b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell G â€” Training Loop ===\n",
    "root = Path(\"..\").resolve()\n",
    "ckpt_dir = root / \"checkpoints\"\n",
    "best_path = ckpt_dir / \"best_cnn_bigru_pretrained_top104.pt\"\n",
    "save_path = ckpt_dir / \"best_cnn_bigru_pretrained_top104.pt\"\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    tr_loss, tr_acc = run_epoch(train_loader, train=True)\n",
    "    va_loss, va_acc = run_epoch(val_loader,   train=False)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}/{epochs} | \"\n",
    "          f\"train loss {tr_loss:.4f} acc {tr_acc:.3f} | \"\n",
    "          f\"val loss {va_loss:.4f} acc {va_acc:.3f}\")\n",
    "\n",
    "    if va_acc > best_val_acc:\n",
    "        best_val_acc = va_acc\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"  âžœ New best val acc={best_val_acc:.3f} (model saved)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6789f1a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
