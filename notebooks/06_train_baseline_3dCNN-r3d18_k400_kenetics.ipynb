{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdb629bf",
   "metadata": {},
   "source": [
    "### Cell A â€” Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1497cf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: /home/falasoul/notebooks/USD/AAI-590/Capstone/AAI-590-G3-ASL/data/wlasl_preprocessed/manifest_nslt2000_roi_top104_balanced_clean.csv\n",
      "Samples: 1159 | classes=104\n",
      "Columns: ['video_id', 'path', 'gloss', 'label', 'split', 'exists', 'label_new']\n",
      "label_new min/max: 0 103\n",
      "label_new nunique: 104\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>path</th>\n",
       "      <th>gloss</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "      <th>exists</th>\n",
       "      <th>label_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>639</td>\n",
       "      <td>/home/falasoul/notebooks/USD/AAI-590/Capstone/...</td>\n",
       "      <td>accident</td>\n",
       "      <td>8</td>\n",
       "      <td>train</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>624</td>\n",
       "      <td>/home/falasoul/notebooks/USD/AAI-590/Capstone/...</td>\n",
       "      <td>accident</td>\n",
       "      <td>8</td>\n",
       "      <td>train</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>632</td>\n",
       "      <td>/home/falasoul/notebooks/USD/AAI-590/Capstone/...</td>\n",
       "      <td>accident</td>\n",
       "      <td>8</td>\n",
       "      <td>train</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>623</td>\n",
       "      <td>/home/falasoul/notebooks/USD/AAI-590/Capstone/...</td>\n",
       "      <td>accident</td>\n",
       "      <td>8</td>\n",
       "      <td>train</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65009</td>\n",
       "      <td>/home/falasoul/notebooks/USD/AAI-590/Capstone/...</td>\n",
       "      <td>accident</td>\n",
       "      <td>8</td>\n",
       "      <td>train</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   video_id                                               path     gloss  \\\n",
       "0       639  /home/falasoul/notebooks/USD/AAI-590/Capstone/...  accident   \n",
       "1       624  /home/falasoul/notebooks/USD/AAI-590/Capstone/...  accident   \n",
       "2       632  /home/falasoul/notebooks/USD/AAI-590/Capstone/...  accident   \n",
       "3       623  /home/falasoul/notebooks/USD/AAI-590/Capstone/...  accident   \n",
       "4     65009  /home/falasoul/notebooks/USD/AAI-590/Capstone/...  accident   \n",
       "\n",
       "   label  split  exists  label_new  \n",
       "0      8  train    True          0  \n",
       "1      8  train    True          0  \n",
       "2      8  train    True          0  \n",
       "3      8  train    True          0  \n",
       "4      8  train    True          0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Cell A â€” Imports, reproducibility, load balanced ROI manifest ===\n",
    "import os, random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import cv2\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Keep CPU threads tame\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "root = Path(\"..\").resolve()\n",
    "data_dir = root / \"data\" / \"wlasl_preprocessed\"\n",
    "\n",
    "# Use the specific balanced ROI manifest you showed\n",
    "man_path = data_dir / \"manifest_nslt2000_roi_top104_balanced_clean.csv\"\n",
    "assert man_path.exists(), f\"Manifest not found: {man_path}\"\n",
    "\n",
    "df = pd.read_csv(man_path)\n",
    "print(\"Loaded:\", man_path)\n",
    "print(f\"Samples: {len(df)} | classes={df['gloss'].nunique()}\")\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "print(\"label_new min/max:\", df[\"label_new\"].min(), df[\"label_new\"].max())\n",
    "print(\"label_new nunique:\", df[\"label_new\"].nunique())\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ea8585",
   "metadata": {},
   "source": [
    "### Cell B â€” WLASLDataset (using ROI + label_new) + WLASLDataset with on-the-fly augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40ea786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell B â€” WLASLDataset (ROI, label_new) with safe loading + on-the-fly augmentation ===\n",
    "import torch, numpy as np, cv2, decord, random\n",
    "from torch.utils.data import Dataset\n",
    "decord.bridge.set_bridge('torch')\n",
    "\n",
    "\n",
    "def _resize_112(frame_tchw: torch.Tensor) -> torch.Tensor:\n",
    "    T, C, H, W = frame_tchw.shape\n",
    "    arr = frame_tchw.permute(0, 2, 3, 1).cpu().numpy()\n",
    "    out = np.empty((T, 112, 112, C), dtype=np.float32)\n",
    "    for t in range(T):\n",
    "        out[t] = cv2.resize(arr[t], (112, 112), interpolation=cv2.INTER_AREA)\n",
    "    return torch.from_numpy(out).permute(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "def _normalize(frame_tchw, mean=(0.45,)*3, std=(0.225,)*3):\n",
    "    mean = torch.tensor(mean, dtype=frame_tchw.dtype, device=frame_tchw.device)[None, :, None, None]\n",
    "    std  = torch.tensor(std,  dtype=frame_tchw.dtype, device=frame_tchw.device)[None, :, None, None]\n",
    "    return (frame_tchw - mean) / std\n",
    "\n",
    "\n",
    "def uniform_temporal_indices(n_total, clip_len, stride):\n",
    "    if n_total <= 0:\n",
    "        return [0] * clip_len\n",
    "    wanted = (clip_len - 1) * stride + 1\n",
    "    if n_total >= wanted:\n",
    "        start = (n_total - wanted) // 2\n",
    "        return [start + i * stride for i in range(clip_len)]\n",
    "    idxs = [min(i * stride, n_total - 1) for i in range(clip_len)]\n",
    "    return idxs\n",
    "\n",
    "\n",
    "class WLASLDataset(Dataset):\n",
    "    def __init__(self, df, clip_len=32, stride=2, train=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.clip_len = clip_len\n",
    "        self.stride = stride\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    # --------- augmentation helpers (on-the-fly, train only) ---------\n",
    "    def _augment(self, frames: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        frames: [T, C, H, W], values in [0,1] (float32).\n",
    "        Only applied when self.train == True.\n",
    "        \"\"\"\n",
    "        if not self.train:\n",
    "            return frames\n",
    "\n",
    "        T, C, H, W = frames.shape\n",
    "\n",
    "        # 1) Random horizontal flip\n",
    "        if random.random() < 0.5:\n",
    "            frames = torch.flip(frames, dims=[3])  # flip width\n",
    "\n",
    "        # 2) Random Gaussian blur\n",
    "        if random.random() < 0.3:\n",
    "            k = random.choice([3, 5])\n",
    "            fr_np = frames.permute(0, 2, 3, 1).cpu().numpy()  # [T,H,W,C]\n",
    "            for t in range(T):\n",
    "                fr_np[t] = cv2.GaussianBlur(fr_np[t], (k, k), 0)\n",
    "            frames = torch.from_numpy(fr_np).permute(0, 3, 1, 2)\n",
    "\n",
    "        # 3) Random brightness / contrast\n",
    "        if random.random() < 0.3:\n",
    "            alpha = 1.0 + 0.4 * (random.random() - 0.5)   # contrast ~ [0.8, 1.2]\n",
    "            beta  = 0.1 * (random.random() - 0.5)         # brightness ~ [-0.05, 0.05]\n",
    "            frames = frames * alpha + beta\n",
    "            frames = frames.clamp(0.0, 1.0)\n",
    "\n",
    "        # 4) Random cutout mask (simulate occlusion)\n",
    "        if random.random() < 0.3:\n",
    "            mask_size = random.randint(16, 40)\n",
    "            y0 = random.randint(0, max(0, H - mask_size))\n",
    "            x0 = random.randint(0, max(0, W - mask_size))\n",
    "            frames[:, :, y0:y0+mask_size, x0:x0+mask_size] = 0.0\n",
    "\n",
    "        return frames\n",
    "\n",
    "    def _safe_load_clip(self, path: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Try to read a clip with decord. If anything fails, return a dummy zero clip.\n",
    "        Returned shape: [T, C, H, W], float32 in [0,1].\n",
    "        \"\"\"\n",
    "        try:\n",
    "            vr = decord.VideoReader(path)\n",
    "            n = len(vr)\n",
    "            if n <= 0:\n",
    "                raise RuntimeError(\"no frames\")\n",
    "\n",
    "            idxs = uniform_temporal_indices(n, self.clip_len, self.stride)\n",
    "            batch = vr.get_batch(idxs)        # [T,H,W,C]\n",
    "            x = batch.float() / 255.0         # [0,1]\n",
    "            x = x.permute(0, 3, 1, 2)         # [T,C,H,W]\n",
    "            x = _resize_112(x)                # [T,3,112,112]\n",
    "            x = self._augment(x)              # on-the-fly aug (train only)\n",
    "            x = _normalize(x)                 # final normalization\n",
    "            return x\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to read video {path}: {e} â€” using zero clip.\")\n",
    "            x = torch.zeros(self.clip_len, 3, 112, 112, dtype=torch.float32)\n",
    "            x = _normalize(x)\n",
    "            return x\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        row = self.df.iloc[i]\n",
    "        path = row[\"path\"]\n",
    "        label = int(row[\"label_new\"])   # contiguous 0..C-1\n",
    "\n",
    "        x = self._safe_load_clip(path)\n",
    "        return x, label, path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94cb6cb",
   "metadata": {},
   "source": [
    "#### Cell C â€” Split DataFrames & DataLoaders (single worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8283e08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes: 831 train | 192 val | 136 test\n",
      "Device: cuda\n",
      "Sample batch shape: torch.Size([4, 32, 3, 112, 112]) | labels range: 60 -> 88\n"
     ]
    }
   ],
   "source": [
    "# === Cell C â€” Splits + DataLoaders (no multiprocessing) ===\n",
    "\n",
    "train_df = df[df[\"split\"] == \"train\"].reset_index(drop=True)\n",
    "val_df   = df[df[\"split\"] == \"val\"].reset_index(drop=True)\n",
    "test_df  = df[df[\"split\"] == \"test\"].reset_index(drop=True)\n",
    "\n",
    "print(\"Split sizes:\", len(train_df), \"train |\", len(val_df), \"val |\", len(test_df), \"test\")\n",
    "\n",
    "clip_len = 32\n",
    "stride   = 2\n",
    "batch_size = 4   # small to be safe on GPU\n",
    "\n",
    "train_ds = WLASLDataset(train_df, clip_len=clip_len, stride=stride, train=True)\n",
    "val_ds   = WLASLDataset(val_df,   clip_len=clip_len, stride=stride, train=False)\n",
    "test_ds  = WLASLDataset(test_df,  clip_len=clip_len, stride=stride, train=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,              # ðŸ”’ NO worker processes\n",
    "    pin_memory=(device.type == \"cuda\"),\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=(device.type == \"cuda\"),\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=(device.type == \"cuda\"),\n",
    ")\n",
    "\n",
    "x_dbg, y_dbg, _ = next(iter(train_loader))\n",
    "print(\"Sample batch shape:\", x_dbg.shape, \"| labels range:\", y_dbg.min().item(), \"->\", y_dbg.max().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c013386d",
   "metadata": {},
   "source": [
    "#### Cell D â€” Model: R3D-18 baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e525b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes: 104\n",
      "Model: r3d18_k400\n"
     ]
    }
   ],
   "source": [
    "# === Cell D â€” CNN + BiGRU model with PRETRAINED ResNet-18 ===\n",
    "\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.models.video import r3d_18, R3D_18_Weights\n",
    "\n",
    "num_classes = df[\"label_new\"].nunique()\n",
    "print(\"num_classes:\", num_classes)\n",
    "\n",
    "weights = R3D_18_Weights.KINETICS400_V1\n",
    "base = r3d_18(weights=weights)\n",
    "\n",
    "\n",
    "in_f = base.fc.in_features\n",
    "base.fc = nn.Linear(in_f, num_classes)   # num_classes = 104 from df[\"label_new\"]\n",
    "\n",
    "model = base.to(device)\n",
    "print(\"Model:\", \"r3d18_k400\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0758c3da",
   "metadata": {},
   "source": [
    "#### Cell E â€” Optimizer, Scaler, Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5c31001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell E â€” Optimizer, scaler, loss (pretrained) ===\n",
    "from torch.amp import GradScaler\n",
    "\n",
    "epochs = 20\n",
    "lr     = 1e-4      # ðŸ”½ slightly lower than 3e-4 used for scratch\n",
    "wd     = 1e-2\n",
    "amp_on = True\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "scaler = GradScaler(enabled=amp_on)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # can keep or remove smoothing\n",
    "best_val_acc = -1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881fd9a6",
   "metadata": {},
   "source": [
    "#### Cell F â€” run_epoch (with correct [B,C,T,H,W] permute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68cf500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell F â€” Metrics + epoch runner (for CNN+BiGRU) ===\n",
    "\n",
    "def top1_acc(logits, y):\n",
    "    return (logits.argmax(1) == y).float().mean().item()\n",
    "\n",
    "def run_epoch(loader, train=True):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_acc  = 0.0\n",
    "    total_n    = 0\n",
    "\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "\n",
    "    for x, y, _ in loader:\n",
    "        x = x.to(device, non_blocking=True)  # [B,T,C,H,W]\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        # ðŸ”½ New: permute for 3D CNN\n",
    "        x = x.permute(0, 2, 1, 3, 4).contiguous()  # [B,C,T,H,W]\n",
    "\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=amp_on):\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "        if train:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(opt)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            acc = top1_acc(logits, y)\n",
    "            bs  = x.size(0)\n",
    "            total_loss += loss.item() * bs\n",
    "            total_acc  += acc * bs\n",
    "            total_n    += bs\n",
    "\n",
    "    return total_loss / total_n, total_acc / total_n\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03f005f",
   "metadata": {},
   "source": [
    "#### Cell G â€” Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efa56b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/20 | train loss 4.4633 acc 0.070 | val loss 4.2378 acc 0.099\n",
      "  âžœ New best val acc=0.099 (model saved)\n",
      "Epoch 02/20 | train loss 3.8953 acc 0.226 | val loss 3.9737 acc 0.151\n",
      "  âžœ New best val acc=0.151 (model saved)\n",
      "Epoch 03/20 | train loss 3.3753 acc 0.422 | val loss 3.7430 acc 0.177\n",
      "  âžœ New best val acc=0.177 (model saved)\n",
      "Epoch 04/20 | train loss 2.8890 acc 0.622 | val loss 3.4886 acc 0.286\n",
      "  âžœ New best val acc=0.286 (model saved)\n",
      "Epoch 05/20 | train loss 2.3867 acc 0.758 | val loss 3.3109 acc 0.292\n",
      "  âžœ New best val acc=0.292 (model saved)\n",
      "Epoch 06/20 | train loss 1.9941 acc 0.859 | val loss 3.2307 acc 0.339\n",
      "  âžœ New best val acc=0.339 (model saved)\n",
      "Epoch 07/20 | train loss 1.6029 acc 0.933 | val loss 3.1889 acc 0.333\n",
      "Epoch 08/20 | train loss 1.3520 acc 0.969 | val loss 3.2453 acc 0.349\n",
      "  âžœ New best val acc=0.349 (model saved)\n",
      "Epoch 09/20 | train loss 1.1894 acc 0.978 | val loss 3.2014 acc 0.385\n",
      "  âžœ New best val acc=0.385 (model saved)\n",
      "Epoch 10/20 | train loss 1.0975 acc 0.981 | val loss 3.4325 acc 0.359\n",
      "Epoch 11/20 | train loss 1.0618 acc 0.982 | val loss 3.1529 acc 0.354\n",
      "Epoch 12/20 | train loss 1.0157 acc 0.987 | val loss 3.3746 acc 0.370\n",
      "Epoch 13/20 | train loss 0.9834 acc 0.987 | val loss 3.2529 acc 0.349\n",
      "Epoch 14/20 | train loss 0.9789 acc 0.990 | val loss 3.2866 acc 0.365\n",
      "Epoch 15/20 | train loss 0.9830 acc 0.994 | val loss 3.2905 acc 0.344\n",
      "Epoch 16/20 | train loss 0.9639 acc 0.990 | val loss 3.1978 acc 0.396\n",
      "  âžœ New best val acc=0.396 (model saved)\n",
      "Epoch 17/20 | train loss 0.9754 acc 0.989 | val loss 3.3020 acc 0.391\n",
      "Epoch 18/20 | train loss 0.9508 acc 0.990 | val loss 3.2475 acc 0.359\n",
      "Epoch 19/20 | train loss 0.9552 acc 0.992 | val loss 3.3194 acc 0.354\n",
      "Epoch 20/20 | train loss 0.9372 acc 0.993 | val loss 3.1854 acc 0.380\n"
     ]
    }
   ],
   "source": [
    "# === Cell G â€” Training Loop ===\n",
    "# Ensure root, checkpoint and report dirs\n",
    "root = Path(\"..\").resolve()\n",
    "ckpt_dir = root / \"checkpoints\"\n",
    "ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "report_dir = root / \"reports\"\n",
    "report_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Notebook-based prefix for artifact filenames (sanitized)\n",
    "nb_stem = Path(\"06_train_baseline_3dCNN-r3d18_k400_kenetics.ipynb\").stem\n",
    "nb_prefix = str(nb_stem).replace(' ', '_')\n",
    "\n",
    "# names\n",
    "best_path = ckpt_dir / f\"{nb_prefix}_best_cnn_Kinetics-400s.pt\"\n",
    "save_path = best_path\n",
    "\n",
    "# history\n",
    "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    tr_loss, tr_acc = run_epoch(train_loader, train=True)\n",
    "    va_loss, va_acc = run_epoch(val_loader,   train=False)\n",
    "\n",
    "    # record\n",
    "    history[\"train_loss\"].append(tr_loss)\n",
    "    history[\"train_acc\"].append(tr_acc)\n",
    "    history[\"val_loss\"].append(va_loss)\n",
    "    history[\"val_acc\"].append(va_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}/{epochs} | \"\n",
    "          f\"train loss {tr_loss:.4f} acc {tr_acc:.3f} | \"\n",
    "          f\"val loss {va_loss:.4f} acc {va_acc:.3f}\")\n",
    "\n",
    "    if va_acc > best_val_acc:\n",
    "        best_val_acc = va_acc\n",
    "        torch.save(model.state_dict(), str(save_path))\n",
    "        print(f\"  âžœ New best val acc={best_val_acc:.3f} (model saved to {save_path})\")\n",
    "\n",
    "# persist history so later cells (or reruns) can load it\n",
    "import json, datetime\n",
    "history_path = report_dir / f'{nb_prefix}_train_history.json'\n",
    "with open(str(history_path), 'w') as _f:\n",
    "    json.dump(history, _f)\n",
    "print(\"Saved training history to\", history_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6789f1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST â€” loss 3.2135 | acc 0.353\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "ckpt_dir = root / \"checkpoints\"\n",
    "best_path = ckpt_dir / \"best_cnn_Kinetics-400s.pt\"\n",
    "\n",
    "model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "test_loss, test_acc = run_epoch(test_loader, train=False)\n",
    "print(f\"TEST â€” loss {test_loss:.4f} | acc {test_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4515dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1a5a59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0471fb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell H â€” Evaluation, Plots and Test Report ===\n",
    "\"\"\"\n",
    "Load training history and the best checkpoint, evaluate on the test set,\n",
    "plot train/val curves, normalized confusion matrix (for labels actually present),\n",
    "compute per-class accuracy and classification report. Save artifacts to `reports/`\n",
    "with filenames prefixed by the notebook stem so they're unique per-notebook.\n",
    "\"\"\"\n",
    "\n",
    "import os, time, json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# directories and prefix\n",
    "ckpt_dir = root / \"checkpoints\"\n",
    "report_dir = root / \"reports\"\n",
    "report_dir.mkdir(parents=True, exist_ok=True)\n",
    "nb_stem = Path(\"06_train_baseline_3dCNN-r3d18_k400_kenetics.ipynb\").stem\n",
    "nb_prefix = str(nb_stem).replace(' ', '_')\n",
    "\n",
    "# load history\n",
    "history_file = report_dir / f\"{nb_prefix}_train_history.json\"\n",
    "history = None\n",
    "if history_file.exists():\n",
    "    with open(str(history_file), 'r') as f:\n",
    "        history = json.load(f)\n",
    "        print('Loaded history from', history_file)\n",
    "\n",
    "# plot curves if available\n",
    "if history is not None:\n",
    "    epochs_ran = len(history.get('train_loss', []))\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 4))\n",
    "    axs[0].plot(range(1, epochs_ran+1), history.get('train_loss', []), label='train')\n",
    "    axs[0].plot(range(1, epochs_ran+1), history.get('val_loss', []), label='val')\n",
    "    axs[0].set_title('Loss')\n",
    "    axs[0].set_xlabel('epoch')\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].plot(range(1, epochs_ran+1), history.get('train_acc', []), label='train')\n",
    "    axs[1].plot(range(1, epochs_ran+1), history.get('val_acc', []), label='val')\n",
    "    axs[1].set_title('Top-1 Accuracy')\n",
    "    axs[1].set_xlabel('epoch')\n",
    "    axs[1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    curves_path = report_dir / f\"{nb_prefix}_train_val_curves.png\"\n",
    "    plt.savefig(str(curves_path), dpi=150)\n",
    "    display(fig)\n",
    "else:\n",
    "    print('No training history found; skipping curve plots.')\n",
    "\n",
    "# load best model\n",
    "best_model_path = ckpt_dir / f\"{nb_prefix}_best_cnn_Kinetics-400s.pt\"\n",
    "if not best_model_path.exists():\n",
    "    print(f'Best model not found at {best_model_path}. Skipping test evaluation.')\n",
    "else:\n",
    "    model.load_state_dict(torch.load(str(best_model_path), map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    paths = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y, pth in tqdm(test_loader, desc='Evaluating test set'):\n",
    "            x = x.to(device, non_blocking=True)\n",
    "            y = y.to(device, non_blocking=True)\n",
    "            x = x.permute(0, 2, 1, 3, 4).contiguous()\n",
    "\n",
    "            with torch.amp.autocast(device_type=device.type, enabled=amp_on):\n",
    "                logits = model(x)\n",
    "                preds = logits.argmax(1)\n",
    "\n",
    "            y_true.extend(y.cpu().tolist())\n",
    "            y_pred.extend(preds.cpu().tolist())\n",
    "            paths.extend(pth)\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    test_acc = float((y_true == y_pred).mean())\n",
    "    print(f'Test Top-1 accuracy: {test_acc:.4f} (N={len(y_true)})')\n",
    "\n",
    "    # mapping label->gloss if available\n",
    "    label_to_name = None\n",
    "    if 'df' in globals():\n",
    "        label_map_df = df[['label_new', 'gloss']].drop_duplicates()\n",
    "        label_to_name = {int(r['label_new']): r['gloss'] for _, r in label_map_df.iterrows()}\n",
    "\n",
    "    # only consider labels actually present in either y_true or y_pred\n",
    "    present_labels = np.union1d(np.unique(y_true), np.unique(y_pred)).astype(int)\n",
    "    labels_list = present_labels.tolist()\n",
    "\n",
    "    # names list for present labels\n",
    "    if label_to_name is not None:\n",
    "        names_list = [label_to_name.get(int(l), str(int(l))) for l in labels_list]\n",
    "    else:\n",
    "        names_list = [str(int(l)) for l in labels_list]\n",
    "\n",
    "    # confusion matrix for present labels\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels_list)\n",
    "    cm_norm = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + 1e-12)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    if len(labels_list) <= 50:\n",
    "        sns.heatmap(cm_norm, xticklabels=names_list, yticklabels=names_list, cmap='viridis', vmin=0, vmax=1, ax=ax)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "        ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "    else:\n",
    "        sns.heatmap(cm_norm, cmap='viridis', vmin=0, vmax=1, ax=ax)\n",
    "    ax.set_title('Normalized Confusion Matrix (rows=true)')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('True')\n",
    "    plt.tight_layout()\n",
    "    cm_path = report_dir / f\"{nb_prefix}_confusion_matrix_norm.png\"\n",
    "    plt.savefig(str(cm_path), dpi=150)\n",
    "    display(fig)\n",
    "\n",
    "    # per-class accuracy\n",
    "    support = cm.sum(axis=1)\n",
    "    per_class_acc = (cm.diagonal().astype('float') / (support + 1e-12))\n",
    "    class_acc_df = pd.DataFrame({\n",
    "        'label': labels_list,\n",
    "        'gloss': names_list,\n",
    "        'accuracy': per_class_acc,\n",
    "        'support': support\n",
    "    })\n",
    "    class_acc_df = class_acc_df.sort_values('accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    display(class_acc_df.head(20))\n",
    "    per_class_csv = report_dir / f\"{nb_prefix}_per_class_accuracy.csv\"\n",
    "    class_acc_df.to_csv(str(per_class_csv), index=False)\n",
    "\n",
    "    # classification report for present labels only\n",
    "    report_text = classification_report(y_true, y_pred, labels=labels_list, target_names=names_list, zero_division=0)\n",
    "    print('\\nClassification report:\\n')\n",
    "    print(report_text)\n",
    "\n",
    "    # assemble report\n",
    "    report = {\n",
    "        'created_at': time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()),\n",
    "        'model': repr(model.__class__.__name__),\n",
    "        'best_model_path': str(best_model_path),\n",
    "        'num_parameters': int(sum(p.numel() for p in model.parameters())),\n",
    "        'num_trainable_parameters': int(sum(p.numel() for p in model.parameters() if p.requires_grad)),\n",
    "        'dataset': {\n",
    "            'n_train': int(len(train_df)) if 'train_df' in globals() else None,\n",
    "            'n_val': int(len(val_df)) if 'val_df' in globals() else None,\n",
    "            'n_test': int(len(test_df)) if 'test_df' in globals() else None,\n",
    "            'num_classes_present': int(len(labels_list)),\n",
    "            'num_classes_total_in_manifest': int(df['label_new'].nunique()) if 'df' in globals() else None\n",
    "        },\n",
    "        'training': {\n",
    "            'epochs_ran': int(len(history.get('train_loss', []))) if history is not None else None,\n",
    "            'best_val_acc': float(best_val_acc) if 'best_val_acc' in globals() else None,\n",
    "            'final_test_acc': float(test_acc),\n",
    "            'hyperparameters': {\n",
    "                'epochs': int(epochs) if 'epochs' in globals() else None,\n",
    "                'lr': float(lr) if 'lr' in globals() else None,\n",
    "                'weight_decay': float(wd) if 'wd' in globals() else None,\n",
    "                'batch_size': int(batch_size) if 'batch_size' in globals() else None,\n",
    "                'amp_on': bool(amp_on) if 'amp_on' in globals() else None,\n",
    "                'clip_len': int(clip_len) if 'clip_len' in globals() else None,\n",
    "                'stride': int(stride) if 'stride' in globals() else None\n",
    "            }\n",
    "        },\n",
    "        'notes': {\n",
    "            'history_path': str(history_file) if history_file.exists() else None,\n",
    "            'per_class_csv': str(per_class_csv),\n",
    "            'confusion_matrix_png': str(cm_path),\n",
    "            'curves_png': str(curves_path) if 'curves_path' in locals() else None\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # save report\n",
    "    test_report_json = report_dir / f\"{nb_prefix}_test_report.json\"\n",
    "    with open(str(test_report_json), 'w') as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "\n",
    "    test_report_txt = report_dir / f\"{nb_prefix}_test_report.txt\"\n",
    "    with open(str(test_report_txt), 'w') as f:\n",
    "        f.write('Test report generated on: ' + report['created_at'] + '\\n\\n')\n",
    "        f.write('Model: ' + report['model'] + '\\n')\n",
    "        f.write(f\"Best model path: {report['best_model_path']}\\n\")\n",
    "        f.write(f\"Num parameters: {report['num_parameters']} (trainable: {report['num_trainable_parameters']})\\n\")\n",
    "        f.write('\\nDataset splits:\\n')\n",
    "        f.write(f\"  train={report['dataset']['n_train']}, val={report['dataset']['n_val']}, test={report['dataset']['n_test']}\\n\")\n",
    "        f.write(f\"Num classes (present in test): {report['dataset']['num_classes_present']}\\n\")\n",
    "        f.write(f\"Num classes (total in manifest): {report['dataset']['num_classes_total_in_manifest']}\\n\\n\")\n",
    "        f.write('Training summary:\\n')\n",
    "        f.write(f\"  epochs ran: {report['training']['epochs_ran']}, best val acc: {report['training']['best_val_acc']}, final test acc: {report['training']['final_test_acc']}\\n\")\n",
    "        f.write('Hyperparameters:\\n')\n",
    "        for k, v in report['training']['hyperparameters'].items():\n",
    "            f.write(f\"  {k}: {v}\\n\")\n",
    "        f.write('\\nTop 20 per-class accuracy:\\n')\n",
    "        for _, row in class_acc_df.head(20).iterrows():\n",
    "            f.write(f\"  {int(row['label'])}\\t{row['gloss']}\\tacc={row['accuracy']:.3f}\\tsupport={int(row['support'])}\\n\")\n",
    "\n",
    "    print('\\nSaved test_report.json and test_report.txt to', report_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
