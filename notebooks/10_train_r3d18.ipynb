{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4e4a89b-be80-4cf5-b7a1-f35bdfb918b0",
   "metadata": {},
   "source": [
    "(compact baseline, torchvision-free)\n",
    "\n",
    "This is a small 3D CNN baseline (C3D-lite). It’s not as strong as an official R(2+1)D/ResNet-3D, but it trains fast and avoids extra library constraints. You can later swap in a stronger backbone with the same dataloaders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4a66cc-3233-432f-a610-fb298a9cc23a",
   "metadata": {},
   "source": [
    "### Cell 1 — Root, config, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9d549ab-4cd5-45a1-b068-8a7538956030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTHONPATH added: /home/falasoul/notebooks/USD/AAI-590/Capstone/AAI-590-G3-ASL\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from src.data.wlasl_ds import WLASLDataset\n",
    "import sys, json, yaml, math, time\n",
    "root = Path(\"..\").resolve()\n",
    "sys.path.append(str(root))               # so \"import src\" works\n",
    "sys.path.append(str(root / \"src\"))       # optional; parent append is enough\n",
    "print(\"PYTHONPATH added:\", root)\n",
    "\n",
    "CFG = yaml.safe_load(open(root / \"configs\" / \"wlasl100.yaml\"))\n",
    "CKPT_DIR = root / CFG[\"paths\"][\"checkpoints_dir\"]\n",
    "LOG_DIR  = root / CFG[\"paths\"][\"logs_dir\"]\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "from src.utils.seed import seed_everything\n",
    "from src.utils.checkpoints import save_checkpoint, load_checkpoint\n",
    "seed_everything(CFG[\"wlasl\"][\"split_seed\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d87bf12-5754-45f3-8686-12af4c1bf6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports OK; seed: 42\n"
     ]
    }
   ],
   "source": [
    "from src.utils.seed import seed_everything\n",
    "from src.utils.checkpoints import save_checkpoint, load_checkpoint\n",
    "print(\"imports OK; seed:\", seed_everything(42))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7dc46fa-5e35-46da-8396-c3248f1218b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np, cv2, decord, random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "decord.bridge.set_bridge(\"torch\")\n",
    "\n",
    "def _resize_112(frame_tchw: torch.Tensor) -> torch.Tensor:\n",
    "    # frame_tchw: [T,C,H,W] float32 [0,1]\n",
    "    T,C,H,W = frame_tchw.shape\n",
    "    # Use OpenCV for speed; convert to NHWC\n",
    "    arr = frame_tchw.permute(0,2,3,1).cpu().numpy()  # T,H,W,C\n",
    "    out = np.empty((T,112,112,C), dtype=np.float32)\n",
    "    for t in range(T):\n",
    "        out[t] = cv2.resize(arr[t], (112,112), interpolation=cv2.INTER_AREA)\n",
    "    out = torch.from_numpy(out).permute(0,3,1,2)  # T,C,112,112\n",
    "    return out\n",
    "\n",
    "def _normalize(frame_tchw: torch.Tensor, mean=(0.45,0.45,0.45), std=(0.225,0.225,0.225)) -> torch.Tensor:\n",
    "    # per-channel normalization\n",
    "    mean = torch.tensor(mean, dtype=frame_tchw.dtype, device=frame_tchw.device)[None,:,None,None]\n",
    "    std  = torch.tensor(std,  dtype=frame_tchw.dtype, device=frame_tchw.device)[None,:,None,None]\n",
    "    return (frame_tchw - mean) / std\n",
    "\n",
    "def uniform_temporal_indices(n_total, clip_len, stride):\n",
    "    # Aim to cover as much as possible; for short videos, loop-pad\n",
    "    if n_total <= 0: return [0]*clip_len\n",
    "    wanted = (clip_len-1)*stride + 1\n",
    "    if n_total >= wanted:\n",
    "        # center-start for consistent coverage\n",
    "        start = (n_total - wanted)//2\n",
    "        return [start + i*stride for i in range(clip_len)]\n",
    "    # not enough frames: repeat last index\n",
    "    idxs = [min(i*stride, n_total-1) for i in range(clip_len)]\n",
    "    return idxs\n",
    "\n",
    "class WLASLDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, clip_len=32, stride=2, train=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.clip_len = clip_len\n",
    "        self.stride = stride\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        row = self.df.iloc[i]\n",
    "        path = row[\"path\"]\n",
    "        label = int(row[\"label\"])\n",
    "        vr = decord.VideoReader(path)\n",
    "        n = len(vr)\n",
    "\n",
    "        idxs = uniform_temporal_indices(n, self.clip_len, self.stride)\n",
    "        batch = vr.get_batch(idxs)  # [T,H,W,C] uint8\n",
    "        # to float [0,1], TCHW\n",
    "        x = batch.float()/255.0\n",
    "        x = x.permute(0,3,1,2)\n",
    "        # spatial resize 112x112\n",
    "        x = _resize_112(x)\n",
    "        # normalize\n",
    "        x = _normalize(x)\n",
    "        return x, label, path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcacc2a-8c41-42c6-b5cd-85b269a21c61",
   "metadata": {},
   "source": [
    "####  Cell 2 — Load manifest & build DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdcdb093-bced-4c3b-98d5-9d2ffc45156a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded manifest: /home/falasoul/notebooks/USD/AAI-590/Capstone/AAI-590-G3-ASL/data/metadata/wlasl100_manifest.csv\n",
      "Total samples: 752\n",
      "Splits: {'train': np.int64(547), 'val': np.int64(124), 'test': np.int64(81)}\n",
      "Classes: 100\n",
      "Train batches: 69 | Val batches: 16 | Test batches: 11\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from collections import Counter\n",
    "\n",
    "# === Load dataset manifest (created in 02_preprocess_segments.ipynb) ===\n",
    "MANIFEST = root / \"data\" / \"metadata\" / \"wlasl100_manifest.csv\"\n",
    "m = pd.read_csv(MANIFEST)\n",
    "print(\"Loaded manifest:\", MANIFEST)\n",
    "print(\"Total samples:\", len(m))\n",
    "print(\"Splits:\", dict(m[\"split\"].value_counts()))\n",
    "\n",
    "# === Split subsets ===\n",
    "train_df = m[m[\"split\"] == \"train\"].copy()\n",
    "val_df   = m[m[\"split\"] == \"val\"].copy()\n",
    "test_df  = m[m[\"split\"] == \"test\"].copy()\n",
    "\n",
    "# === Read config values ===\n",
    "clip_len   = CFG[\"model\"][\"clip_len\"]\n",
    "frame_step = CFG[\"model\"][\"frame_stride\"]\n",
    "bs         = CFG[\"train\"][\"batch_size\"]\n",
    "nw         = CFG[\"train\"][\"num_workers\"]\n",
    "\n",
    "# === Import the dataset class (from 03_dataset_preview.ipynb or src/data/wlasl_ds.py) ===\n",
    "# If you have the Dataset defined in the preview notebook, just re-run that cell before this.\n",
    "# Otherwise, place it in `src/data/wlasl_ds.py` and import as shown:\n",
    "# from src.data.wlasl_ds import WLASLDataset\n",
    "\n",
    "# === Create train/val/test datasets ===\n",
    "train_ds = WLASLDataset(train_df, clip_len=clip_len, stride=frame_step, train=True)\n",
    "val_ds   = WLASLDataset(val_df,   clip_len=clip_len, stride=frame_step, train=False)\n",
    "test_ds  = WLASLDataset(test_df,  clip_len=clip_len, stride=frame_step, train=False)\n",
    "\n",
    "# === Handle class imbalance via WeightedRandomSampler ===\n",
    "counts = train_df[\"label\"].value_counts().to_dict()\n",
    "weights = train_df[\"label\"].map(lambda y: 1.0 / max(1, counts[y])).values\n",
    "sampler = WeightedRandomSampler(\n",
    "    torch.tensor(weights, dtype=torch.double),\n",
    "    num_samples=len(train_df),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# === Build DataLoaders ===\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=bs, sampler=sampler,\n",
    "    num_workers=nw, pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=bs, shuffle=False,\n",
    "    num_workers=nw, pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_ds, batch_size=bs, shuffle=False,\n",
    "    num_workers=nw, pin_memory=True\n",
    ")\n",
    "\n",
    "# === Confirm stats ===\n",
    "num_classes = m[\"label\"].nunique()\n",
    "print(f\"Classes: {num_classes}\")\n",
    "print(f\"Train batches: {len(train_loader)} | Val batches: {len(val_loader)} | Test batches: {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e2b6dc-1f95-498d-853f-b4709b676bca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
