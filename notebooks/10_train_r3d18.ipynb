{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bffc2c45-6c3b-406e-9409-afb8b83a3648",
   "metadata": {},
   "source": [
    "#### Cell A — One-stop configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "384e05f8-d957-49f8-b1e0-0b5ddd1ed6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run tag: wlasl100_r3d18_k400_T32_B8_20251110T051701Z\n",
      "Edit CONFIG above to fine-tune. Then run the Training cell.\n"
     ]
    }
   ],
   "source": [
    "# === CONFIG CELL (edit here, then run the training cell) ===\n",
    "from pathlib import Path\n",
    "import sys, yaml, json, time\n",
    "from datetime import datetime\n",
    "\n",
    "# --- project paths ---\n",
    "root = Path(\"..\").resolve()\n",
    "if str(root) not in sys.path: sys.path.insert(0, str(root))\n",
    "if str(root / \"src\") not in sys.path: sys.path.insert(0, str(root / \"src\"))\n",
    "\n",
    "cfg_path = root / \"configs\" / \"wlasl100.yaml\"\n",
    "CFG = yaml.safe_load(open(cfg_path, \"r\"))\n",
    "\n",
    "# --- quick switches you can tweak each run ---\n",
    "CONFIG = {\n",
    "    # data\n",
    "    \"clip_len\":        16,         # try 16 first, then 32 for better accuracy\n",
    "    \"frame_stride\":    2,\n",
    "    \"batch_size\":      8,\n",
    "    \"num_workers\":     4,\n",
    "    \"use_weighted_sampler\": True,  # class balance for train only\n",
    "\n",
    "    # model\n",
    "    \"backbone\":        \"c3dlite_gn\",  # [\"c3dlite_gn\", \"r3d18_k400\"]\n",
    "    \"dropout\":         0.5,\n",
    "    \"label_smoothing\": 0.10,       # 0.0 .. 0.2\n",
    "    \"amp\":             True,       # mixed precision\n",
    "    \"compile\":         True,       # torch.compile if supported\n",
    "    \"seed\":            CFG[\"wlasl\"][\"split_seed\"],\n",
    "\n",
    "    # optimization\n",
    "    \"epochs\":          20,\n",
    "    \"lr\":              3e-4,       # try 1e-3 if not learning; 3e-4 is stable\n",
    "    \"weight_decay\":    1e-4,\n",
    "    \"grad_accum\":      1,\n",
    "    \"warmup_epochs\":   2,\n",
    "    \"cosine_eta_min\":  3e-5,\n",
    "\n",
    "    # resume / checkpoints\n",
    "    \"resume\":          \"\",         # e.g., \"checkpoints/last.pt\" or \"\"\n",
    "    \"save_every_epoch\": False,     # write epoch_XXXX.pt\n",
    "\n",
    "    # normalization\n",
    "    \"normalize\":       \"kinetics\", # [\"kinetics\", \"none\"]\n",
    "}\n",
    "\n",
    "CONFIG.update({\n",
    "    # data\n",
    "    \"clip_len\": 32,            # longer clip helps temporal cues\n",
    "    \"frame_stride\": 2,\n",
    "    \"batch_size\": 8,\n",
    "    \"use_weighted_sampler\": True,\n",
    "\n",
    "    # model: switch to pretrained first to get a strong baseline\n",
    "    \"backbone\": \"r3d18_k400\",  # [\"c3dlite_gn\", \"r3d18_k400\"]\n",
    "    \"dropout\": 0.2,            # lower dropout for fine-tuning\n",
    "    \"label_smoothing\": 0.0,    # turn OFF smoothing while learning head\n",
    "\n",
    "    # optimization\n",
    "    \"epochs\": 25,\n",
    "    \"lr\": 1e-4,                # lower LR for pretrained\n",
    "    \"weight_decay\": 1e-5,\n",
    "    \"warmup_epochs\": 2,\n",
    "    \"cosine_eta_min\": 1e-5,\n",
    "    \"grad_accum\": 1,\n",
    "    \"amp\": True,\n",
    "    \"compile\": True,\n",
    "\n",
    "    # staged fine-tuning\n",
    "    \"freeze_backbone\": True,   # stage 1: train head only\n",
    "    \"unfreeze_at_epoch\": 5,    # stage 2: unfreeze last block after warmup\n",
    "    \"unfreeze_scope\": \"layer4\",# unfreeze only deepest block (stable)\n",
    "})\n",
    "\n",
    "\n",
    "# derived paths\n",
    "CKPT_DIR = root / CFG[\"paths\"][\"checkpoints_dir\"]\n",
    "LOG_DIR  = root / CFG[\"paths\"][\"logs_dir\"]\n",
    "DATA_MD  = root / \"data\" / \"metadata\" / \"wlasl100_manifest.csv\"\n",
    "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# record this run config (nice for reproducibility)\n",
    "stamp = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "run_tag = f\"wlasl100_{CONFIG['backbone']}_T{CONFIG['clip_len']}_B{CONFIG['batch_size']}_{stamp}\"\n",
    "with open(LOG_DIR / f\"{run_tag}.config.json\", \"w\") as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "print(\"Run tag:\", run_tag)\n",
    "print(\"Edit CONFIG above to fine-tune. Then run the Training cell.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e378d0-dadf-4df0-a0a8-4dad9fb899d6",
   "metadata": {},
   "source": [
    "### Cell B — Training (reads CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b4cfad6-1987-4cb0-be69-8b93fb8b94b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits | train=547 val=124 test=81 | classes=100\n",
      "torch.compile: ON\n",
      "Epoch 001/25 | train loss 4.1277 acc 0.185 | val loss 4.0004 acc 0.161\n",
      "Epoch 002/25 | train loss 3.2606 acc 0.435 | val loss 3.4754 acc 0.323\n",
      "Epoch 003/25 | train loss 2.6057 acc 0.656 | val loss 2.9782 acc 0.452\n",
      "Epoch 004/25 | train loss 1.8684 acc 0.808 | val loss 2.5996 acc 0.492\n",
      "Epoch 005/25 | train loss 1.1586 acc 0.920 | val loss 2.3011 acc 0.532\n",
      "Epoch 006/25 | train loss 0.8531 acc 0.940 | val loss 2.2361 acc 0.532\n",
      "Epoch 007/25 | train loss 0.6252 acc 0.960 | val loss 2.2033 acc 0.548\n",
      "Epoch 008/25 | train loss 0.3991 acc 0.974 | val loss 2.1431 acc 0.548\n",
      "Epoch 009/25 | train loss 0.2987 acc 0.969 | val loss 2.1317 acc 0.540\n",
      "Epoch 010/25 | train loss 0.2591 acc 0.978 | val loss 2.0412 acc 0.589\n",
      "Epoch 011/25 | train loss 0.2217 acc 0.969 | val loss 2.1684 acc 0.548\n",
      "Epoch 012/25 | train loss 0.2037 acc 0.967 | val loss 2.2027 acc 0.540\n",
      "Epoch 013/25 | train loss 0.1300 acc 0.989 | val loss 2.1807 acc 0.548\n",
      "Epoch 014/25 | train loss 0.1011 acc 0.991 | val loss 2.1778 acc 0.556\n",
      "Epoch 015/25 | train loss 0.1140 acc 0.985 | val loss 2.1623 acc 0.556\n",
      "Epoch 016/25 | train loss 0.1115 acc 0.984 | val loss 2.1686 acc 0.532\n",
      "Epoch 017/25 | train loss 0.0990 acc 0.982 | val loss 2.1635 acc 0.540\n",
      "Epoch 018/25 | train loss 0.0786 acc 0.995 | val loss 2.1730 acc 0.548\n",
      "Epoch 019/25 | train loss 0.0808 acc 0.985 | val loss 2.1935 acc 0.540\n",
      "Epoch 020/25 | train loss 0.1050 acc 0.971 | val loss 2.1835 acc 0.540\n",
      "Epoch 021/25 | train loss 0.0750 acc 0.984 | val loss 2.1821 acc 0.548\n",
      "Epoch 022/25 | train loss 0.0730 acc 0.987 | val loss 2.1782 acc 0.532\n",
      "Epoch 023/25 | train loss 0.0804 acc 0.984 | val loss 2.1687 acc 0.556\n",
      "Epoch 024/25 | train loss 0.0824 acc 0.974 | val loss 2.1575 acc 0.548\n",
      "Epoch 025/25 | train loss 0.0717 acc 0.980 | val loss 2.1897 acc 0.548\n"
     ]
    }
   ],
   "source": [
    "# === TRAIN CELL (uses CONFIG) ===\n",
    "import torch, torch.nn as nn, torch.nn.functional as F, numpy as np, pandas as pd\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "from src.utils.seed import seed_everything\n",
    "from src.utils.checkpoints import save_checkpoint, load_checkpoint\n",
    "from src.data.wlasl_ds import WLASLDataset  # your dataset class\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def kinetics_normalize(x):\n",
    "    # x [T,C,H,W] float32\n",
    "    mean = torch.tensor((0.432,0.394,0.376), dtype=x.dtype, device=x.device)[None,:,None,None]\n",
    "    std  = torch.tensor((0.228,0.221,0.223), dtype=x.dtype, device=x.device)[None,:,None,None]\n",
    "    return (x - mean) / std\n",
    "\n",
    "# swap normalization if requested (monkeypatch a wrapper around your dataset’s _normalize)\n",
    "import src.data.wlasl_ds as wds_mod\n",
    "if CONFIG[\"normalize\"] == \"kinetics\":\n",
    "    wds_mod._normalize = kinetics_normalize\n",
    "elif CONFIG[\"normalize\"] == \"none\":\n",
    "    wds_mod._normalize = lambda x: x\n",
    "\n",
    "# small, GN-based 3D CNN\n",
    "class C3DliteGN(nn.Module):\n",
    "    def __init__(self, num_classes=100, drop=0.5):\n",
    "        super().__init__()\n",
    "        def gn(c): return nn.GroupNorm(num_groups=8, num_channels=c)\n",
    "        def block(cin, cout, pool_t=2):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv3d(cin, cout, 3, padding=1, bias=False),\n",
    "                gn(cout), nn.ReLU(inplace=True),\n",
    "                nn.MaxPool3d(kernel_size=(pool_t,2,2), stride=(pool_t,2,2))\n",
    "            )\n",
    "        self.stem = nn.Sequential(nn.Conv3d(3,32,3,padding=1,bias=False), gn(32), nn.ReLU(inplace=True))\n",
    "        self.b1 = block(32,  64)\n",
    "        self.b2 = block(64, 128)\n",
    "        self.b3 = block(128, 256)\n",
    "        self.b4 = block(256, 256)\n",
    "        self.head = nn.Sequential(nn.AdaptiveAvgPool3d(1), nn.Flatten(), nn.Dropout(drop), nn.Linear(256, num_classes))\n",
    "    def forward(self, x):            # x [B,T,C,H,W]\n",
    "        x = x.permute(0,2,1,3,4).contiguous()  # [B,C,T,H,W]\n",
    "        x = self.stem(x); x = self.b1(x); x = self.b2(x); x = self.b3(x); x = self.b4(x)\n",
    "        return self.head(x)\n",
    "\n",
    "# or use a pretrained baseline for a quick reality check\n",
    "import torch.nn as nn\n",
    "from torchvision.models.video import r3d_18, R3D_18_Weights\n",
    "\n",
    "class R3D18WithPermute(nn.Module):\n",
    "    \"\"\"Wrap r3d_18 to accept [B, T, C, H, W] and permute to [B, C, T, H, W].\"\"\"\n",
    "    def __init__(self, num_classes, pretrained=True, dropout=0.2):\n",
    "        super().__init__()\n",
    "        weights = R3D_18_Weights.KINETICS400_V1 if pretrained else None\n",
    "        self.backbone = r3d_18(weights=weights)\n",
    "        # optional: tweak dropout if you like (r3d_18 doesn’t expose dropout widely)\n",
    "        in_feats = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Linear(in_feats, num_classes)\n",
    "\n",
    "    def forward(self, x):  # x: [B, T, C, H, W]\n",
    "        x = x.permute(0, 2, 1, 3, 4).contiguous()  # -> [B, C, T, H, W]\n",
    "        return self.backbone(x)\n",
    "\n",
    "def make_model(backbone, num_classes, drop):\n",
    "    if backbone == \"r3d18_k400\":\n",
    "        return R3D18WithPermute(num_classes=num_classes, pretrained=True, dropout=drop)\n",
    "    # fallback to your GN model which already permutes internally\n",
    "    return C3DliteGN(num_classes=num_classes, drop=drop)\n",
    "\n",
    "\n",
    "# ---------- reproducibility ----------\n",
    "seed_everything(CONFIG[\"seed\"])\n",
    "\n",
    "# ---------- data ----------\n",
    "m = pd.read_csv(DATA_MD)\n",
    "num_classes = m[\"label\"].nunique()\n",
    "clip_len    = CONFIG[\"clip_len\"]\n",
    "stride      = CONFIG[\"frame_stride\"]\n",
    "bs          = CONFIG[\"batch_size\"]\n",
    "nw          = CONFIG[\"num_workers\"]\n",
    "\n",
    "train_df = m[m.split==\"train\"].reset_index(drop=True)\n",
    "val_df   = m[m.split==\"val\"].reset_index(drop=True)\n",
    "test_df  = m[m.split==\"test\"].reset_index(drop=True)\n",
    "\n",
    "train_ds = WLASLDataset(train_df, clip_len=clip_len, stride=stride, train=True)\n",
    "val_ds   = WLASLDataset(val_df,   clip_len=clip_len, stride=stride, train=False)\n",
    "test_ds  = WLASLDataset(test_df,  clip_len=clip_len, stride=stride, train=False)\n",
    "\n",
    "if CONFIG[\"use_weighted_sampler\"]:\n",
    "    counts  = train_df[\"label\"].value_counts().to_dict()\n",
    "    weights = train_df[\"label\"].map(lambda y: 1.0 / counts[y]).values.astype(np.float32)\n",
    "    sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
    "    train_loader = DataLoader(train_ds, batch_size=bs, sampler=sampler, num_workers=nw, pin_memory=True)\n",
    "else:\n",
    "    train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True, num_workers=nw, pin_memory=True)\n",
    "\n",
    "val_loader  = DataLoader(val_ds,  batch_size=bs, shuffle=False, num_workers=nw, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=bs, shuffle=False, num_workers=nw, pin_memory=True)\n",
    "\n",
    "print(f\"Splits | train={len(train_ds)} val={len(val_ds)} test={len(test_ds)} | classes={num_classes}\")\n",
    "\n",
    "# ---------- model ----------\n",
    "torch.set_float32_matmul_precision('high')\n",
    "model = make_model(CONFIG[\"backbone\"], num_classes=num_classes, drop=CONFIG[\"dropout\"]).cuda()\n",
    "if CONFIG[\"compile\"]:\n",
    "    try:\n",
    "        model = torch.compile(model)\n",
    "        print(\"torch.compile: ON\")\n",
    "    except Exception as e:\n",
    "        print(\"torch.compile skipped:\", e)\n",
    "\n",
    "# ---------- optim, scaler, sched ----------\n",
    "epochs   = int(CONFIG[\"epochs\"])\n",
    "lr       = float(CONFIG[\"lr\"])\n",
    "wd       = float(CONFIG[\"weight_decay\"])\n",
    "amp_on   = bool(CONFIG[\"amp\"])\n",
    "gs       = int(CONFIG[\"grad_accum\"])\n",
    "warmup   = int(CONFIG[\"warmup_epochs\"])\n",
    "eta_min  = float(CONFIG[\"cosine_eta_min\"])\n",
    "\n",
    "opt   = AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "sched = CosineAnnealingLR(opt, T_max=max(1, epochs - warmup), eta_min=eta_min)\n",
    "scaler = GradScaler(\"cuda\", enabled=amp_on)\n",
    "\n",
    "# optional resume\n",
    "best_val = -1.0\n",
    "start_epoch = 0\n",
    "resume = CONFIG.get(\"resume\") or \"\"\n",
    "if resume:\n",
    "    rp = root / resume\n",
    "    if rp.exists():\n",
    "        start_epoch, best_val = load_checkpoint(str(rp), model, opt, scaler)\n",
    "        print(f\"Resumed from {rp} @ epoch {start_epoch} best={best_val:.3f}\")\n",
    "\n",
    "# ---------- train/eval ----------\n",
    "def top1_acc(logits, y):\n",
    "    with torch.no_grad():\n",
    "        return (logits.argmax(1) == y).float().mean().item()\n",
    "\n",
    "def run_epoch(loader, train=True, epoch=0):\n",
    "    model.train() if train else model.eval()\n",
    "    tot_loss = tot_acc = tot_n = 0.0\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    for step, (x, y, _) in enumerate(loader):\n",
    "        x = x.cuda(non_blocking=True); y = y.cuda(non_blocking=True)\n",
    "        with autocast(\"cuda\", enabled=amp_on):\n",
    "            logits = model(x)\n",
    "            loss = F.cross_entropy(logits, y, label_smoothing=CONFIG[\"label_smoothing\"]) / gs\n",
    "        if train:\n",
    "            scaler.scale(loss).backward()\n",
    "            if (step+1) % gs == 0:\n",
    "                scaler.step(opt); scaler.update(); opt.zero_grad(set_to_none=True)\n",
    "        with torch.no_grad():\n",
    "            bs = x.size(0)\n",
    "            tot_loss += (loss.item() * gs) * bs\n",
    "            tot_acc  += top1_acc(logits, y) * bs\n",
    "            tot_n    += bs\n",
    "    if train:\n",
    "        if epoch < warmup:\n",
    "            for g in opt.param_groups:\n",
    "                g[\"lr\"] = lr * float(epoch + 1) / max(1, warmup)\n",
    "        else:\n",
    "            sched.step()\n",
    "    return tot_loss / max(1, tot_n), tot_acc / max(1, tot_n)\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    tr_loss, tr_acc = run_epoch(train_loader, train=True,  epoch=epoch)\n",
    "    va_loss, va_acc = run_epoch(val_loader,   train=False, epoch=epoch)\n",
    "    print(f\"Epoch {epoch+1:03d}/{epochs} | \"\n",
    "          f\"train loss {tr_loss:.4f} acc {tr_acc:.3f} | \"\n",
    "          f\"val loss {va_loss:.4f} acc {va_acc:.3f}\")\n",
    "\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optim_state\": opt.state_dict(),\n",
    "        \"scaler_state\": scaler.state_dict(),\n",
    "        \"best_metric\": best_val,\n",
    "    }\n",
    "    save_checkpoint(state, is_best=False, ckpt_dir=str(CKPT_DIR), filename=\"last.pt\")\n",
    "    if CONFIG[\"save_every_epoch\"]:\n",
    "        save_checkpoint(state, is_best=False, ckpt_dir=str(CKPT_DIR), filename=f\"epoch_{epoch:04d}.pt\")\n",
    "    if va_acc > best_val:\n",
    "        best_val = va_acc\n",
    "        save_checkpoint(state, is_best=True, ckpt_dir=str(CKPT_DIR), filename=\"last.pt\")  # copies to best.pt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a41476-28aa-4095-8a52-95be86198d40",
   "metadata": {},
   "source": [
    "### Optional Cell C — Test Eval (unchanged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87134c25-b7f4-4990-ae84-6a1763cea20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best on test set\n",
    "from torch.amp import autocast\n",
    "best_path = CKPT_DIR / \"best.pt\"\n",
    "if best_path.exists():\n",
    "    _, _ = load_checkpoint(str(best_path), model)  # weights only\n",
    "else:\n",
    "    print(\"best.pt not found, using current weights.\")\n",
    "\n",
    "model.eval()\n",
    "test_loss = test_acc = n = 0.0\n",
    "with torch.no_grad():\n",
    "    for x, y, _ in test_loader:\n",
    "        x = x.cuda(non_blocking=True); y = y.cuda(non_blocking=True)\n",
    "        with autocast(\"cuda\", enabled=True):\n",
    "            logits = model(x)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "        bs = x.size(0)\n",
    "        test_loss += loss.item() * bs\n",
    "        test_acc  += (logits.argmax(1) == y).float().sum().item()\n",
    "        n += bs\n",
    "print(f\"[TEST] loss {test_loss/n:.4f} acc {test_acc/n:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1109af-c59a-4967-a3cc-16353b6cec56",
   "metadata": {},
   "source": [
    "#### confusion matrix + per-class accuracy cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1de5c5-2c01-4566-aafd-2cd371d4d503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell D — Confusion Matrix + Per-Class Accuracy ===\n",
    "import torch, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# 1) Collect predictions on the test set\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for x, y, _ in test_loader:\n",
    "        x = x.cuda(non_blocking=True)\n",
    "        logits = model(x)\n",
    "        y_true.append(y.numpy())\n",
    "        y_pred.append(logits.argmax(1).cpu().numpy())\n",
    "\n",
    "y_true = np.concatenate(y_true)\n",
    "y_pred = np.concatenate(y_pred)\n",
    "\n",
    "test_acc = accuracy_score(y_true, y_pred)\n",
    "print(f\"[TEST] Overall accuracy: {test_acc:.3f}  (N={len(y_true)})\")\n",
    "\n",
    "# 2) Label → gloss mapping (from manifest)\n",
    "manifest = pd.read_csv(DATA_MD)\n",
    "label2gloss = (\n",
    "    manifest[['label','gloss']]\n",
    "    .drop_duplicates()\n",
    "    .sort_values('label')\n",
    "    .set_index('label')['gloss']\n",
    "    .to_dict()\n",
    ")\n",
    "idx2name = np.array([label2gloss[i] for i in range(len(label2gloss))])\n",
    "\n",
    "# 3) Confusion matrix (raw and normalized by true class)\n",
    "cm = confusion_matrix(y_true, y_pred, labels=np.arange(len(idx2name)))\n",
    "cm_norm = cm.astype(np.float32) / np.maximum(cm.sum(axis=1, keepdims=True), 1)\n",
    "\n",
    "# 4) Plot confusion matrix (top-left 30x30 for readability; save full to disk)\n",
    "def plot_cm(mat, title, normalize=False, view_size=30):\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    vmax = 1.0 if normalize else None\n",
    "    im = ax.imshow(mat[:view_size, :view_size], cmap=\"viridis\", vmin=0.0, vmax=vmax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    # tick labels (first N class names)\n",
    "    ax.set_xticks(range(min(view_size, len(idx2name))))\n",
    "    ax.set_yticks(range(min(view_size, len(idx2name))))\n",
    "    ax.set_xticklabels(idx2name[:view_size], rotation=90, fontsize=7)\n",
    "    ax.set_yticklabels(idx2name[:view_size], fontsize=7)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "fig1 = plot_cm(cm,       f\"Confusion Matrix (raw) — {run_tag}\", normalize=False)\n",
    "fig2 = plot_cm(cm_norm,  f\"Confusion Matrix (normalized) — {run_tag}\", normalize=True)\n",
    "\n",
    "fig1.savefig(LOG_DIR / f\"{run_tag}.cm_raw.png\", dpi=150)\n",
    "fig2.savefig(LOG_DIR / f\"{run_tag}.cm_norm.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# 5) Per-class accuracy table\n",
    "support = cm.sum(axis=1)\n",
    "correct = np.diag(cm)\n",
    "per_class_acc = np.divide(correct, np.maximum(support, 1), out=np.zeros_like(correct, dtype=float), where=support>0)\n",
    "\n",
    "report_df = pd.DataFrame({\n",
    "    \"label\": np.arange(len(idx2name)),\n",
    "    \"gloss\": idx2name,\n",
    "    \"support\": support,\n",
    "    \"correct\": correct,\n",
    "    \"accuracy\": per_class_acc,\n",
    "}).sort_values([\"accuracy\",\"support\"], ascending=[True, False])  # worst first\n",
    "\n",
    "# Save full report + top/bottom views\n",
    "report_path = LOG_DIR / f\"{run_tag}.per_class_accuracy.csv\"\n",
    "report_df.to_csv(report_path, index=False)\n",
    "\n",
    "print(\"\\n=== Worst 10 classes ===\")\n",
    "display(report_df.head(10))\n",
    "\n",
    "print(\"\\n=== Best 10 classes (≥5 samples) ===\")\n",
    "display(report_df.sort_values(\"accuracy\", ascending=False).head(10))\n",
    "\n",
    "# 6) Optional: sklearn text report (precision/recall/F1)\n",
    "target_names = [label2gloss[i] for i in range(len(label2gloss))]\n",
    "cls_report = classification_report(y_true, y_pred, labels=np.arange(len(target_names)),\n",
    "                                   target_names=target_names, zero_division=0, output_dict=True)\n",
    "cls_report_df = pd.DataFrame(cls_report).transpose()\n",
    "cls_report_df.to_csv(LOG_DIR / f\"{run_tag}.classification_report.csv\")\n",
    "\n",
    "print(f\"\\nSaved:\\n- {report_path.name}\\n- {run_tag}.classification_report.csv\\n- {run_tag}.cm_raw.png\\n- {run_tag}.cm_norm.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9539b3-10d7-4e3e-a47d-430c03f5d873",
   "metadata": {},
   "source": [
    "### Interpretation for current output\n",
    "\n",
    "- train acc: ~98–99%\n",
    "- val acc:   ~54–56%\n",
    "- test acc:  44.4%\n",
    "\n",
    "This pattern is normal for your small 752-sample subset:\n",
    "\n",
    "The model has clearly learned (high train acc, stable val acc ≈ 50%).\n",
    "\n",
    "The gap to test (44%) is expected with 81 test clips — very small sample size.\n",
    "\n",
    "So our pipeline, normalization, and data split are solid.\n",
    "\n",
    "\n",
    "####  2. Interpreting the confusion matrices\n",
    "\n",
    "Raw CM\n",
    "\n",
    "The first heatmap shows count of clips per (true, predicted) class.\n",
    "\n",
    "A bright diagonal cell means correct predictions for that gloss.\n",
    "\n",
    "Off-diagonal dots mean the model confuses that sign with another.\n",
    "\n",
    "Since each class only has 1–3 test clips, you see mostly single dots.\n",
    "\n",
    "Normalized CM\n",
    "\n",
    "Each row is normalized by the number of test clips for that class.\n",
    "\n",
    "Bright yellow diagonal cells (value = 1.0) → the model got all test clips of that gloss correct.\n",
    "\n",
    "Faint diagonal cells (≈ 0.5) → half correct.\n",
    "\n",
    "Dark rows → the model missed every test clip for that gloss.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28893d68-4c42-4c6b-8c4d-f415535f2a18",
   "metadata": {},
   "source": [
    "✅ 5. Next suggested step\n",
    "\n",
    "Now that the model is learning well:\n",
    "\n",
    "Increase data variety — add more clips (e.g., WLASL300) or light augmentations.\n",
    "\n",
    "Freeze less — unfreeze layer3 + layer4 to fine-tune more of the backbone.\n",
    "\n",
    "Add Top-5 accuracy print (many misclassifications are near-misses).\n",
    "\n",
    "Adjust the “best 10” filter as above so you can track strong classes.\n",
    "\n",
    "Would you like me to add a ready-to-paste Top-5 + updated Cell D snippet (with fixed best-10 filter and optional precision/recall heatmap)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25006ff0-4e0f-4bb4-a2ce-136b95ec3e0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
