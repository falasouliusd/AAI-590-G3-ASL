{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdb629bf",
   "metadata": {},
   "source": [
    "### Cell A â€” Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1497cf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: /home/falasoul/notebooks/USD/AAI-590/Capstone/AAI-590-G3-ASL/data/wlasl_preprocessed/manifest_nslt2000_roi_top104_balanced_clean.csv\n",
      "Samples: 1159 | classes=104\n",
      "Columns: ['video_id', 'path', 'gloss', 'label', 'split', 'exists', 'label_new']\n",
      "label_new min/max: 0 103\n",
      "label_new nunique: 104\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>path</th>\n",
       "      <th>gloss</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "      <th>exists</th>\n",
       "      <th>label_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>639</td>\n",
       "      <td>/home/falasoul/notebooks/USD/AAI-590/Capstone/...</td>\n",
       "      <td>accident</td>\n",
       "      <td>8</td>\n",
       "      <td>train</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>624</td>\n",
       "      <td>/home/falasoul/notebooks/USD/AAI-590/Capstone/...</td>\n",
       "      <td>accident</td>\n",
       "      <td>8</td>\n",
       "      <td>train</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>632</td>\n",
       "      <td>/home/falasoul/notebooks/USD/AAI-590/Capstone/...</td>\n",
       "      <td>accident</td>\n",
       "      <td>8</td>\n",
       "      <td>train</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>623</td>\n",
       "      <td>/home/falasoul/notebooks/USD/AAI-590/Capstone/...</td>\n",
       "      <td>accident</td>\n",
       "      <td>8</td>\n",
       "      <td>train</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65009</td>\n",
       "      <td>/home/falasoul/notebooks/USD/AAI-590/Capstone/...</td>\n",
       "      <td>accident</td>\n",
       "      <td>8</td>\n",
       "      <td>train</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   video_id                                               path     gloss  \\\n",
       "0       639  /home/falasoul/notebooks/USD/AAI-590/Capstone/...  accident   \n",
       "1       624  /home/falasoul/notebooks/USD/AAI-590/Capstone/...  accident   \n",
       "2       632  /home/falasoul/notebooks/USD/AAI-590/Capstone/...  accident   \n",
       "3       623  /home/falasoul/notebooks/USD/AAI-590/Capstone/...  accident   \n",
       "4     65009  /home/falasoul/notebooks/USD/AAI-590/Capstone/...  accident   \n",
       "\n",
       "   label  split  exists  label_new  \n",
       "0      8  train    True          0  \n",
       "1      8  train    True          0  \n",
       "2      8  train    True          0  \n",
       "3      8  train    True          0  \n",
       "4      8  train    True          0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Cell A â€” Imports, reproducibility, load balanced ROI manifest ===\n",
    "import os, random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import cv2\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Keep CPU threads tame\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "root = Path(\"..\").resolve()\n",
    "data_dir = root / \"data\" / \"wlasl_preprocessed\"\n",
    "\n",
    "# Use the specific balanced ROI manifest you showed\n",
    "man_path = data_dir / \"manifest_nslt2000_roi_top104_balanced_clean.csv\"\n",
    "assert man_path.exists(), f\"Manifest not found: {man_path}\"\n",
    "\n",
    "df = pd.read_csv(man_path)\n",
    "print(\"Loaded:\", man_path)\n",
    "print(f\"Samples: {len(df)} | classes={df['gloss'].nunique()}\")\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "print(\"label_new min/max:\", df[\"label_new\"].min(), df[\"label_new\"].max())\n",
    "print(\"label_new nunique:\", df[\"label_new\"].nunique())\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ea8585",
   "metadata": {},
   "source": [
    "### Cell B â€” WLASLDataset (using ROI + label_new) + WLASLDataset with on-the-fly augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40ea786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell B â€” WLASLDataset (ROI, label_new) with safe loading + on-the-fly augmentation ===\n",
    "import torch, numpy as np, cv2, decord, random\n",
    "from torch.utils.data import Dataset\n",
    "decord.bridge.set_bridge('torch')\n",
    "\n",
    "\n",
    "def _resize_112(frame_tchw: torch.Tensor) -> torch.Tensor:\n",
    "    T, C, H, W = frame_tchw.shape\n",
    "    arr = frame_tchw.permute(0, 2, 3, 1).cpu().numpy()\n",
    "    out = np.empty((T, 112, 112, C), dtype=np.float32)\n",
    "    for t in range(T):\n",
    "        out[t] = cv2.resize(arr[t], (112, 112), interpolation=cv2.INTER_AREA)\n",
    "    return torch.from_numpy(out).permute(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "def _normalize(frame_tchw, mean=(0.45,)*3, std=(0.225,)*3):\n",
    "    mean = torch.tensor(mean, dtype=frame_tchw.dtype, device=frame_tchw.device)[None, :, None, None]\n",
    "    std  = torch.tensor(std,  dtype=frame_tchw.dtype, device=frame_tchw.device)[None, :, None, None]\n",
    "    return (frame_tchw - mean) / std\n",
    "\n",
    "\n",
    "def uniform_temporal_indices(n_total, clip_len, stride):\n",
    "    if n_total <= 0:\n",
    "        return [0] * clip_len\n",
    "    wanted = (clip_len - 1) * stride + 1\n",
    "    if n_total >= wanted:\n",
    "        start = (n_total - wanted) // 2\n",
    "        return [start + i * stride for i in range(clip_len)]\n",
    "    idxs = [min(i * stride, n_total - 1) for i in range(clip_len)]\n",
    "    return idxs\n",
    "\n",
    "\n",
    "class WLASLDataset(Dataset):\n",
    "    def __init__(self, df, clip_len=32, stride=2, train=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.clip_len = clip_len\n",
    "        self.stride = stride\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    # --------- augmentation helpers (on-the-fly, train only) ---------\n",
    "    def _augment(self, frames: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        frames: [T, C, H, W], values in [0,1] (float32).\n",
    "        Only applied when self.train == True.\n",
    "        \"\"\"\n",
    "        if not self.train:\n",
    "            return frames\n",
    "\n",
    "        T, C, H, W = frames.shape\n",
    "\n",
    "        # 1) Random horizontal flip\n",
    "        if random.random() < 0.5:\n",
    "            frames = torch.flip(frames, dims=[3])  # flip width\n",
    "\n",
    "        # 2) Random Gaussian blur\n",
    "        if random.random() < 0.3:\n",
    "            k = random.choice([3, 5])\n",
    "            fr_np = frames.permute(0, 2, 3, 1).cpu().numpy()  # [T,H,W,C]\n",
    "            for t in range(T):\n",
    "                fr_np[t] = cv2.GaussianBlur(fr_np[t], (k, k), 0)\n",
    "            frames = torch.from_numpy(fr_np).permute(0, 3, 1, 2)\n",
    "\n",
    "        # 3) Random brightness / contrast\n",
    "        if random.random() < 0.3:\n",
    "            alpha = 1.0 + 0.4 * (random.random() - 0.5)   # contrast ~ [0.8, 1.2]\n",
    "            beta  = 0.1 * (random.random() - 0.5)         # brightness ~ [-0.05, 0.05]\n",
    "            frames = frames * alpha + beta\n",
    "            frames = frames.clamp(0.0, 1.0)\n",
    "\n",
    "        # 4) Random cutout mask (simulate occlusion)\n",
    "        if random.random() < 0.3:\n",
    "            mask_size = random.randint(16, 40)\n",
    "            y0 = random.randint(0, max(0, H - mask_size))\n",
    "            x0 = random.randint(0, max(0, W - mask_size))\n",
    "            frames[:, :, y0:y0+mask_size, x0:x0+mask_size] = 0.0\n",
    "\n",
    "        return frames\n",
    "\n",
    "    def _safe_load_clip(self, path: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Try to read a clip with decord. If anything fails, return a dummy zero clip.\n",
    "        Returned shape: [T, C, H, W], float32 in [0,1].\n",
    "        \"\"\"\n",
    "        try:\n",
    "            vr = decord.VideoReader(path)\n",
    "            n = len(vr)\n",
    "            if n <= 0:\n",
    "                raise RuntimeError(\"no frames\")\n",
    "\n",
    "            idxs = uniform_temporal_indices(n, self.clip_len, self.stride)\n",
    "            batch = vr.get_batch(idxs)        # [T,H,W,C]\n",
    "            x = batch.float() / 255.0         # [0,1]\n",
    "            x = x.permute(0, 3, 1, 2)         # [T,C,H,W]\n",
    "            x = _resize_112(x)                # [T,3,112,112]\n",
    "            x = self._augment(x)              # on-the-fly aug (train only)\n",
    "            x = _normalize(x)                 # final normalization\n",
    "            return x\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to read video {path}: {e} â€” using zero clip.\")\n",
    "            x = torch.zeros(self.clip_len, 3, 112, 112, dtype=torch.float32)\n",
    "            x = _normalize(x)\n",
    "            return x\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        row = self.df.iloc[i]\n",
    "        path = row[\"path\"]\n",
    "        label = int(row[\"label_new\"])   # contiguous 0..C-1\n",
    "\n",
    "        x = self._safe_load_clip(path)\n",
    "        return x, label, path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94cb6cb",
   "metadata": {},
   "source": [
    "#### Cell C â€” Split DataFrames & DataLoaders (single worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8283e08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes: 831 train | 192 val | 136 test\n",
      "Device: cuda\n",
      "Sample batch shape: torch.Size([4, 32, 3, 112, 112]) | labels range: 60 -> 88\n"
     ]
    }
   ],
   "source": [
    "# === Cell C â€” Splits + DataLoaders (no multiprocessing) ===\n",
    "\n",
    "train_df = df[df[\"split\"] == \"train\"].reset_index(drop=True)\n",
    "val_df   = df[df[\"split\"] == \"val\"].reset_index(drop=True)\n",
    "test_df  = df[df[\"split\"] == \"test\"].reset_index(drop=True)\n",
    "\n",
    "print(\"Split sizes:\", len(train_df), \"train |\", len(val_df), \"val |\", len(test_df), \"test\")\n",
    "\n",
    "clip_len = 32\n",
    "stride   = 2\n",
    "batch_size = 4   # small to be safe on GPU\n",
    "\n",
    "train_ds = WLASLDataset(train_df, clip_len=clip_len, stride=stride, train=True)\n",
    "val_ds   = WLASLDataset(val_df,   clip_len=clip_len, stride=stride, train=False)\n",
    "test_ds  = WLASLDataset(test_df,  clip_len=clip_len, stride=stride, train=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,              # ðŸ”’ NO worker processes\n",
    "    pin_memory=(device.type == \"cuda\"),\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=(device.type == \"cuda\"),\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=(device.type == \"cuda\"),\n",
    ")\n",
    "\n",
    "x_dbg, y_dbg, _ = next(iter(train_loader))\n",
    "print(\"Sample batch shape:\", x_dbg.shape, \"| labels range:\", y_dbg.min().item(), \"->\", y_dbg.max().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c013386d",
   "metadata": {},
   "source": [
    "#### Cell D â€” Model: R3D-18 baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e525b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes: 104\n",
      "Model on: cuda\n"
     ]
    }
   ],
   "source": [
    "# === Cell D â€” CNN + BiGRU model with PRETRAINED ResNet-18 ===\n",
    "\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "class CnnBiGRUClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        rnn_hidden: int = 256,\n",
    "        rnn_layers: int = 1,\n",
    "        dropout: float = 0.3,\n",
    "        use_pretrained: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 2D CNN backbone (ResNet-18)\n",
    "        # use_pretrained=True -> ImageNet weights, False -> from scratch\n",
    "        if use_pretrained:\n",
    "            try:\n",
    "                # Newer torchvision API\n",
    "                from torchvision.models import ResNet18_Weights\n",
    "                weights = ResNet18_Weights.IMAGENET1K_V1\n",
    "                base = resnet18(weights=weights)\n",
    "            except Exception:\n",
    "                # Fallback for older versions\n",
    "                base = resnet18(pretrained=True)\n",
    "        else:\n",
    "            base = resnet18(weights=None)\n",
    "\n",
    "        # Take everything except the final FC and global pool\n",
    "        self.cnn = nn.Sequential(*list(base.children())[:-2])  # conv -> layer4\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.cnn_out_dim = base.fc.in_features  # 512 for ResNet-18\n",
    "\n",
    "        # BiGRU over time\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=self.cnn_out_dim,\n",
    "            hidden_size=rnn_hidden,\n",
    "            num_layers=rnn_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(rnn_hidden * 2, num_classes)  # *2 for bidirectional\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [B, T, C, H, W]\n",
    "        \"\"\"\n",
    "        B, T, C, H, W = x.shape\n",
    "\n",
    "        # Merge batch and time to process frames with 2D CNN\n",
    "        x = x.view(B * T, C, H, W)              # [B*T, C, H, W]\n",
    "\n",
    "        feats = self.cnn(x)                     # [B*T, C', h, w]\n",
    "        feats = self.pool(feats)                # [B*T, C', 1, 1]\n",
    "        feats = feats.view(B, T, self.cnn_out_dim)  # [B, T, F]\n",
    "\n",
    "        # BiGRU over temporal dimension\n",
    "        rnn_out, _ = self.rnn(feats)           # [B, T, 2*hidden]\n",
    "\n",
    "        # Use last time step\n",
    "        last = rnn_out[:, -1, :]               # [B, 2*hidden]\n",
    "\n",
    "        out = self.dropout(last)\n",
    "        logits = self.fc(out)                  # [B, num_classes]\n",
    "        return logits\n",
    "\n",
    "# Build PRETRAINED model\n",
    "num_classes = df[\"label_new\"].nunique()\n",
    "print(\"num_classes:\", num_classes)\n",
    "\n",
    "model = CnnBiGRUClassifier(\n",
    "    num_classes=num_classes,\n",
    "    rnn_hidden=256,\n",
    "    rnn_layers=1,\n",
    "    dropout=0.3,\n",
    "    use_pretrained=True,    # ðŸ”´ this is the key difference\n",
    ").to(device)\n",
    "\n",
    "print(\"Model on:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0758c3da",
   "metadata": {},
   "source": [
    "#### Cell E â€” Optimizer, Scaler, Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5c31001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell E â€” Optimizer, scaler, loss (pretrained) ===\n",
    "from torch.amp import GradScaler\n",
    "\n",
    "epochs = 20\n",
    "lr     = 1e-4      # ðŸ”½ slightly lower than 3e-4 used for scratch\n",
    "wd     = 1e-2\n",
    "amp_on = True\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "scaler = GradScaler(enabled=amp_on)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # can keep or remove smoothing\n",
    "best_val_acc = -1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881fd9a6",
   "metadata": {},
   "source": [
    "#### Cell F â€” run_epoch (with correct [B,C,T,H,W] permute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68cf500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell F â€” Metrics + epoch runner (for CNN+BiGRU) ===\n",
    "\n",
    "def top1_acc(logits, y):\n",
    "    return (logits.argmax(1) == y).float().mean().item()\n",
    "\n",
    "def run_epoch(loader, train=True):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_acc  = 0.0\n",
    "    total_n    = 0\n",
    "\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "\n",
    "    for x, y, _ in loader:\n",
    "        # x is already [B, T, C, H, W] from WLASLDataset\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.amp.autocast(device_type=device.type, enabled=amp_on):\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "        if train:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            acc = top1_acc(logits, y)\n",
    "            bs  = x.size(0)\n",
    "            total_loss += loss.item() * bs\n",
    "            total_acc  += acc * bs\n",
    "            total_n    += bs\n",
    "\n",
    "    return total_loss / total_n, total_acc / total_n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03f005f",
   "metadata": {},
   "source": [
    "#### Cell G â€” Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efa56b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/20 | train loss 4.7299 acc 0.010 | val loss 4.6258 acc 0.036\n",
      "  âžœ New best val acc=0.036 (model saved)\n",
      "Epoch 02/20 | train loss 4.5225 acc 0.048 | val loss 4.5716 acc 0.062\n",
      "  âžœ New best val acc=0.062 (model saved)\n",
      "Epoch 03/20 | train loss 4.3202 acc 0.084 | val loss 4.5735 acc 0.042\n",
      "Epoch 04/20 | train loss 4.1067 acc 0.138 | val loss 4.4875 acc 0.062\n",
      "Epoch 05/20 | train loss 3.8339 acc 0.202 | val loss 4.5178 acc 0.068\n",
      "  âžœ New best val acc=0.068 (model saved)\n",
      "Epoch 06/20 | train loss 3.5696 acc 0.264 | val loss 4.5277 acc 0.073\n",
      "  âžœ New best val acc=0.073 (model saved)\n",
      "Epoch 07/20 | train loss 3.2973 acc 0.333 | val loss 4.5534 acc 0.089\n",
      "  âžœ New best val acc=0.089 (model saved)\n",
      "Epoch 08/20 | train loss 2.9894 acc 0.443 | val loss 4.5451 acc 0.109\n",
      "  âžœ New best val acc=0.109 (model saved)\n",
      "Epoch 09/20 | train loss 2.6626 acc 0.526 | val loss 4.5590 acc 0.120\n",
      "  âžœ New best val acc=0.120 (model saved)\n",
      "Epoch 10/20 | train loss 2.3752 acc 0.619 | val loss 4.6643 acc 0.104\n",
      "Epoch 11/20 | train loss 2.1507 acc 0.679 | val loss 4.7392 acc 0.094\n",
      "Epoch 12/20 | train loss 1.9548 acc 0.728 | val loss 4.7301 acc 0.099\n",
      "Epoch 13/20 | train loss 1.7623 acc 0.768 | val loss 4.7901 acc 0.104\n",
      "Epoch 14/20 | train loss 1.5944 acc 0.809 | val loss 4.7543 acc 0.130\n",
      "  âžœ New best val acc=0.130 (model saved)\n",
      "Epoch 15/20 | train loss 1.4263 acc 0.863 | val loss 4.8199 acc 0.130\n",
      "Epoch 16/20 | train loss 1.3288 acc 0.884 | val loss 4.9791 acc 0.115\n",
      "Epoch 17/20 | train loss 1.2474 acc 0.905 | val loss 4.9909 acc 0.115\n",
      "Epoch 18/20 | train loss 1.2054 acc 0.915 | val loss 5.0469 acc 0.125\n",
      "Epoch 19/20 | train loss 1.1820 acc 0.925 | val loss 5.1216 acc 0.120\n",
      "Epoch 20/20 | train loss 1.1545 acc 0.928 | val loss 5.1103 acc 0.109\n"
     ]
    }
   ],
   "source": [
    "# === Cell G â€” Training Loop ===\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "root = Path(\"..\").resolve()\n",
    "ckpt_dir = root / \"checkpoints\"\n",
    "report_dir = root / \"reports\"\n",
    "ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "report_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Notebook prefix (sanitized stem)\n",
    "nb_prefix = Path(\"06_train_baseline_CNN_BiGRU_aug-RestNet18.ipynb\").stem.replace(' ', '_')\n",
    "\n",
    "best_val_acc = -1.0\n",
    "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "best_ckpt_path = ckpt_dir / f\"{nb_prefix}_best_cnn_bigru_pretrained_top104.pt\"\n",
    "history_path = report_dir / f\"{nb_prefix}_train_history.json\"\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    tr_loss, tr_acc = run_epoch(train_loader, train=True)\n",
    "    va_loss, va_acc = run_epoch(val_loader,   train=False)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}/{epochs} | \"\n",
    "          f\"train loss {tr_loss:.4f} acc {tr_acc:.3f} | \"\n",
    "          f\"val loss {va_loss:.4f} acc {va_acc:.3f}\")\n",
    "\n",
    "    # record history\n",
    "    history['train_loss'].append(float(tr_loss))\n",
    "    history['train_acc'].append(float(tr_acc))\n",
    "    history['val_loss'].append(float(va_loss))\n",
    "    history['val_acc'].append(float(va_acc))\n",
    "\n",
    "    # persist history after each epoch (safe)\n",
    "    with open(history_path, 'w') as fh:\n",
    "        json.dump(history, fh, indent=2)\n",
    "\n",
    "    if va_acc > best_val_acc:\n",
    "        best_val_acc = va_acc\n",
    "        torch.save(model.state_dict(), str(best_ckpt_path))\n",
    "        print(f\"  âžœ New best val acc={best_val_acc:.3f} (model saved: {best_ckpt_path})\")\n",
    "\n",
    "# final save of history\n",
    "with open(history_path, 'w') as fh:\n",
    "    json.dump(history, fh, indent=2)\n",
    "\n",
    "print('Training finished. History saved to', history_path)\n",
    "print('Best checkpoint saved to', best_ckpt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6789f1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST â€” loss 5.0669 | acc 0.074\n"
     ]
    }
   ],
   "source": [
    "# === Cell H â€” Evaluation & Reporting ===\n",
    "# This cell loads the prefixed best checkpoint and the training history, runs inference on test_loader,\n",
    "# computes present-label-safe metrics (confusion matrix, per-class accuracy, classification report),\n",
    "# and saves artifacts into `reports/` and confirms checkpoint location in `checkpoints/`.\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import Counter\n",
    "\n",
    "# Paths (must match those used in training cell)\n",
    "root = Path(\"..\").resolve()\n",
    "ckpt_dir = root / \"checkpoints\"\n",
    "report_dir = root / \"reports\"\n",
    "nb_prefix = Path(\"06_train_baseline_CNN_BiGRU_aug-RestNet18.ipynb\").stem.replace(' ', '_')\n",
    "best_ckpt_path = ckpt_dir / f\"{nb_prefix}_best_cnn_bigru_pretrained_top104.pt\"\n",
    "history_path = report_dir / f\"{nb_prefix}_train_history.json\"\n",
    "\n",
    "assert best_ckpt_path.exists(), f\"Best checkpoint not found: {best_ckpt_path}\"\n",
    "assert history_path.exists(), f\"Training history not found: {history_path}\"\n",
    "\n",
    "# Load history and plot train/val curves\n",
    "with open(history_path, 'r') as fh:\n",
    "    history = json.load(fh)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(history['train_loss'], label='train_loss')\n",
    "plt.plot(history['val_loss'], label='val_loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.title(f\"{nb_prefix} â€” Loss curves\")\n",
    "loss_png = report_dir / f\"{nb_prefix}_train_val_loss.png\"\n",
    "plt.savefig(loss_png, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(history['train_acc'], label='train_acc')\n",
    "plt.plot(history['val_acc'], label='val_acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.title(f\"{nb_prefix} â€” Accuracy curves\")\n",
    "acc_png = report_dir / f\"{nb_prefix}_train_val_acc.png\"\n",
    "plt.savefig(acc_png, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print('Saved train/val plots to:', loss_png, acc_png)\n",
    "\n",
    "# Load model\n",
    "model.load_state_dict(torch.load(str(best_ckpt_path), map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Run inference on test set and collect predictions\n",
    "y_true = []\n",
    "y_pred = []\n",
    "paths = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y, p in tqdm(test_loader, desc='Inference'):\n",
    "        x = x.to(device)\n",
    "        logits = model(x)\n",
    "        preds = logits.argmax(1).cpu().numpy()\n",
    "        y_cpu = y.cpu().numpy()\n",
    "        y_true.extend(y_cpu.tolist())\n",
    "        y_pred.extend(preds.tolist())\n",
    "        paths.extend(list(p))\n",
    "\n",
    "y_true = np.array(y_true, dtype=int)\n",
    "y_pred = np.array(y_pred, dtype=int)\n",
    "\n",
    "# compute present labels to avoid sklearn target_names mismatch\n",
    "present_labels = np.union1d(np.unique(y_true), np.unique(y_pred)).astype(int)\n",
    "labels_list = present_labels.tolist()\n",
    "\n",
    "# Map label -> gloss (human-readable) using manifest df\n",
    "label_to_name = {int(r['label_new']): str(r['gloss']) for _, r in df[['label_new', 'gloss']].iterrows()}\n",
    "names_list = [label_to_name.get(int(lbl), str(int(lbl))) for lbl in labels_list]\n",
    "\n",
    "# Confusion matrix (raw and normalized by true support)\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels_list)\n",
    "cm_norm = cm.astype(float) / (cm.sum(axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm_norm, xticklabels=names_list, yticklabels=names_list, cmap='viridis', vmin=0.0, vmax=1.0)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title(f\"{nb_prefix} â€” Normalized Confusion Matrix (labels present: {len(labels_list)})\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "cm_png = report_dir / f\"{nb_prefix}_confusion_matrix_norm.png\"\n",
    "plt.savefig(cm_png, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Per-class accuracy & support\n",
    "support = cm.sum(axis=1)\n",
    "per_class_acc = np.diag(cm) / (support + 1e-12)\n",
    "per_class_df = pd.DataFrame({\n",
    "    'label': labels_list,\n",
    "    'name': names_list,\n",
    "    'support': support.tolist(),\n",
    "    'accuracy': per_class_acc.tolist()\n",
    "})\n",
    "per_class_csv = report_dir / f\"{nb_prefix}_per_class_accuracy.csv\"\n",
    "per_class_df.to_csv(per_class_csv, index=False)\n",
    "\n",
    "# Classification report (scikit-learn)\n",
    "clf_report = classification_report(y_true, y_pred, labels=labels_list, target_names=names_list, zero_division=0, output_dict=True)\n",
    "clf_txt = classification_report(y_true, y_pred, labels=labels_list, target_names=names_list, zero_division=0)\n",
    "\n",
    "# Save reports\n",
    "test_report_json = report_dir / f\"{nb_prefix}_test_report.json\"\n",
    "with open(test_report_json, 'w') as fh:\n",
    "    json.dump({\n",
    "        'model_class': model.__class__.__name__,\n",
    "        'num_parameters': sum(p.numel() for p in model.parameters()),\n",
    "        'history_path': str(history_path),\n",
    "        'best_checkpoint': str(best_ckpt_path),\n",
    "        'train_samples': len(train_ds),\n",
    "        'val_samples': len(val_ds),\n",
    "        'test_samples': len(test_ds),\n",
    "        'labels_present': labels_list,\n",
    "        'label_names_present': names_list,\n",
    "        'classification_report': clf_report,\n",
    "        'hyperparameters': {\n",
    "            'epochs': epochs,\n",
    "            'lr': lr,\n",
    "            'wd': wd,\n",
    "            'amp_on': amp_on,\n",
    "            'batch_size': batch_size,\n",
    "            'clip_len': clip_len,\n",
    "            'stride': stride\n",
    "        }\n",
    "    }, fh, indent=2)\n",
    "\n",
    "test_report_txt = report_dir / f\"{nb_prefix}_test_report.txt\"\n",
    "with open(test_report_txt, 'w') as fh:\n",
    "    fh.write(f\"Model: {model.__class__.__name__}\\n\")\n",
    "    fh.write(f\"Num parameters: {sum(p.numel() for p in model.parameters())}\\n\")\n",
    "    fh.write(f\"Train/Val/Test sizes: {len(train_ds)}/{len(val_ds)}/{len(test_ds)}\\n\")\n",
    "    fh.write(f\"Labels present ({len(labels_list)}): {labels_list}\\n\")\n",
    "    fh.write('\\nClassification report:\\n')\n",
    "    fh.write(clf_txt)\n",
    "\n",
    "print('Saved artifacts:')\n",
    "print(' -', cm_png)\n",
    "print(' -', per_class_csv)\n",
    "print(' -', test_report_json)\n",
    "print(' -', test_report_txt)\n",
    "print('\\nSummary accuracy (macro, micro):')\n",
    "\n",
    "# Simple metrics\n",
    "macro_acc = np.mean(list(d['recall'] for d in clf_report.values() if isinstance(d, dict)))\n",
    "micro_acc = (y_true == y_pred).mean()\n",
    "print('Micro acc:', float(micro_acc))\n",
    "print('Macro acc:', float(macro_acc))\n",
    "\n",
    "# Also save a small predictions CSV for debugging / inspection\n",
    "preds_csv = report_dir / f\"{nb_prefix}_predictions.csv\"\n",
    "pd.DataFrame({'path': paths, 'y_true': y_true.tolist(), 'y_pred': y_pred.tolist()}).to_csv(preds_csv, index=False)\n",
    "print('Saved predictions CSV to', preds_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4515dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1a5a59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
